#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç›®æ ‡æ£€æµ‹è®­ç»ƒ - LoRAå¾®è°ƒç‰ˆæœ¬
ä½¿ç”¨LoRAå¤§å¹…å‡å°æ¨¡å‹å¤§å°
"""

import os
os.environ['HF_HOME'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'
os.environ['TRANSFORMERS_CACHE'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'

import torch
from torch.utils.data import Dataset
from transformers import (
    DetrImageProcessor,
    DetrForObjectDetection,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model
from PIL import Image
import matplotlib.pyplot as plt
import json
from datetime import datetime

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(CURRENT_DIR, 'data')
MODEL_DIR = os.path.join(CURRENT_DIR, 'trained_model_lora')
RESULTS_DIR = os.path.join(CURRENT_DIR, 'training_results_lora')

# åˆ›å»ºç›®å½•
os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(RESULTS_DIR, exist_ok=True)

print("=" * 70)
print("ğŸ¯ ç›®æ ‡æ£€æµ‹è®­ç»ƒ - LoRAå¾®è°ƒç‰ˆæœ¬")
print("=" * 70)

# å®šä¹‰ç±»åˆ«
CATEGORIES = ['èƒŒæ™¯', 'åœ†å½¢', 'æ–¹å½¢', 'ä¸‰è§’å½¢']
id2label = {i: label for i, label in enumerate(CATEGORIES)}
label2id = {label: i for i, label in enumerate(CATEGORIES)}

class DetectionDataset(Dataset):
    """ç›®æ ‡æ£€æµ‹æ•°æ®é›†"""
    
    def __init__(self, data_dir, processor, split='train'):
        self.data_dir = data_dir
        self.processor = processor
        self.split = split
        
        images_dir = os.path.join(data_dir, 'images')
        annotations_dir = os.path.join(data_dir, 'annotations')
        
        self.samples = []
        for ann_file in os.listdir(annotations_dir):
            if ann_file.startswith(split) and ann_file.endswith('.json'):
                ann_path = os.path.join(annotations_dir, ann_file)
                with open(ann_path, 'r') as f:
                    annotation = json.load(f)
                
                img_path = os.path.join(images_dir, annotation['file_name'])
                if os.path.exists(img_path):
                    self.samples.append((img_path, annotation))
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, annotation = self.samples[idx]
        
        # åŠ è½½å›¾åƒ
        image = Image.open(img_path).convert('RGB')
        w, h = image.size
        
        # å‡†å¤‡COCOæ ¼å¼æ ‡æ³¨
        boxes = []
        labels = []
        for ann in annotation['annotations']:
            x, y, width, height = ann['bbox']
            x_center = (x + width / 2) / w
            y_center = (y + height / 2) / h
            norm_width = width / w
            norm_height = height / h
            boxes.append([x_center, y_center, norm_width, norm_height])
            labels.append(ann['category_id'])
        
        # æ„å»ºCOCOæ ¼å¼çš„target
        target = {
            'image_id': annotation['image_id'],
            'annotations': [
                {
                    'image_id': annotation['image_id'],
                    'category_id': label,
                    'bbox': box,
                    'area': box[2] * box[3] * w * h,
                    'iscrowd': 0
                }
                for box, label in zip(boxes, labels)
            ]
        }
        
        # é¢„å¤„ç†
        encoding = self.processor(images=image, annotations=target, return_tensors="pt")
        
        # ç§»é™¤batchç»´åº¦
        pixel_values = encoding['pixel_values'].squeeze(0)
        labels = {k: v.squeeze(0) if isinstance(v, torch.Tensor) else v 
                 for k, v in encoding['labels'][0].items()}
        
        return {'pixel_values': pixel_values, 'labels': labels}

def collate_fn(batch):
    """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°"""
    pixel_values = torch.stack([item['pixel_values'] for item in batch])
    labels = [item['labels'] for item in batch]
    return {'pixel_values': pixel_values, 'labels': labels}

def plot_training_results(log_history, save_path):
    """ç»˜åˆ¶è®­ç»ƒç»“æœå›¾"""
    print("\nğŸ“Š ç”Ÿæˆè®­ç»ƒç»“æœå›¾...")
    
    train_loss = []
    steps = []
    
    for log in log_history:
        if 'loss' in log:
            train_loss.append(log['loss'])
            steps.append(log.get('step', len(train_loss)))
    
    fig, ax = plt.subplots(1, 1, figsize=(12, 6))
    fig.suptitle('ç›®æ ‡æ£€æµ‹è®­ç»ƒç»“æœ (LoRA)', fontsize=16, fontweight='bold')
    
    if train_loss:
        ax.plot(steps, train_loss, 'b-', linewidth=2, label='è®­ç»ƒæŸå¤±')
        ax.set_xlabel('è®­ç»ƒæ­¥æ•°')
        ax.set_ylabel('æŸå¤±å€¼')
        ax.set_title('è®­ç»ƒæŸå¤±æ›²çº¿')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… è®­ç»ƒç»“æœå›¾å·²ä¿å­˜: {save_path}")
    plt.close()

def main():
    # æ£€æŸ¥æ•°æ®é›†
    images_dir = os.path.join(DATA_DIR, 'images')
    annotations_dir = os.path.join(DATA_DIR, 'annotations')
    if not os.path.exists(images_dir) or not os.path.exists(annotations_dir):
        print(f"\nâŒ é”™è¯¯: æ•°æ®é›†ä¸å­˜åœ¨ï¼")
        print(f"   è¯·ç¡®ä¿æ•°æ®é›†ä½äº:")
        print(f"   - å›¾åƒ: {images_dir}")
        print(f"   - æ ‡æ³¨: {annotations_dir}")
        return
    
    print("\nğŸ”§ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
    model_name = "facebook/detr-resnet-50"
    
    processor = DetrImageProcessor.from_pretrained(model_name)
    model = DetrForObjectDetection.from_pretrained(
        model_name,
        num_labels=len(CATEGORIES),
        id2label=id2label,
        label2id=label2id,
        ignore_mismatched_sizes=True
    )
    
    print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # é…ç½®LoRA
    print("\nğŸ¯ é…ç½®LoRA...")
    lora_config = LoraConfig(
        r=8,  # LoRAç§©ï¼ˆç›®æ ‡æ£€æµ‹ç”¨è¾ƒå°çš„ç§©ï¼‰
        lora_alpha=16,
        target_modules=["q_proj", "v_proj"],  # DETRçš„æ³¨æ„åŠ›æ¨¡å—
        lora_dropout=0.1,
        bias="none",
    )
    
    # åº”ç”¨LoRA
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    print("âœ… LoRAé…ç½®å®Œæˆ")
    
    # åˆ›å»ºæ•°æ®é›†
    print("\nğŸ“š å‡†å¤‡æ•°æ®é›†...")
    train_dataset = DetectionDataset(DATA_DIR, processor, 'train')
    val_dataset = DetectionDataset(DATA_DIR, processor, 'val')
    
    print(f"âœ… è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"âœ… éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=MODEL_DIR,
        num_train_epochs=20,
        per_device_train_batch_size=2,
        per_device_eval_batch_size=2,
        learning_rate=2e-4,  # LoRAç”¨æ›´é«˜çš„å­¦ä¹ ç‡
        warmup_steps=50,
        weight_decay=0.01,
        logging_dir=os.path.join(RESULTS_DIR, 'logs'),
        logging_steps=10,
        save_strategy="no",
        remove_unused_columns=False,
        push_to_hub=False,
        report_to="none",
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=collate_fn,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("\nğŸš€ å¼€å§‹LoRAå¾®è°ƒ...")
    print("=" * 70)
    
    train_result = trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜LoRAæ¨¡å‹...")
    try:
        model.save_pretrained(MODEL_DIR)
        processor.save_pretrained(MODEL_DIR)
        print(f"âœ… LoRAæ¨¡å‹å·²ä¿å­˜åˆ°: {MODEL_DIR}")
        
        # æ£€æŸ¥æ¨¡å‹å¤§å°
        import glob
        model_files = glob.glob(os.path.join(MODEL_DIR, '*.bin')) + glob.glob(os.path.join(MODEL_DIR, '*.safetensors'))
        if model_files:
            total_size = sum(os.path.getsize(f) for f in model_files) / (1024 * 1024)
            print(f"ğŸ“¦ LoRAæ¨¡å‹å¤§å°: {total_size:.2f} MB (åŸæ¨¡å‹: ~160 MB)")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")
    
    # ä¿å­˜è®­ç»ƒä¿¡æ¯
    metrics = train_result.metrics
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    
    # ç»˜åˆ¶è®­ç»ƒç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    plot_path = os.path.join(RESULTS_DIR, f'training_results_{timestamp}.png')
    plot_training_results(trainer.state.log_history, plot_path)
    
    # ä¿å­˜è®­ç»ƒæ‘˜è¦
    summary = {
        'è®­ç»ƒæ—¶é—´': timestamp,
        'æ¨¡å‹': model_name,
        'å¾®è°ƒæ–¹æ³•': 'LoRA',
        'LoRAé…ç½®': {
            'r': lora_config.r,
            'lora_alpha': lora_config.lora_alpha,
            'target_modules': lora_config.target_modules,
        },
        'ç±»åˆ«æ•°': len(CATEGORIES),
        'ç±»åˆ«': CATEGORIES,
        'è®­ç»ƒæ ·æœ¬æ•°': len(train_dataset),
        'éªŒè¯æ ·æœ¬æ•°': len(val_dataset),
        'è®­ç»ƒè½®æ•°': training_args.num_train_epochs,
        'æœ€ç»ˆè®­ç»ƒæŸå¤±': f"{metrics.get('train_loss', 0):.4f}",
    }
    
    summary_path = os.path.join(RESULTS_DIR, f'training_summary_{timestamp}.json')
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print("\n" + "=" * 70)
    print("âœ… LoRAå¾®è°ƒå®Œæˆï¼")
    print("=" * 70)
    print(f"\nğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: {MODEL_DIR}")
    print(f"ğŸ“Š è®­ç»ƒç»“æœå›¾: {plot_path}")
    print(f"ğŸ“„ è®­ç»ƒæ‘˜è¦: {summary_path}")
    print(f"\nğŸ’¡ LoRAä¼˜åŠ¿: æ¨¡å‹å¤§å°ä»~160MBå¤§å¹…å‡å°‘ï¼")

if __name__ == '__main__':
    main()
