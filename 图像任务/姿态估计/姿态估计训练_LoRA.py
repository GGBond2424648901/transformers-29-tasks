#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å§¿æ€ä¼°è®¡è®­ç»ƒ - LoRAå¾®è°ƒç‰ˆæœ¬
ä½¿ç”¨LoRAå¤§å¹…å‡å°æ¨¡å‹å¤§å°
"""

import os
os.environ['HF_HOME'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'
os.environ['TRANSFORMERS_CACHE'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'

import torch
import torch.nn as nn
from torch.utils.data import Dataset
from transformers import (
    ViTImageProcessor,
    ViTModel,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import json
from datetime import datetime

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(CURRENT_DIR, 'data')
MODEL_DIR = os.path.join(CURRENT_DIR, 'trained_model_lora')
RESULTS_DIR = os.path.join(CURRENT_DIR, 'training_results_lora')

# åˆ›å»ºç›®å½•
os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(RESULTS_DIR, exist_ok=True)

print("=" * 70)
print("ğŸ¤¸ å§¿æ€ä¼°è®¡è®­ç»ƒ - LoRAå¾®è°ƒç‰ˆæœ¬")
print("=" * 70)

# å®šä¹‰å…³é”®ç‚¹
KEYPOINTS = ['å¤´éƒ¨', 'å·¦æ‰‹', 'å³æ‰‹', 'å·¦è„š', 'å³è„š']
NUM_KEYPOINTS = len(KEYPOINTS)

class PoseEstimationModel(nn.Module):
    """å§¿æ€ä¼°è®¡æ¨¡å‹ï¼ˆåŸºäºViT + LoRAï¼‰"""
    
    def __init__(self, vit_model, num_keypoints=5):
        super().__init__()
        self.vit = vit_model
        self.num_keypoints = num_keypoints
        
        # å…³é”®ç‚¹å›å½’å¤´
        hidden_size = self.vit.config.hidden_size
        self.keypoint_head = nn.Sequential(
            nn.Linear(hidden_size, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, num_keypoints * 2)
        )
    
    def forward(self, pixel_values, labels=None):
        outputs = self.vit(pixel_values=pixel_values)
        pooled_output = outputs.pooler_output
        
        keypoints = self.keypoint_head(pooled_output)
        keypoints = keypoints.view(-1, self.num_keypoints, 2)
        
        loss = None
        if labels is not None:
            loss = nn.functional.mse_loss(keypoints, labels)
        
        return {'loss': loss, 'keypoints': keypoints}

class PoseDataset(Dataset):
    """å§¿æ€ä¼°è®¡æ•°æ®é›†"""
    
    def __init__(self, data_dir, processor, split='train'):
        self.data_dir = data_dir
        self.processor = processor
        self.split = split
        
        images_dir = os.path.join(data_dir, 'images')
        annotations_dir = os.path.join(data_dir, 'annotations')
        
        self.samples = []
        for ann_file in os.listdir(annotations_dir):
            if ann_file.startswith(split) and ann_file.endswith('.json'):
                ann_path = os.path.join(annotations_dir, ann_file)
                with open(ann_path, 'r') as f:
                    annotation = json.load(f)
                
                img_path = os.path.join(images_dir, annotation['file_name'])
                if os.path.exists(img_path):
                    self.samples.append((img_path, annotation))
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, annotation = self.samples[idx]
        
        image = Image.open(img_path).convert('RGB')
        encoding = self.processor(images=image, return_tensors="pt")
        pixel_values = encoding['pixel_values'].squeeze()
        
        keypoints = torch.tensor(annotation['keypoints'], dtype=torch.float32)
        
        return {
            'pixel_values': pixel_values,
            'labels': keypoints
        }

def compute_metrics(eval_pred):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    predictions, labels = eval_pred
    
    mse = np.mean((predictions - labels) ** 2)
    mae = np.mean(np.abs(predictions - labels))
    pixel_error = mae * 224
    
    return {
        'mse': mse,
        'mae': mae,
        'pixel_error': pixel_error
    }

class PoseTrainer(Trainer):
    """è‡ªå®šä¹‰å§¿æ€ä¼°è®¡è®­ç»ƒå™¨"""
    
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs, labels=labels)
        loss = outputs['loss']
        return (loss, outputs) if return_outputs else loss
    
    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):
        labels = inputs.pop("labels")
        
        with torch.no_grad():
            outputs = model(**inputs, labels=labels)
            loss = outputs['loss']
            keypoints = outputs['keypoints']
        
        return (loss, keypoints, labels)

def plot_training_results(log_history, save_path):
    """ç»˜åˆ¶è®­ç»ƒç»“æœå›¾"""
    print("\nğŸ“Š ç”Ÿæˆè®­ç»ƒç»“æœå›¾...")
    
    train_loss = []
    eval_loss = []
    eval_mae = []
    eval_pixel_error = []
    steps = []
    eval_steps = []
    
    for log in log_history:
        if 'loss' in log:
            train_loss.append(log['loss'])
            steps.append(log.get('step', len(train_loss)))
        if 'eval_loss' in log:
            eval_loss.append(log['eval_loss'])
            eval_mae.append(log.get('eval_mae', 0))
            eval_pixel_error.append(log.get('eval_pixel_error', 0))
            eval_steps.append(log.get('step', len(eval_loss)))
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('å§¿æ€ä¼°è®¡è®­ç»ƒç»“æœ (LoRA)', fontsize=16, fontweight='bold')
    
    if train_loss:
        axes[0, 0].plot(steps, train_loss, 'b-', linewidth=2, label='è®­ç»ƒæŸå¤±')
        axes[0, 0].set_xlabel('è®­ç»ƒæ­¥æ•°')
        axes[0, 0].set_ylabel('æŸå¤±å€¼')
        axes[0, 0].set_title('è®­ç»ƒæŸå¤±æ›²çº¿')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
    
    if eval_loss:
        axes[0, 1].plot(eval_steps, eval_loss, 'r-', linewidth=2, label='éªŒè¯æŸå¤±')
        axes[0, 1].set_xlabel('è®­ç»ƒæ­¥æ•°')
        axes[0, 1].set_ylabel('æŸå¤±å€¼')
        axes[0, 1].set_title('éªŒè¯æŸå¤±æ›²çº¿')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
    
    if eval_mae:
        axes[1, 0].plot(eval_steps, eval_mae, 'g-', linewidth=2, label='å¹³å‡ç»å¯¹è¯¯å·®')
        axes[1, 0].set_xlabel('è®­ç»ƒæ­¥æ•°')
        axes[1, 0].set_ylabel('MAE')
        axes[1, 0].set_title('å¹³å‡ç»å¯¹è¯¯å·®æ›²çº¿')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
    
    if eval_pixel_error:
        axes[1, 1].plot(eval_steps, eval_pixel_error, 'm-', linewidth=2, label='åƒç´ è¯¯å·®')
        axes[1, 1].set_xlabel('è®­ç»ƒæ­¥æ•°')
        axes[1, 1].set_ylabel('åƒç´ è¯¯å·®')
        axes[1, 1].set_title('å…³é”®ç‚¹åƒç´ è¯¯å·®æ›²çº¿')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… è®­ç»ƒç»“æœå›¾å·²ä¿å­˜: {save_path}")
    plt.close()

def main():
    # æ£€æŸ¥æ•°æ®é›†
    images_dir = os.path.join(DATA_DIR, 'images')
    annotations_dir = os.path.join(DATA_DIR, 'annotations')
    if not os.path.exists(images_dir) or not os.path.exists(annotations_dir):
        print(f"\nâŒ é”™è¯¯: æ•°æ®é›†ä¸å­˜åœ¨ï¼")
        print(f"   è¯·ç¡®ä¿æ•°æ®é›†ä½äº:")
        print(f"   - å›¾åƒ: {images_dir}")
        print(f"   - æ ‡æ³¨: {annotations_dir}")
        return
    
    print("\nğŸ”§ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
    model_name = "google/vit-base-patch16-224"
    
    processor = ViTImageProcessor.from_pretrained(model_name)
    vit_model = ViTModel.from_pretrained(model_name)
    
    print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # é…ç½®LoRA
    print("\nğŸ¯ é…ç½®LoRA...")
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["query", "value"],
        lora_dropout=0.1,
        bias="none",
    )
    
    # åº”ç”¨LoRAåˆ°ViT
    vit_model = get_peft_model(vit_model, lora_config)
    vit_model.print_trainable_parameters()
    
    # åˆ›å»ºå§¿æ€ä¼°è®¡æ¨¡å‹
    model = PoseEstimationModel(vit_model, num_keypoints=NUM_KEYPOINTS)
    
    print("âœ… LoRAé…ç½®å®Œæˆ")
    
    # åˆ›å»ºæ•°æ®é›†
    print("\nğŸ“š å‡†å¤‡æ•°æ®é›†...")
    train_dataset = PoseDataset(DATA_DIR, processor, 'train')
    val_dataset = PoseDataset(DATA_DIR, processor, 'val')
    
    print(f"âœ… è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"âœ… éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=MODEL_DIR,
        num_train_epochs=30,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        learning_rate=2e-4,  # LoRAç”¨æ›´é«˜çš„å­¦ä¹ ç‡
        warmup_steps=50,
        weight_decay=0.01,
        logging_dir=os.path.join(RESULTS_DIR, 'logs'),
        logging_steps=10,
        evaluation_strategy="epoch",
        save_strategy="no",
        load_best_model_at_end=False,
        remove_unused_columns=False,
        push_to_hub=False,
        report_to="none",
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = PoseTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("\nğŸš€ å¼€å§‹LoRAå¾®è°ƒ...")
    print("=" * 70)
    
    train_result = trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜LoRAæ¨¡å‹...")
    try:
        # ä¿å­˜LoRAæƒé‡
        model.vit.save_pretrained(os.path.join(MODEL_DIR, 'vit_lora'))
        # ä¿å­˜å®Œæ•´æ¨¡å‹æƒé‡
        torch.save(model.state_dict(), os.path.join(MODEL_DIR, 'pytorch_model.bin'))
        processor.save_pretrained(MODEL_DIR)
        
        # ä¿å­˜æ¨¡å‹é…ç½®
        model_config = {
            'model_type': 'pose_estimation_lora',
            'base_model': model_name,
            'num_keypoints': NUM_KEYPOINTS,
            'keypoints': KEYPOINTS,
            'lora_config': {
                'r': lora_config.r,
                'lora_alpha': lora_config.lora_alpha,
                'target_modules': lora_config.target_modules,
            }
        }
        with open(os.path.join(MODEL_DIR, 'config.json'), 'w', encoding='utf-8') as f:
            json.dump(model_config, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… LoRAæ¨¡å‹å·²ä¿å­˜åˆ°: {MODEL_DIR}")
        
        # æ£€æŸ¥æ¨¡å‹å¤§å°
        import glob
        model_files = glob.glob(os.path.join(MODEL_DIR, '**/*.bin'), recursive=True) + \
                     glob.glob(os.path.join(MODEL_DIR, '**/*.safetensors'), recursive=True)
        if model_files:
            total_size = sum(os.path.getsize(f) for f in model_files) / (1024 * 1024)
            print(f"ğŸ“¦ LoRAæ¨¡å‹å¤§å°: {total_size:.2f} MB (åŸæ¨¡å‹: ~330 MB)")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")
    
    # ä¿å­˜è®­ç»ƒä¿¡æ¯
    metrics = train_result.metrics
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    
    # æœ€ç»ˆè¯„ä¼°
    print("\nğŸ“Š æœ€ç»ˆè¯„ä¼°...")
    eval_metrics = trainer.evaluate()
    trainer.log_metrics("eval", eval_metrics)
    trainer.save_metrics("eval", eval_metrics)
    
    # ç»˜åˆ¶è®­ç»ƒç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    plot_path = os.path.join(RESULTS_DIR, f'training_results_{timestamp}.png')
    plot_training_results(trainer.state.log_history, plot_path)
    
    # ä¿å­˜è®­ç»ƒæ‘˜è¦
    summary = {
        'è®­ç»ƒæ—¶é—´': timestamp,
        'æ¨¡å‹': model_name,
        'å¾®è°ƒæ–¹æ³•': 'LoRA',
        'LoRAé…ç½®': {
            'r': lora_config.r,
            'lora_alpha': lora_config.lora_alpha,
            'target_modules': lora_config.target_modules,
        },
        'å…³é”®ç‚¹æ•°': NUM_KEYPOINTS,
        'å…³é”®ç‚¹': KEYPOINTS,
        'è®­ç»ƒæ ·æœ¬æ•°': len(train_dataset),
        'éªŒè¯æ ·æœ¬æ•°': len(val_dataset),
        'è®­ç»ƒè½®æ•°': training_args.num_train_epochs,
        'æœ€ç»ˆæŒ‡æ ‡': {
            'MAE': f"{eval_metrics.get('eval_mae', 0):.6f}",
            'MSE': f"{eval_metrics.get('eval_mse', 0):.6f}",
            'åƒç´ è¯¯å·®': f"{eval_metrics.get('eval_pixel_error', 0):.2f}åƒç´ ",
        }
    }
    
    summary_path = os.path.join(RESULTS_DIR, f'training_summary_{timestamp}.json')
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print("\n" + "=" * 70)
    print("âœ… LoRAå¾®è°ƒå®Œæˆï¼")
    print("=" * 70)
    print(f"\nğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: {MODEL_DIR}")
    print(f"ğŸ“Š è®­ç»ƒç»“æœå›¾: {plot_path}")
    print(f"ğŸ“„ è®­ç»ƒæ‘˜è¦: {summary_path}")
    print(f"\nğŸ¯ æœ€ç»ˆåƒç´ è¯¯å·®: {eval_metrics.get('eval_pixel_error', 0):.2f} åƒç´ ")
    print(f"ğŸ¯ æœ€ç»ˆMAE: {eval_metrics.get('eval_mae', 0):.6f}")
    print(f"\nğŸ’¡ LoRAä¼˜åŠ¿: æ¨¡å‹å¤§å°ä»~330MBå¤§å¹…å‡å°‘ï¼")

if __name__ == '__main__':
    main()
