#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•è§†é¢‘åˆ†ç±»åŠŸèƒ½
"""

import os
os.environ['HF_HOME'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'
os.environ['TRANSFORMERS_CACHE'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'

from transformers import AutoImageProcessor, AutoModelForVideoClassification
import torch
from PIL import Image
import numpy as np

print("=" * 70)
print("ğŸ¬ æµ‹è¯•è§†é¢‘åˆ†ç±»æ¨¡å‹")
print("=" * 70)

print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
processor = AutoImageProcessor.from_pretrained("MCG-NJU/videomae-base-finetuned-kinetics")
model = AutoModelForVideoClassification.from_pretrained("MCG-NJU/videomae-base-finetuned-kinetics")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {device}")

# åˆ›å»ºæµ‹è¯•å›¾åƒï¼ˆ16å¸§ï¼‰- ä½¿ç”¨numpyæ•°ç»„
print("\nğŸ–¼ï¸ åˆ›å»ºæµ‹è¯•å›¾åƒ...")
test_frames = []
for i in range(16):
    # åˆ›å»ºä¸€ä¸ªæ¸å˜çš„æµ‹è¯•å›¾åƒï¼Œä½¿ç”¨numpyæ•°ç»„
    img_array = np.zeros((224, 224, 3), dtype=np.uint8)
    img_array[:, :, 0] = i * 15  # Ré€šé“
    img_array[:, :, 1] = 100      # Gé€šé“
    img_array[:, :, 2] = 200      # Bé€šé“
    img = Image.fromarray(img_array)
    test_frames.append(img)

print(f"âœ… åˆ›å»ºäº† {len(test_frames)} å¸§æµ‹è¯•å›¾åƒï¼Œæ¯å¸§å¤§å°: {test_frames[0].size}")

# æµ‹è¯•å¤„ç†å™¨
print("\nğŸ”§ æµ‹è¯•å›¾åƒå¤„ç†å™¨...")
try:
    # VideoMAEå¤„ç†å™¨æœŸæœ›è¾“å…¥æ˜¯ [video1, video2, ...] æ ¼å¼
    # æ¯ä¸ªvideoæ˜¯ä¸€ä¸ªåŒ…å«å¤šå¸§çš„åˆ—è¡¨
    # æˆ‘ä»¬åªæœ‰ä¸€ä¸ªè§†é¢‘ï¼Œæ‰€ä»¥åŒ…è£…æˆ [frames]
    inputs = processor(
        test_frames,  # ç›´æ¥ä¼ é€’å¸§åˆ—è¡¨
        return_tensors="pt"
    )
    print(f"âœ… å¤„ç†å™¨è¾“å‡º: {inputs.keys()}")
    print(f"   - pixel_values shape: {inputs['pixel_values'].shape}")
    
    # ç§»åˆ°è®¾å¤‡
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # æµ‹è¯•æ¨¡å‹æ¨ç†
    print("\nğŸ¤– æµ‹è¯•æ¨¡å‹æ¨ç†...")
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = torch.nn.functional.softmax(logits, dim=-1)
    
    print(f"âœ… æ¨¡å‹è¾“å‡º logits shape: {logits.shape}")
    print(f"   - ç±»åˆ«æ•°é‡: {logits.shape[-1]}")
    
    # è·å–top-5ç»“æœ
    top_probs, top_indices = torch.topk(probs, 5)
    
    print("\nğŸ† Top-5 é¢„æµ‹ç»“æœ:")
    for i, (prob, idx) in enumerate(zip(top_probs[0], top_indices[0]), 1):
        label = model.config.id2label.get(idx.item(), f"ç±»åˆ«_{idx.item()}")
        print(f"   {i}. {label}: {prob.item()*100:.2f}%")
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è§†é¢‘åˆ†ç±»åŠŸèƒ½æ­£å¸¸å·¥ä½œ")
    
except Exception as e:
    print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
