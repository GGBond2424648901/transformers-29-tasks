# ğŸ¯ LoRAå¾®è°ƒä¸æ¨¡å‹ä¼˜åŒ–å®Œæ•´æŒ‡å—

> æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨LoRAæŠ€æœ¯å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œé«˜æ•ˆå¾®è°ƒï¼Œä»¥åŠæ¨¡å‹ä¼˜åŒ–ã€é‡åŒ–ã€å‰ªæã€è’¸é¦å’Œéƒ¨ç½²çš„å®Œæ•´æµç¨‹ã€‚

## ğŸ“š ç›®å½•

1. [LoRAåŸºç¡€æ¦‚å¿µ](#1-loraåŸºç¡€æ¦‚å¿µ)
2. [LoRAå·¥ä½œåŸç†](#2-loraå·¥ä½œåŸç†)
3. [LoRAåœ¨29ä¸ªé¡¹ç›®ä¸­çš„åº”ç”¨](#3-loraåœ¨29ä¸ªé¡¹ç›®ä¸­çš„åº”ç”¨)
4. [æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯](#4-æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯)
5. [å®Œæ•´è®­ç»ƒæµç¨‹](#5-å®Œæ•´è®­ç»ƒæµç¨‹)
6. [å®æˆ˜ä»£ç ç¤ºä¾‹](#6-å®æˆ˜ä»£ç ç¤ºä¾‹)
7. [æ¨¡å‹éƒ¨ç½²æŒ‡å—](#7-æ¨¡å‹éƒ¨ç½²æŒ‡å—)
8. [å¸¸è§é—®é¢˜è§£ç­”](#8-å¸¸è§é—®é¢˜è§£ç­”)

---

## 1. LoRAåŸºç¡€æ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯LoRAï¼Ÿ

**LoRA (Low-Rank Adaptation of Large Language Models)** æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ï¼Œç”±å¾®è½¯ç ”ç©¶é™¢åœ¨2021å¹´æå‡ºã€‚

#### æ ¸å¿ƒæ€æƒ³

```
ä¼ ç»Ÿå¾®è°ƒï¼šæ›´æ–°æ‰€æœ‰æ¨¡å‹å‚æ•°ï¼ˆ100%å‚æ•°ï¼‰
LoRAå¾®è°ƒï¼šåªæ›´æ–°å°‘é‡ä½ç§©çŸ©é˜µï¼ˆ0.1%-1%å‚æ•°ï¼‰
```

#### ä¸»è¦ä¼˜åŠ¿

| ä¼˜åŠ¿ | è¯´æ˜ | æ•°æ® |
|------|------|------|
| ğŸ’° **å‚æ•°æ•ˆç‡** | åªéœ€è®­ç»ƒæå°‘å‚æ•° | 0.1%-1%çš„åŸæ¨¡å‹å‚æ•° |
| ğŸš€ **è®­ç»ƒé€Ÿåº¦** | è®­ç»ƒæ—¶é—´å¤§å¹…å‡å°‘ | å¿«50%-70% |
| ğŸ’¾ **æ˜¾å­˜å ç”¨** | æ˜¾å­˜éœ€æ±‚æ˜¾è‘—é™ä½ | å‡å°‘60%-80% |
| ğŸ“¦ **å­˜å‚¨å‹å¥½** | LoRAæƒé‡æ–‡ä»¶å¾ˆå° | é€šå¸¸å‡ MBåˆ°å‡ ç™¾MB |
| ğŸ”„ **æ˜“äºåˆ‡æ¢** | å¯å¿«é€Ÿåˆ‡æ¢ä¸åŒä»»åŠ¡ | ç§’çº§åˆ‡æ¢ |
| ğŸ¯ **ç²¾åº¦ä¿æŒ** | æ€§èƒ½æ¥è¿‘å…¨é‡å¾®è°ƒ | ç²¾åº¦æŸå¤±<1% |

### 1.2 ä¸ºä»€ä¹ˆéœ€è¦LoRAï¼Ÿ

#### ä¼ ç»Ÿå¾®è°ƒçš„é—®é¢˜

```python
# ä¼ ç»Ÿå¾®è°ƒï¼šéœ€è¦æ›´æ–°æ‰€æœ‰å‚æ•°
model = AutoModelForCausalLM.from_pretrained("llama-7b")  # 7Bå‚æ•°
# è®­ç»ƒæ—¶éœ€è¦ï¼š
# - æ˜¾å­˜ï¼š~28GBï¼ˆFP32ï¼‰æˆ– ~14GBï¼ˆFP16ï¼‰
# - æ—¶é—´ï¼šæ•°å°æ—¶åˆ°æ•°å¤©
# - å­˜å‚¨ï¼šæ¯ä¸ªä»»åŠ¡éƒ½è¦ä¿å­˜å®Œæ•´æ¨¡å‹ï¼ˆ~13GBï¼‰
```

#### LoRAçš„è§£å†³æ–¹æ¡ˆ

```python
# LoRAå¾®è°ƒï¼šåªæ›´æ–°LoRAå‚æ•°
model = AutoModelForCausalLM.from_pretrained("llama-7b")
model = get_peft_model(model, lora_config)  # æ·»åŠ LoRAå±‚
# è®­ç»ƒæ—¶åªéœ€ï¼š
# - æ˜¾å­˜ï¼š~6GBï¼ˆå‡å°‘60%ï¼‰
# - æ—¶é—´ï¼šå¿«50%ä»¥ä¸Š
# - å­˜å‚¨ï¼šæ¯ä¸ªä»»åŠ¡åªéœ€å‡ MBçš„LoRAæƒé‡
```

### 1.3 LoRAé€‚ç”¨åœºæ™¯

#### âœ… éå¸¸é€‚åˆ

- å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒï¼ˆGPTã€LLaMAã€ChatGLMï¼‰
- è§†è§‰Transformerå¾®è°ƒï¼ˆViTã€Swinï¼‰
- å¤šæ¨¡æ€æ¨¡å‹å¾®è°ƒï¼ˆBLIPã€CLIPï¼‰
- éŸ³é¢‘æ¨¡å‹å¾®è°ƒï¼ˆWhisperã€Wav2Vec2ï¼‰
- èµ„æºå—é™ç¯å¢ƒï¼ˆæ¶ˆè´¹çº§GPUï¼‰
- éœ€è¦å¤šä»»åŠ¡åˆ‡æ¢çš„åœºæ™¯

#### âš ï¸ ä¸å¤ªé€‚åˆ

- æ¨¡å‹ç»“æ„éœ€è¦å¤§å¹…æ”¹å˜
- éœ€è¦ä»å¤´è®­ç»ƒçš„åœºæ™¯
- ä»»åŠ¡ä¸é¢„è®­ç»ƒå·®å¼‚æå¤§
- å¯¹ç²¾åº¦è¦æ±‚æé«˜çš„å…³é”®åº”ç”¨



---

## 2. LoRAå·¥ä½œåŸç†

### 2.1 æ•°å­¦åŸç†

#### ä¼ ç»Ÿå¾®è°ƒ

å¯¹äºé¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡çŸ©é˜µ `W âˆˆ R^(dÃ—k)`ï¼š

```
W_new = W + Î”W
```

éœ€è¦è®­ç»ƒ `d Ã— k` ä¸ªå‚æ•°ï¼ˆé€šå¸¸æ˜¯æ•°ç™¾ä¸‡åˆ°æ•°åäº¿ï¼‰

#### LoRAå¾®è°ƒ

```
W_new = W + Î±/r Â· BA
```

å…¶ä¸­ï¼š
- `W âˆˆ R^(dÃ—k)`: é¢„è®­ç»ƒæƒé‡ï¼ˆ**å†»ç»“ï¼Œä¸æ›´æ–°**ï¼‰
- `B âˆˆ R^(dÃ—r)`: LoRAçŸ©é˜µBï¼ˆ**å¯è®­ç»ƒ**ï¼‰
- `A âˆˆ R^(rÃ—k)`: LoRAçŸ©é˜µAï¼ˆ**å¯è®­ç»ƒ**ï¼‰
- `r`: ç§©ï¼ˆrankï¼‰ï¼Œé€šå¸¸ä¸º 4-64ï¼Œè¿œå°äº min(d, k)
- `Î±`: ç¼©æ”¾å› å­ï¼ˆlora_alphaï¼‰

åªéœ€è®­ç»ƒ `r Ã— (d + k)` ä¸ªå‚æ•°ï¼

#### å‚æ•°é‡å¯¹æ¯”

ä»¥LLaMA-7Bä¸ºä¾‹ï¼ˆd=4096, k=4096ï¼‰ï¼š

```
ä¼ ç»Ÿå¾®è°ƒï¼š4096 Ã— 4096 = 16,777,216 å‚æ•°/å±‚
LoRA (r=8)ï¼š8 Ã— (4096 + 4096) = 65,536 å‚æ•°/å±‚

å‚æ•°å‡å°‘ï¼š99.6%ï¼
```

### 2.2 LoRAå±‚çš„ç»“æ„

```
è¾“å…¥ x
  â†“
åŸå§‹å±‚: WÂ·x (å†»ç»“)
  â†“
  +  â† LoRAè·¯å¾„: (Î±/r)Â·BÂ·AÂ·x (å¯è®­ç»ƒ)
  â†“
è¾“å‡º y
```

#### ä»£ç å®ç°

```python
class LoRALayer(nn.Module):
    def __init__(self, in_features, out_features, rank=8, alpha=16):
        super().__init__()
        self.rank = rank
        self.alpha = alpha
        
        # LoRAçŸ©é˜µ
        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
        
        # åˆå§‹åŒ–
        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B)
        
        self.scaling = self.alpha / self.rank
    
    def forward(self, x, original_output):
        # åŸå§‹è¾“å‡º + LoRAè¾“å‡º
        lora_output = (x @ self.lora_A.T @ self.lora_B.T) * self.scaling
        return original_output + lora_output
```

### 2.3 LoRAçš„å…³é”®å‚æ•°

#### r (rank) - ç§©

- **ä½œç”¨**: æ§åˆ¶LoRAçŸ©é˜µçš„ç»´åº¦
- **èŒƒå›´**: é€šå¸¸ 4-64
- **å½±å“**: 
  - è¶Šå¤§ï¼šè¡¨è¾¾èƒ½åŠ›è¶Šå¼ºï¼Œå‚æ•°è¶Šå¤š
  - è¶Šå°ï¼šå‚æ•°è¶Šå°‘ï¼Œå¯èƒ½æ¬ æ‹Ÿåˆ

**æ¨èå€¼**:
```python
å°æ¨¡å‹ï¼ˆ<1Bï¼‰: r=4-8
ä¸­æ¨¡å‹ï¼ˆ1B-7Bï¼‰: r=8-16
å¤§æ¨¡å‹ï¼ˆ>7Bï¼‰: r=16-64
```

#### lora_alpha - ç¼©æ”¾å› å­

- **ä½œç”¨**: æ§åˆ¶LoRAè¾“å‡ºçš„ç¼©æ”¾
- **èŒƒå›´**: é€šå¸¸æ˜¯ r çš„ 1-4 å€
- **å½±å“**: æ§åˆ¶LoRAå¯¹æœ€ç»ˆè¾“å‡ºçš„å½±å“ç¨‹åº¦

**æ¨èå€¼**:
```python
lora_alpha = 2 * r  # å¸¸ç”¨é…ç½®
# ä¾‹å¦‚: r=8, alpha=16
```

#### target_modules - ç›®æ ‡æ¨¡å—

- **ä½œç”¨**: æŒ‡å®šå“ªäº›å±‚åº”ç”¨LoRA
- **é€‰æ‹©**: é€šå¸¸é€‰æ‹©æ³¨æ„åŠ›å±‚çš„æŠ•å½±çŸ©é˜µ

**ä¸åŒæ¨¡å‹çš„æ¨èé…ç½®**:

```python
# LLaMA/GPT
target_modules = ["q_proj", "v_proj"]  # æœ€å°é…ç½®
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]  # å®Œæ•´é…ç½®

# BERT
target_modules = ["query", "value"]
target_modules = ["query", "key", "value"]  # å®Œæ•´é…ç½®

# ViT (Vision Transformer)
target_modules = ["qkv"]  # ViTé€šå¸¸åˆå¹¶äº†QKV
target_modules = ["qkv", "proj"]  # åŒ…å«è¾“å‡ºæŠ•å½±

# Whisper
target_modules = ["q_proj", "v_proj"]
```

#### lora_dropout - Dropoutç‡

- **ä½œç”¨**: é˜²æ­¢è¿‡æ‹Ÿåˆ
- **èŒƒå›´**: 0.0-0.1
- **æ¨è**: 0.05-0.1



---

## 3. LoRAåœ¨29ä¸ªé¡¹ç›®ä¸­çš„åº”ç”¨

### 3.1 æ–‡æœ¬ä»»åŠ¡ï¼ˆ7ä¸ªé¡¹ç›®ï¼‰

#### âœ… å¯ä»¥ä½¿ç”¨LoRAçš„é¡¹ç›®

| é¡¹ç›® | é€‚ç”¨æ€§ | æ¨èé…ç½® | é¢„æœŸæ•ˆæœ |
|------|--------|---------|---------|
| é—®ç­”ç³»ç»Ÿ | â­â­â­â­â­ | r=8, alpha=16 | æ˜¾å­˜å‡å°‘60% |
| å‘½åå®ä½“è¯†åˆ« | â­â­â­â­â­ | r=8, alpha=16 | è®­ç»ƒå¿«50% |
| æ–‡æœ¬åˆ†ç±» | â­â­â­â­â­ | r=4, alpha=8 | å‚æ•°å‡å°‘99% |
| æ–‡æœ¬æ‘˜è¦ | â­â­â­â­â­ | r=16, alpha=32 | ç²¾åº¦æŸå¤±<1% |
| æœºå™¨ç¿»è¯‘ | â­â­â­â­â­ | r=16, alpha=32 | å¤šè¯­è¨€åˆ‡æ¢ |
| æ©ç è¯å¡«å…… | â­â­â­â­ | r=8, alpha=16 | å¿«é€Ÿå¾®è°ƒ |
| é›¶æ ·æœ¬åˆ†ç±» | â­â­â­ | r=4, alpha=8 | é€‚é…ç‰¹å®šé¢†åŸŸ |

#### é…ç½®ç¤ºä¾‹ï¼šé—®ç­”ç³»ç»Ÿ

```python
from peft import LoraConfig, get_peft_model, TaskType
from transformers import AutoModelForQuestionAnswering

# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-chinese")

# 2. é…ç½®LoRA
lora_config = LoraConfig(
    task_type=TaskType.QUESTION_ANS,
    r=8,                              # ç§©
    lora_alpha=16,                    # ç¼©æ”¾å› å­
    target_modules=["query", "value"], # BERTçš„æ³¨æ„åŠ›å±‚
    lora_dropout=0.1,
    bias="none"
)

# 3. åº”ç”¨LoRA
model = get_peft_model(model, lora_config)

# 4. æŸ¥çœ‹å¯è®­ç»ƒå‚æ•°
model.print_trainable_parameters()
# è¾“å‡º: trainable params: 294,912 || all params: 102,267,648 || trainable%: 0.29%
```

### 3.2 å›¾åƒä»»åŠ¡ï¼ˆ10ä¸ªé¡¹ç›®ï¼‰

#### âœ… å¯ä»¥ä½¿ç”¨LoRAçš„é¡¹ç›®

| é¡¹ç›® | é€‚ç”¨æ€§ | æ¨èé…ç½® | é¢„æœŸæ•ˆæœ |
|------|--------|---------|---------|
| å›¾åƒåˆ†ç±» | â­â­â­â­â­ | r=4, alpha=8 | å¿«é€Ÿé€‚é…æ–°ç±»åˆ« |
| ç›®æ ‡æ£€æµ‹ | â­â­â­â­ | r=8, alpha=16 | ç‰¹å®šåœºæ™¯ä¼˜åŒ– |
| å›¾åƒåˆ†å‰² | â­â­â­â­ | r=8, alpha=16 | åŒ»ç–—/å«æ˜Ÿå›¾åƒ |
| å§¿æ€ä¼°è®¡ | â­â­â­â­ | r=4, alpha=8 | ç‰¹å®šäººç¾¤é€‚é… |
| æ·±åº¦ä¼°è®¡ | â­â­â­â­ | r=8, alpha=16 | å®¤å†…/å®¤å¤–åœºæ™¯ |
| è§†é¢‘åˆ†ç±» | â­â­â­â­ | r=8, alpha=16 | ç‰¹å®šè§†é¢‘ç±»å‹ |
| é›¶æ ·æœ¬å›¾åƒåˆ†ç±» | â­â­â­ | r=4, alpha=8 | CLIPå¾®è°ƒ |
| å…³é”®ç‚¹æ£€æµ‹ | â­â­â­â­ | r=4, alpha=8 | ç‰¹å®šå¯¹è±¡ |

#### é…ç½®ç¤ºä¾‹ï¼šå›¾åƒåˆ†ç±»ï¼ˆViTï¼‰

```python
from peft import LoraConfig, get_peft_model
from transformers import ViTForImageClassification

# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = ViTForImageClassification.from_pretrained(
    "google/vit-base-patch16-224",
    num_labels=10  # ä½ çš„ç±»åˆ«æ•°
)

# 2. é…ç½®LoRA
lora_config = LoraConfig(
    r=4,
    lora_alpha=8,
    target_modules=["query", "value"],  # ViTçš„æ³¨æ„åŠ›å±‚
    lora_dropout=0.1,
    bias="none"
)

# 3. åº”ç”¨LoRA
model = get_peft_model(model, lora_config)

# 4. å†»ç»“åˆ†ç±»å¤´ä»¥å¤–çš„å‚æ•°ï¼ˆå¯é€‰ï¼‰
for name, param in model.named_parameters():
    if "classifier" not in name and "lora" not in name:
        param.requires_grad = False
```

### 3.3 éŸ³é¢‘ä»»åŠ¡ï¼ˆ5ä¸ªé¡¹ç›®ï¼‰

#### âœ… å¯ä»¥ä½¿ç”¨LoRAçš„é¡¹ç›®

| é¡¹ç›® | é€‚ç”¨æ€§ | æ¨èé…ç½® | é¢„æœŸæ•ˆæœ |
|------|--------|---------|---------|
| éŸ³é¢‘åˆ†ç±» | â­â­â­â­â­ | r=8, alpha=16 | ç‰¹å®šéŸ³é¢‘ç±»å‹ |
| è¯­éŸ³è¯†åˆ« | â­â­â­â­â­ | r=8, alpha=16 | æ–¹è¨€/å£éŸ³é€‚é… |
| è¯­éŸ³åˆ°è¯­éŸ³ | â­â­â­â­ | r=8, alpha=16 | éŸ³è‰²è½¬æ¢ |
| æ–‡æœ¬åˆ°éŸ³ä¹ | â­â­â­ | r=16, alpha=32 | é£æ ¼é€‚é… |
| æ–‡æœ¬è½¬è¯­éŸ³ | â­â­â­â­ | r=8, alpha=16 | éŸ³è‰²å®šåˆ¶ |

#### é…ç½®ç¤ºä¾‹ï¼šè¯­éŸ³è¯†åˆ«ï¼ˆWhisperï¼‰

```python
from peft import LoraConfig, get_peft_model
from transformers import WhisperForConditionalGeneration

# 1. åŠ è½½Whisperæ¨¡å‹
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-base")

# 2. é…ç½®LoRA
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],  # Whisperçš„æ³¨æ„åŠ›å±‚
    lora_dropout=0.05,
    bias="none"
)

# 3. åº”ç”¨LoRA
model = get_peft_model(model, lora_config)

# Whisperç‰¹åˆ«é€‚åˆLoRAï¼š
# - å¯ä»¥å¿«é€Ÿé€‚é…ä¸åŒè¯­è¨€
# - å¯ä»¥é€‚é…ç‰¹å®šå£éŸ³/æ–¹è¨€
# - å¯ä»¥é€‚é…ä¸“ä¸šæœ¯è¯­ï¼ˆåŒ»ç–—ã€æ³•å¾‹ç­‰ï¼‰
```

### 3.4 å¤šæ¨¡æ€ä»»åŠ¡ï¼ˆ6ä¸ªé¡¹ç›®ï¼‰

#### âœ… å¯ä»¥ä½¿ç”¨LoRAçš„é¡¹ç›®

| é¡¹ç›® | é€‚ç”¨æ€§ | æ¨èé…ç½® | é¢„æœŸæ•ˆæœ |
|------|--------|---------|---------|
| å›¾åƒæè¿°ç”Ÿæˆ | â­â­â­â­â­ | r=8, alpha=16 | ç‰¹å®šé¢†åŸŸæè¿° |
| è§†è§‰é—®ç­” | â­â­â­â­â­ | r=8, alpha=16 | ä¸“ä¸šé¢†åŸŸQA |
| è¡¨æ ¼é—®ç­” | â­â­â­â­ | r=8, alpha=16 | ç‰¹å®šè¡¨æ ¼æ ¼å¼ |
| æ–‡æ¡£ç†è§£ | â­â­â­â­ | r=8, alpha=16 | ç‰¹å®šæ–‡æ¡£ç±»å‹ |
| éŸ³é¢‘æ–‡æœ¬ç†è§£ | â­â­â­â­ | r=8, alpha=16 | ç‰¹å®šåœºæ™¯ |
| è§†è§‰æ–‡æœ¬ç”Ÿæˆ | â­â­â­â­ | r=8, alpha=16 | é£æ ¼é€‚é… |

#### é…ç½®ç¤ºä¾‹ï¼šå›¾åƒæè¿°ç”Ÿæˆï¼ˆBLIPï¼‰

```python
from peft import LoraConfig, get_peft_model
from transformers import BlipForConditionalGeneration

# 1. åŠ è½½BLIPæ¨¡å‹
model = BlipForConditionalGeneration.from_pretrained(
    "Salesforce/blip-image-captioning-base"
)

# 2. é…ç½®LoRAï¼ˆåŒæ—¶åº”ç”¨åˆ°è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨ï¼‰
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=[
        "qkv",          # è§†è§‰ç¼–ç å™¨
        "query",        # æ–‡æœ¬ç¼–ç å™¨
        "value"
    ],
    lora_dropout=0.1,
    bias="none"
)

# 3. åº”ç”¨LoRA
model = get_peft_model(model, lora_config)

# BLIPçš„LoRAä¼˜åŠ¿ï¼š
# - å¯ä»¥é€‚é…ç‰¹å®šé¢†åŸŸçš„å›¾åƒï¼ˆåŒ»ç–—ã€è‰ºæœ¯ç­‰ï¼‰
# - å¯ä»¥è°ƒæ•´æè¿°é£æ ¼ï¼ˆç®€æ´ã€è¯¦ç»†ã€è¯—æ„ç­‰ï¼‰
# - å¯ä»¥é€‚é…ç‰¹å®šè¯­è¨€æˆ–æœ¯è¯­
```

### 3.5 æƒ…æ„Ÿåˆ†æï¼ˆ1ä¸ªé¡¹ç›®ï¼‰

#### âœ… å¯ä»¥ä½¿ç”¨LoRA

| é¡¹ç›® | é€‚ç”¨æ€§ | æ¨èé…ç½® | é¢„æœŸæ•ˆæœ |
|------|--------|---------|---------|
| æƒ…æ„Ÿåˆ†æ | â­â­â­â­â­ | r=4, alpha=8 | ç‰¹å®šé¢†åŸŸæƒ…æ„Ÿ |

```python
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForSequenceClassification

# 1. åŠ è½½æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-chinese",
    num_labels=3  # æ­£é¢ã€è´Ÿé¢ã€ä¸­æ€§
)

# 2. é…ç½®LoRA
lora_config = LoraConfig(
    r=4,
    lora_alpha=8,
    target_modules=["query", "value"],
    lora_dropout=0.1
)

# 3. åº”ç”¨LoRA
model = get_peft_model(model, lora_config)
```



---

## 4. æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯

### 4.1 é‡åŒ–ï¼ˆQuantizationï¼‰

#### ä»€ä¹ˆæ˜¯é‡åŒ–ï¼Ÿ

å°†æ¨¡å‹æƒé‡ä»é«˜ç²¾åº¦ï¼ˆFP32/FP16ï¼‰è½¬æ¢ä¸ºä½ç²¾åº¦ï¼ˆINT8/INT4ï¼‰ï¼Œå‡å°‘æ¨¡å‹å¤§å°å’Œæ¨ç†æ—¶é—´ã€‚

#### é‡åŒ–ç±»å‹å¯¹æ¯”

| ç±»å‹ | ç²¾åº¦ | æ¨¡å‹å¤§å° | æ¨ç†é€Ÿåº¦ | ç²¾åº¦æŸå¤± | é€‚ç”¨åœºæ™¯ |
|------|------|---------|---------|---------|---------|
| FP32 | 32ä½æµ®ç‚¹ | 100% | 1x | 0% | è®­ç»ƒã€é«˜ç²¾åº¦æ¨ç† |
| FP16 | 16ä½æµ®ç‚¹ | 50% | 2x | <0.1% | GPUæ¨ç† |
| INT8 | 8ä½æ•´æ•° | 25% | 3-4x | 0.5-1% | CPU/GPUæ¨ç† |
| INT4 | 4ä½æ•´æ•° | 12.5% | 4-6x | 1-3% | èµ„æºå—é™è®¾å¤‡ |

#### 4.1.1 è®­ç»ƒåé‡åŒ–ï¼ˆPTQï¼‰

**æ–¹æ³•1ï¼š8ä½é‡åŒ–**

```python
from transformers import AutoModelForCausalLM
import torch

# åŠ è½½æ¨¡å‹æ—¶ç›´æ¥é‡åŒ–
model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    load_in_8bit=True,      # 8ä½é‡åŒ–
    device_map="auto",       # è‡ªåŠ¨åˆ†é…è®¾å¤‡
    torch_dtype=torch.float16
)

# æ•ˆæœï¼š
# - æ¨¡å‹å¤§å°å‡å°‘75%
# - æ˜¾å­˜å ç”¨å‡å°‘75%
# - æ¨ç†é€Ÿåº¦æå‡3-4å€
# - ç²¾åº¦æŸå¤±<1%
```

**æ–¹æ³•2ï¼š4ä½é‡åŒ–ï¼ˆæ›´æ¿€è¿›ï¼‰**

```python
from transformers import BitsAndBytesConfig

# é…ç½®4ä½é‡åŒ–
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",           # ä½¿ç”¨NF4é‡åŒ–
    bnb_4bit_compute_dtype=torch.float16, # è®¡ç®—æ—¶ä½¿ç”¨FP16
    bnb_4bit_use_double_quant=True        # åŒé‡é‡åŒ–
)

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    quantization_config=bnb_config,
    device_map="auto"
)

# æ•ˆæœï¼š
# - æ¨¡å‹å¤§å°å‡å°‘87.5%
# - 7Bæ¨¡å‹åªéœ€~4GBæ˜¾å­˜
# - å¯åœ¨æ¶ˆè´¹çº§GPUè¿è¡Œå¤§æ¨¡å‹
```

**æ–¹æ³•3ï¼šåŠ¨æ€é‡åŒ–**

```python
import torch.quantization as quantization

# åŠ¨æ€é‡åŒ–ï¼ˆæ¨ç†æ—¶é‡åŒ–ï¼‰
quantized_model = quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},  # é‡åŒ–çš„å±‚ç±»å‹
    dtype=torch.qint8
)

# ç‰¹ç‚¹ï¼š
# - ä¸éœ€è¦æ ¡å‡†æ•°æ®
# - æƒé‡é‡åŒ–ï¼Œæ¿€æ´»åŠ¨æ€é‡åŒ–
# - é€‚åˆCPUæ¨ç†
```

#### 4.1.2 é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰

```python
import torch.quantization as quantization

# 1. å‡†å¤‡æ¨¡å‹
model.qconfig = quantization.get_default_qat_qconfig('fbgemm')
model_prepared = quantization.prepare_qat(model, inplace=False)

# 2. è®­ç»ƒï¼ˆæ¨¡æ‹Ÿé‡åŒ–ï¼‰
for epoch in range(num_epochs):
    train_one_epoch(model_prepared, train_loader)

# 3. è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
model_quantized = quantization.convert(model_prepared, inplace=False)

# ä¼˜åŠ¿ï¼š
# - ç²¾åº¦æŸå¤±æœ€å°ï¼ˆ<0.5%ï¼‰
# - æ¨¡å‹åœ¨è®­ç»ƒæ—¶å°±é€‚åº”é‡åŒ–
```

#### 4.1.3 LoRA + é‡åŒ–ç»„åˆ

```python
from peft import prepare_model_for_kbit_training

# 1. åŠ è½½é‡åŒ–æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    load_in_4bit=True,
    device_map="auto"
)

# 2. å‡†å¤‡LoRAè®­ç»ƒ
model = prepare_model_for_kbit_training(model)

# 3. æ·»åŠ LoRA
lora_config = LoraConfig(r=8, lora_alpha=16, ...)
model = get_peft_model(model, lora_config)

# 4. è®­ç»ƒ
trainer.train()

# ä¼˜åŠ¿ï¼š
# - 4ä½é‡åŒ– + LoRA = æè‡´æ•ˆç‡
# - 7Bæ¨¡å‹åªéœ€4-6GBæ˜¾å­˜è®­ç»ƒ
# - è®­ç»ƒé€Ÿåº¦å¿«ï¼Œç²¾åº¦æŸå¤±å°
```

### 4.2 å‰ªæï¼ˆPruningï¼‰

#### ä»€ä¹ˆæ˜¯å‰ªæï¼Ÿ

ç§»é™¤æ¨¡å‹ä¸­ä¸é‡è¦çš„æƒé‡æˆ–ç¥ç»å…ƒï¼Œå‡å°‘æ¨¡å‹å¤§å°å’Œè®¡ç®—é‡ã€‚

#### 4.2.1 éç»“æ„åŒ–å‰ªæ

```python
import torch.nn.utils.prune as prune

# å‰ªæå•ä¸ªå±‚
layer = model.bert.encoder.layer[0].attention.self.query
prune.l1_unstructured(
    layer,
    name="weight",
    amount=0.3  # å‰ªæ30%çš„æƒé‡
)

# æŸ¥çœ‹å‰ªææ•ˆæœ
print(list(layer.named_parameters()))
# è¾“å‡º: [('weight_orig', ...), ('weight_mask', ...)]

# æ°¸ä¹…åº”ç”¨å‰ªæ
prune.remove(layer, 'weight')
```

**å…¨å±€å‰ªæ**

```python
# æ”¶é›†æ‰€æœ‰è¦å‰ªæçš„å±‚
parameters_to_prune = []
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Linear):
        parameters_to_prune.append((module, "weight"))

# å…¨å±€å‰ªæï¼ˆä¿ç•™æœ€é‡è¦çš„80%æƒé‡ï¼‰
prune.global_unstructured(
    parameters_to_prune,
    pruning_method=prune.L1Unstructured,
    amount=0.2
)

# æ•ˆæœï¼š
# - æ¨¡å‹å¤§å°å‡å°‘20%
# - æ¨ç†é€Ÿåº¦æå‡10-20%
# - ç²¾åº¦æŸå¤±1-2%
```

#### 4.2.2 ç»“æ„åŒ–å‰ªæ

```python
# å‰ªææ•´ä¸ªé€šé“/ç¥ç»å…ƒ
prune.ln_structured(
    module,
    name="weight",
    amount=0.5,    # å‰ªæ50%çš„é€šé“
    n=2,           # L2èŒƒæ•°
    dim=0          # å‰ªæè¾“å‡ºé€šé“
)

# ä¼˜åŠ¿ï¼š
# - çœŸæ­£å‡å°‘è®¡ç®—é‡
# - ä¸éœ€è¦ç‰¹æ®Šç¡¬ä»¶æ”¯æŒ
# - å¯ä»¥å‡å°‘å®é™…æ¨ç†æ—¶é—´
```

#### 4.2.3 æ¸è¿›å¼å‰ªæ

```python
# åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å‰ªæ
def progressive_pruning(model, initial_sparsity=0.0, final_sparsity=0.5, num_steps=100):
    for step in range(num_steps):
        # è®¡ç®—å½“å‰å‰ªæç‡
        current_sparsity = initial_sparsity + (final_sparsity - initial_sparsity) * (step / num_steps)
        
        # åº”ç”¨å‰ªæ
        for module in model.modules():
            if isinstance(module, torch.nn.Linear):
                prune.l1_unstructured(module, name="weight", amount=current_sparsity)
        
        # è®­ç»ƒä¸€ä¸ªepoch
        train_one_epoch(model, train_loader)
        
        # ç§»é™¤å‰ªæé‡å‚æ•°åŒ–
        for module in model.modules():
            if isinstance(module, torch.nn.Linear):
                prune.remove(module, 'weight')
```

### 4.3 çŸ¥è¯†è’¸é¦ï¼ˆDistillationï¼‰

#### ä»€ä¹ˆæ˜¯è’¸é¦ï¼Ÿ

ç”¨å¤§æ¨¡å‹ï¼ˆæ•™å¸ˆï¼‰çš„çŸ¥è¯†è®­ç»ƒå°æ¨¡å‹ï¼ˆå­¦ç”Ÿï¼‰ï¼Œè®©å°æ¨¡å‹å­¦ä¹ å¤§æ¨¡å‹çš„è¡Œä¸ºã€‚

#### 4.3.1 åŸºç¡€è’¸é¦

```python
import torch.nn.functional as F

def distillation_loss(student_logits, teacher_logits, labels, temperature=2.0, alpha=0.5):
    """
    è’¸é¦æŸå¤± = alpha * è½¯æ ‡ç­¾æŸå¤± + (1-alpha) * ç¡¬æ ‡ç­¾æŸå¤±
    """
    # è½¯æ ‡ç­¾æŸå¤±ï¼ˆä»æ•™å¸ˆå­¦ä¹ ï¼‰
    soft_loss = F.kl_div(
        F.log_softmax(student_logits / temperature, dim=-1),
        F.softmax(teacher_logits / temperature, dim=-1),
        reduction='batchmean'
    ) * (temperature ** 2)
    
    # ç¡¬æ ‡ç­¾æŸå¤±ï¼ˆä»çœŸå®æ ‡ç­¾å­¦ä¹ ï¼‰
    hard_loss = F.cross_entropy(student_logits, labels)
    
    # ç»„åˆæŸå¤±
    return alpha * soft_loss + (1 - alpha) * hard_loss

# è®­ç»ƒå¾ªç¯
teacher_model.eval()  # æ•™å¸ˆæ¨¡å‹ä¸æ›´æ–°
student_model.train()

for batch in train_loader:
    inputs, labels = batch
    
    # æ•™å¸ˆæ¨¡å‹é¢„æµ‹
    with torch.no_grad():
        teacher_logits = teacher_model(inputs).logits
    
    # å­¦ç”Ÿæ¨¡å‹é¢„æµ‹
    student_logits = student_model(inputs).logits
    
    # è®¡ç®—è’¸é¦æŸå¤±
    loss = distillation_loss(student_logits, teacher_logits, labels)
    
    # åå‘ä¼ æ’­
    loss.backward()
    optimizer.step()
```

#### 4.3.2 ç‰¹å¾è’¸é¦

```python
def feature_distillation_loss(student_features, teacher_features):
    """
    è®©å­¦ç”Ÿæ¨¡å‹çš„ä¸­é—´ç‰¹å¾æ¥è¿‘æ•™å¸ˆæ¨¡å‹
    """
    loss = 0
    for s_feat, t_feat in zip(student_features, teacher_features):
        # MSEæŸå¤±
        loss += F.mse_loss(s_feat, t_feat)
    return loss

# è®­ç»ƒæ—¶åŒæ—¶ä½¿ç”¨logitså’Œç‰¹å¾
total_loss = (
    distillation_loss(student_logits, teacher_logits, labels) +
    0.1 * feature_distillation_loss(student_features, teacher_features)
)
```

#### 4.3.3 è‡ªè’¸é¦

```python
# ç”¨æ¨¡å‹è‡ªå·±çš„é¢„æµ‹ä½œä¸ºè½¯æ ‡ç­¾
def self_distillation(model, inputs, labels, temperature=2.0):
    # ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­ï¼ˆç”Ÿæˆè½¯æ ‡ç­¾ï¼‰
    with torch.no_grad():
        teacher_logits = model(inputs).logits
    
    # ç¬¬äºŒæ¬¡å‰å‘ä¼ æ’­ï¼ˆå­¦ç”Ÿï¼‰
    student_logits = model(inputs).logits
    
    # è’¸é¦æŸå¤±
    loss = distillation_loss(student_logits, teacher_logits, labels, temperature)
    return loss
```

#### 4.3.4 è’¸é¦æ•ˆæœ

| æ¨¡å‹ | å‚æ•°é‡ | å¤§å° | é€Ÿåº¦ | ç²¾åº¦ |
|------|--------|------|------|------|
| BERT-baseï¼ˆæ•™å¸ˆï¼‰ | 110M | 440MB | 1x | 92.0% |
| DistilBERTï¼ˆå­¦ç”Ÿï¼‰ | 66M | 264MB | 1.6x | 91.3% |
| TinyBERTï¼ˆå­¦ç”Ÿï¼‰ | 14M | 56MB | 9.4x | 90.5% |

**è’¸é¦ä¼˜åŠ¿**ï¼š
- ä¿ç•™å¤§æ¨¡å‹çš„æ€§èƒ½
- å¤§å¹…å‡å°‘æ¨¡å‹å¤§å°
- æ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦
- ç²¾åº¦æŸå¤±å¯æ§



---

## 5. å®Œæ•´è®­ç»ƒæµç¨‹

### 5.1 LoRAå¾®è°ƒå®Œæ•´æµç¨‹

#### æµç¨‹å›¾

```
1. å‡†å¤‡æ•°æ®
   â†“
2. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
   â†“
3. é…ç½®LoRA
   â†“
4. å†»ç»“åŸå§‹å‚æ•°
   â†“
5. æ·»åŠ LoRAå±‚
   â†“
6. è®­ç»ƒï¼ˆåªæ›´æ–°LoRAå‚æ•°ï¼‰
   â†“
7. ä¿å­˜LoRAæƒé‡
   â†“
8. æ¨ç†ï¼ˆåˆå¹¶æˆ–ç‹¬ç«‹åŠ è½½ï¼‰
   â†“
9. éƒ¨ç½²
```

#### 5.1.1 æ­¥éª¤1ï¼šå‡†å¤‡æ•°æ®

```python
from datasets import load_dataset
from transformers import AutoTokenizer

# åŠ è½½æ•°æ®é›†
dataset = load_dataset("your_dataset")

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")

# æ•°æ®é¢„å¤„ç†
def preprocess_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=512
    )

# åº”ç”¨é¢„å¤„ç†
tokenized_dataset = dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=dataset["train"].column_names
)
```

#### 5.1.2 æ­¥éª¤2-5ï¼šåŠ è½½æ¨¡å‹å¹¶é…ç½®LoRA

```python
from transformers import AutoModelForSequenceClassification
from peft import LoraConfig, get_peft_model, TaskType

# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-chinese",
    num_labels=3
)

# 2. é…ç½®LoRA
lora_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,  # ä»»åŠ¡ç±»å‹
    r=8,                          # ç§©
    lora_alpha=16,                # ç¼©æ”¾å› å­
    target_modules=["query", "value"],  # ç›®æ ‡æ¨¡å—
    lora_dropout=0.1,
    bias="none",
    inference_mode=False
)

# 3. åº”ç”¨LoRAï¼ˆè‡ªåŠ¨å†»ç»“åŸå§‹å‚æ•°å¹¶æ·»åŠ LoRAå±‚ï¼‰
model = get_peft_model(model, lora_config)

# 4. æŸ¥çœ‹å¯è®­ç»ƒå‚æ•°
model.print_trainable_parameters()
# è¾“å‡ºç¤ºä¾‹ï¼š
# trainable params: 294,912 || all params: 102,267,648 || trainable%: 0.29%
```

#### 5.1.3 æ­¥éª¤6ï¼šè®­ç»ƒ

```python
from transformers import TrainingArguments, Trainer

# è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./lora_output",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    learning_rate=3e-4,          # LoRAé€šå¸¸ç”¨è¾ƒå¤§å­¦ä¹ ç‡
    weight_decay=0.01,
    logging_steps=100,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    save_total_limit=3,
    load_best_model_at_end=True,
    fp16=True,                   # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    gradient_accumulation_steps=2
)

# åˆ›å»ºTrainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    tokenizer=tokenizer
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

#### 5.1.4 æ­¥éª¤7ï¼šä¿å­˜LoRAæƒé‡

```python
# åªä¿å­˜LoRAæƒé‡ï¼ˆå‡ MBï¼‰
model.save_pretrained("./lora_weights")
tokenizer.save_pretrained("./lora_weights")

# ä¿å­˜çš„æ–‡ä»¶ï¼š
# lora_weights/
# â”œâ”€â”€ adapter_config.json  # LoRAé…ç½®
# â”œâ”€â”€ adapter_model.bin    # LoRAæƒé‡ï¼ˆå¾ˆå°ï¼ï¼‰
# â””â”€â”€ tokenizerç›¸å…³æ–‡ä»¶
```

#### 5.1.5 æ­¥éª¤8ï¼šåŠ è½½å’Œæ¨ç†

**æ–¹æ³•1ï¼šåˆå¹¶LoRAæƒé‡**

```python
from peft import PeftModel

# 1. åŠ è½½åŸºç¡€æ¨¡å‹
base_model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-chinese"
)

# 2. åŠ è½½LoRAæƒé‡
model = PeftModel.from_pretrained(base_model, "./lora_weights")

# 3. åˆå¹¶æƒé‡ï¼ˆå¯é€‰ï¼Œç”¨äºéƒ¨ç½²ï¼‰
model = model.merge_and_unload()

# 4. æ¨ç†
inputs = tokenizer("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•", return_tensors="pt")
outputs = model(**inputs)
predictions = outputs.logits.argmax(dim=-1)
```

**æ–¹æ³•2ï¼šç‹¬ç«‹åŠ è½½ï¼ˆæ¨èç”¨äºå¤šä»»åŠ¡åˆ‡æ¢ï¼‰**

```python
# ä¿æŒLoRAæƒé‡ç‹¬ç«‹ï¼Œå¯ä»¥å¿«é€Ÿåˆ‡æ¢
model = PeftModel.from_pretrained(base_model, "./lora_weights")

# æ¨ç†
outputs = model(**inputs)

# åˆ‡æ¢åˆ°å¦ä¸€ä¸ªä»»åŠ¡
model.load_adapter("./another_lora_weights", adapter_name="task2")
model.set_adapter("task2")
```

### 5.2 æ•°æ®å‡†å¤‡è¯¦è§£

#### 5.2.1 æ–‡æœ¬æ•°æ®

```python
# æ ¼å¼1ï¼šCSVæ–‡ä»¶
import pandas as pd

df = pd.read_csv("data.csv")
# åˆ—ï¼štext, label

# è½¬æ¢ä¸ºDataset
from datasets import Dataset
dataset = Dataset.from_pandas(df)

# æ ¼å¼2ï¼šJSONæ–‡ä»¶
dataset = load_dataset("json", data_files="data.json")

# æ ¼å¼3ï¼šè‡ªå®šä¹‰æ ¼å¼
def load_custom_data():
    data = {
        "text": [],
        "label": []
    }
    # è¯»å–ä½ çš„æ•°æ®
    with open("data.txt") as f:
        for line in f:
            text, label = line.strip().split("\t")
            data["text"].append(text)
            data["label"].append(int(label))
    return Dataset.from_dict(data)
```

#### 5.2.2 å›¾åƒæ•°æ®

```python
from datasets import load_dataset
from torchvision import transforms

# åŠ è½½å›¾åƒæ•°æ®é›†
dataset = load_dataset("imagefolder", data_dir="./images")
# ç›®å½•ç»“æ„ï¼š
# images/
# â”œâ”€â”€ train/
# â”‚   â”œâ”€â”€ class1/
# â”‚   â””â”€â”€ class2/
# â””â”€â”€ val/

# å›¾åƒé¢„å¤„ç†
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])

def preprocess_images(examples):
    examples["pixel_values"] = [
        transform(image.convert("RGB")) 
        for image in examples["image"]
    ]
    return examples

dataset = dataset.map(preprocess_images, batched=True)
```

#### 5.2.3 éŸ³é¢‘æ•°æ®

```python
from datasets import load_dataset, Audio

# åŠ è½½éŸ³é¢‘æ•°æ®é›†
dataset = load_dataset("audiofolder", data_dir="./audio")

# é‡é‡‡æ ·åˆ°16kHz
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))

# éŸ³é¢‘é¢„å¤„ç†
from transformers import WhisperProcessor

processor = WhisperProcessor.from_pretrained("openai/whisper-base")

def preprocess_audio(examples):
    audio_arrays = [x["array"] for x in examples["audio"]]
    inputs = processor(
        audio_arrays,
        sampling_rate=16000,
        return_tensors="pt",
        padding=True
    )
    return inputs

dataset = dataset.map(preprocess_audio, batched=True)
```

### 5.3 è®­ç»ƒæŠ€å·§

#### 5.3.1 å­¦ä¹ ç‡è°ƒæ•´

```python
# LoRAé€šå¸¸ä½¿ç”¨æ¯”å…¨é‡å¾®è°ƒæ›´å¤§çš„å­¦ä¹ ç‡
learning_rates = {
    "å°æ¨¡å‹ï¼ˆ<1Bï¼‰": 5e-4,
    "ä¸­æ¨¡å‹ï¼ˆ1B-7Bï¼‰": 3e-4,
    "å¤§æ¨¡å‹ï¼ˆ>7Bï¼‰": 1e-4
}

# ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
from transformers import get_linear_schedule_with_warmup

num_training_steps = len(train_loader) * num_epochs
num_warmup_steps = num_training_steps // 10

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=num_warmup_steps,
    num_training_steps=num_training_steps
)
```

#### 5.3.2 æ¢¯åº¦ç´¯ç§¯

```python
# å½“æ˜¾å­˜ä¸è¶³æ—¶ï¼Œä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
training_args = TrainingArguments(
    per_device_train_batch_size=4,      # å®é™…batch size
    gradient_accumulation_steps=4,       # ç´¯ç§¯4æ­¥
    # ç­‰æ•ˆbatch size = 4 * 4 = 16
)
```

#### 5.3.3 æ··åˆç²¾åº¦è®­ç»ƒ

```python
# ä½¿ç”¨FP16åŠ é€Ÿè®­ç»ƒ
training_args = TrainingArguments(
    fp16=True,  # å¯ç”¨FP16
    # æˆ–ä½¿ç”¨BF16ï¼ˆå¦‚æœç¡¬ä»¶æ”¯æŒï¼‰
    # bf16=True
)

# æ•ˆæœï¼š
# - è®­ç»ƒé€Ÿåº¦æå‡2å€
# - æ˜¾å­˜å ç”¨å‡å°‘50%
# - ç²¾åº¦æŸå¤±å¯å¿½ç•¥
```

#### 5.3.4 æ—©åœ

```python
from transformers import EarlyStoppingCallback

# æ·»åŠ æ—©åœå›è°ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    callbacks=[
        EarlyStoppingCallback(
            early_stopping_patience=3,  # 3ä¸ªevalå‘¨æœŸä¸æ”¹å–„å°±åœæ­¢
            early_stopping_threshold=0.001
        )
    ]
)
```



---

## 6. å®æˆ˜ä»£ç ç¤ºä¾‹

### 6.1 æ–‡æœ¬åˆ†ç±»ï¼ˆæƒ…æ„Ÿåˆ†æï¼‰

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LoRAå¾®è°ƒç¤ºä¾‹ï¼šæƒ…æ„Ÿåˆ†æ
"""

import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model, TaskType
import torch

# 1. å‡†å¤‡æ•°æ®
print("ğŸ“š åŠ è½½æ•°æ®é›†...")
dataset = load_dataset("csv", data_files={
    "train": "train.csv",
    "test": "test.csv"
})

# 2. åŠ è½½åˆ†è¯å™¨
print("ğŸ”¤ åŠ è½½åˆ†è¯å™¨...")
tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")

# 3. æ•°æ®é¢„å¤„ç†
def preprocess(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )

tokenized_dataset = dataset.map(preprocess, batched=True)

# 4. åŠ è½½æ¨¡å‹
print("ğŸ¤– åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-chinese",
    num_labels=3  # æ­£é¢ã€è´Ÿé¢ã€ä¸­æ€§
)

# 5. é…ç½®LoRA
print("âš™ï¸ é…ç½®LoRA...")
lora_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    r=8,
    lora_alpha=16,
    target_modules=["query", "value"],
    lora_dropout=0.1,
    bias="none"
)

# 6. åº”ç”¨LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# 7. è®­ç»ƒé…ç½®
training_args = TrainingArguments(
    output_dir="./sentiment_lora",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    learning_rate=3e-4,
    weight_decay=0.01,
    logging_steps=100,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    fp16=True
)

# 8. åˆ›å»ºTrainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer
)

# 9. è®­ç»ƒ
print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
trainer.train()

# 10. ä¿å­˜LoRAæƒé‡
print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
model.save_pretrained("./sentiment_lora_final")
tokenizer.save_pretrained("./sentiment_lora_final")

print("âœ… è®­ç»ƒå®Œæˆï¼")
```

### 6.2 å›¾åƒåˆ†ç±»ï¼ˆViT + LoRAï¼‰

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LoRAå¾®è°ƒç¤ºä¾‹ï¼šå›¾åƒåˆ†ç±»
"""

from datasets import load_dataset
from transformers import (
    ViTImageProcessor,
    ViTForImageClassification,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model
import torch
from torchvision.transforms import (
    Compose, Resize, ToTensor, Normalize
)

# 1. åŠ è½½æ•°æ®
print("ğŸ“š åŠ è½½å›¾åƒæ•°æ®é›†...")
dataset = load_dataset("imagefolder", data_dir="./images")

# 2. å›¾åƒå¤„ç†å™¨
processor = ViTImageProcessor.from_pretrained("google/vit-base-patch16-224")

# 3. æ•°æ®é¢„å¤„ç†
def transform(examples):
    inputs = processor(examples["image"], return_tensors="pt")
    inputs["labels"] = examples["label"]
    return inputs

dataset = dataset.map(transform, batched=True)

# 4. åŠ è½½æ¨¡å‹
print("ğŸ¤– åŠ è½½ViTæ¨¡å‹...")
model = ViTForImageClassification.from_pretrained(
    "google/vit-base-patch16-224",
    num_labels=10,  # ä½ çš„ç±»åˆ«æ•°
    ignore_mismatched_sizes=True
)

# 5. é…ç½®LoRA
lora_config = LoraConfig(
    r=4,
    lora_alpha=8,
    target_modules=["query", "value"],  # ViTçš„æ³¨æ„åŠ›å±‚
    lora_dropout=0.1,
    bias="none"
)

# 6. åº”ç”¨LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# 7. è®­ç»ƒ
training_args = TrainingArguments(
    output_dir="./vit_lora",
    num_train_epochs=5,
    per_device_train_batch_size=32,
    learning_rate=5e-4,
    fp16=True,
    evaluation_strategy="epoch",
    save_strategy="epoch"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"]
)

trainer.train()
model.save_pretrained("./vit_lora_final")
```

### 6.3 è¯­éŸ³è¯†åˆ«ï¼ˆWhisper + LoRAï¼‰

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LoRAå¾®è°ƒç¤ºä¾‹ï¼šè¯­éŸ³è¯†åˆ«ï¼ˆWhisperï¼‰
"""

from datasets import load_dataset, Audio
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer
)
from peft import LoraConfig, get_peft_model, TaskType

# 1. åŠ è½½æ•°æ®
print("ğŸ“š åŠ è½½éŸ³é¢‘æ•°æ®é›†...")
dataset = load_dataset("audiofolder", data_dir="./audio")
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))

# 2. åŠ è½½å¤„ç†å™¨
processor = WhisperProcessor.from_pretrained("openai/whisper-base")

# 3. æ•°æ®é¢„å¤„ç†
def prepare_dataset(batch):
    audio = batch["audio"]
    
    # å¤„ç†éŸ³é¢‘
    batch["input_features"] = processor(
        audio["array"],
        sampling_rate=audio["sampling_rate"],
        return_tensors="pt"
    ).input_features[0]
    
    # å¤„ç†æ–‡æœ¬
    batch["labels"] = processor.tokenizer(batch["text"]).input_ids
    
    return batch

dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names["train"])

# 4. åŠ è½½æ¨¡å‹
print("ğŸ¤– åŠ è½½Whisperæ¨¡å‹...")
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-base")

# 5. é…ç½®LoRA
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none"
)

# 6. åº”ç”¨LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# 7. è®­ç»ƒ
training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper_lora",
    per_device_train_batch_size=8,
    learning_rate=1e-3,
    num_train_epochs=3,
    fp16=True,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    predict_with_generate=True
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    tokenizer=processor.feature_extractor
)

trainer.train()
model.save_pretrained("./whisper_lora_final")
```

### 6.4 å¤šæ¨¡æ€ï¼ˆBLIP + LoRAï¼‰

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LoRAå¾®è°ƒç¤ºä¾‹ï¼šå›¾åƒæè¿°ç”Ÿæˆï¼ˆBLIPï¼‰
"""

from datasets import load_dataset
from transformers import (
    BlipProcessor,
    BlipForConditionalGeneration,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer
)
from peft import LoraConfig, get_peft_model

# 1. åŠ è½½æ•°æ®
dataset = load_dataset("imagefolder", data_dir="./image_caption_data")

# 2. åŠ è½½å¤„ç†å™¨
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

# 3. æ•°æ®é¢„å¤„ç†
def preprocess(examples):
    inputs = processor(
        images=examples["image"],
        text=examples["caption"],
        return_tensors="pt",
        padding=True
    )
    return inputs

dataset = dataset.map(preprocess, batched=True)

# 4. åŠ è½½æ¨¡å‹
model = BlipForConditionalGeneration.from_pretrained(
    "Salesforce/blip-image-captioning-base"
)

# 5. é…ç½®LoRAï¼ˆåŒæ—¶åº”ç”¨åˆ°è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨ï¼‰
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["qkv", "query", "value"],  # è§†è§‰+æ–‡æœ¬
    lora_dropout=0.1,
    bias="none"
)

# 6. åº”ç”¨LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# 7. è®­ç»ƒ
training_args = Seq2SeqTrainingArguments(
    output_dir="./blip_lora",
    per_device_train_batch_size=16,
    learning_rate=5e-4,
    num_train_epochs=5,
    fp16=True,
    evaluation_strategy="epoch"
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"]
)

trainer.train()
model.save_pretrained("./blip_lora_final")
```

### 6.5 LoRA + é‡åŒ–ç»„åˆ

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LoRA + 4ä½é‡åŒ–ï¼šåœ¨æ¶ˆè´¹çº§GPUä¸Šè®­ç»ƒå¤§æ¨¡å‹
"""

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import torch

# 1. é…ç½®4ä½é‡åŒ–
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)

# 2. åŠ è½½é‡åŒ–æ¨¡å‹
print("ğŸ¤– åŠ è½½4ä½é‡åŒ–æ¨¡å‹...")
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",  # 7Bæ¨¡å‹
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

# 3. å‡†å¤‡LoRAè®­ç»ƒ
model = prepare_model_for_kbit_training(model)

# 4. é…ç½®LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# 5. åº”ç”¨LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# è¾“å‡º: trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06%

# 6. è®­ç»ƒ
# 7Bæ¨¡å‹ + 4ä½é‡åŒ– + LoRA = åªéœ€4-6GBæ˜¾å­˜ï¼
trainer.train()

# 7. ä¿å­˜
model.save_pretrained("./llama2_7b_lora")
```



---

## 7. æ¨¡å‹éƒ¨ç½²æŒ‡å—

### 7.1 éƒ¨ç½²æ–¹å¼å¯¹æ¯”

| éƒ¨ç½²æ–¹å¼ | ä¼˜åŠ¿ | åŠ£åŠ¿ | é€‚ç”¨åœºæ™¯ |
|---------|------|------|---------|
| **åˆå¹¶éƒ¨ç½²** | æ¨ç†å¿«ï¼Œå…¼å®¹æ€§å¥½ | éœ€è¦å®Œæ•´æ¨¡å‹ç©ºé—´ | å•ä»»åŠ¡ç”Ÿäº§ç¯å¢ƒ |
| **ç‹¬ç«‹éƒ¨ç½²** | çµæ´»åˆ‡æ¢ï¼ŒèŠ‚çœç©ºé—´ | æ¨ç†ç¨æ…¢ | å¤šä»»åŠ¡åˆ‡æ¢ |
| **é‡åŒ–éƒ¨ç½²** | æ¨¡å‹å°ï¼Œæ¨ç†å¿« | ç²¾åº¦ç•¥é™ | èµ„æºå—é™ç¯å¢ƒ |

### 7.2 åˆå¹¶éƒ¨ç½²

#### 7.2.1 åˆå¹¶LoRAæƒé‡

```python
from peft import PeftModel
from transformers import AutoModelForSequenceClassification

# 1. åŠ è½½åŸºç¡€æ¨¡å‹
base_model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-chinese"
)

# 2. åŠ è½½LoRAæƒé‡
model = PeftModel.from_pretrained(base_model, "./lora_weights")

# 3. åˆå¹¶æƒé‡
model = model.merge_and_unload()

# 4. ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
model.save_pretrained("./merged_model")
tokenizer.save_pretrained("./merged_model")

# ç°åœ¨å¯ä»¥åƒæ™®é€šæ¨¡å‹ä¸€æ ·ä½¿ç”¨
```

#### 7.2.2 Flask APIéƒ¨ç½²

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LoRAæ¨¡å‹Flask APIéƒ¨ç½²
"""

from flask import Flask, request, jsonify
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

app = Flask(__name__)

# åŠ è½½æ¨¡å‹
print("åŠ è½½æ¨¡å‹...")
model = AutoModelForSequenceClassification.from_pretrained("./merged_model")
tokenizer = AutoTokenizer.from_pretrained("./merged_model")
model.eval()

# å¦‚æœæœ‰GPU
if torch.cuda.is_available():
    model = model.cuda()

@app.route('/predict', methods=['POST'])
def predict():
    try:
        # è·å–è¾“å…¥
        data = request.json
        text = data.get('text', '')
        
        # åˆ†è¯
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=512
        )
        
        # æ¨ç†
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = torch.softmax(outputs.logits, dim=-1)
            predicted_class = predictions.argmax(dim=-1).item()
            confidence = predictions[0][predicted_class].item()
        
        # è¿”å›ç»“æœ
        return jsonify({
            'class': predicted_class,
            'confidence': float(confidence),
            'probabilities': predictions[0].tolist()
        })
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### 7.3 ç‹¬ç«‹éƒ¨ç½²ï¼ˆå¤šä»»åŠ¡åˆ‡æ¢ï¼‰

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šLoRAé€‚é…å™¨éƒ¨ç½²
"""

from peft import PeftModel
from transformers import AutoModelForSequenceClassification, AutoTokenizer

class MultiTaskModel:
    def __init__(self, base_model_name):
        # åŠ è½½åŸºç¡€æ¨¡å‹
        self.base_model = AutoModelForSequenceClassification.from_pretrained(
            base_model_name
        )
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        self.current_task = None
        self.model = None
    
    def load_task(self, task_name, lora_path):
        """åŠ è½½ç‰¹å®šä»»åŠ¡çš„LoRAæƒé‡"""
        print(f"åŠ è½½ä»»åŠ¡: {task_name}")
        self.model = PeftModel.from_pretrained(
            self.base_model,
            lora_path,
            adapter_name=task_name
        )
        self.current_task = task_name
    
    def switch_task(self, task_name):
        """åˆ‡æ¢åˆ°å¦ä¸€ä¸ªä»»åŠ¡"""
        if self.model is None:
            raise ValueError("è¯·å…ˆåŠ è½½è‡³å°‘ä¸€ä¸ªä»»åŠ¡")
        self.model.set_adapter(task_name)
        self.current_task = task_name
    
    def predict(self, text):
        """é¢„æµ‹"""
        if self.model is None:
            raise ValueError("è¯·å…ˆåŠ è½½ä»»åŠ¡")
        
        inputs = self.tokenizer(text, return_tensors="pt")
        outputs = self.model(**inputs)
        return outputs.logits.argmax(dim=-1).item()

# ä½¿ç”¨ç¤ºä¾‹
multi_model = MultiTaskModel("bert-base-chinese")

# åŠ è½½å¤šä¸ªä»»åŠ¡
multi_model.load_task("sentiment", "./sentiment_lora")
multi_model.model.load_adapter("./ner_lora", adapter_name="ner")
multi_model.model.load_adapter("./classification_lora", adapter_name="classification")

# åˆ‡æ¢ä»»åŠ¡
multi_model.switch_task("sentiment")
result1 = multi_model.predict("è¿™ä¸ªç”µå½±å¾ˆå¥½çœ‹")

multi_model.switch_task("ner")
result2 = multi_model.predict("å¼ ä¸‰åœ¨åŒ—äº¬å·¥ä½œ")

# ä¼˜åŠ¿ï¼š
# - åªéœ€åŠ è½½ä¸€ä¸ªåŸºç¡€æ¨¡å‹
# - å¯ä»¥å¿«é€Ÿåˆ‡æ¢ä»»åŠ¡ï¼ˆç§’çº§ï¼‰
# - èŠ‚çœå†…å­˜ç©ºé—´
```

### 7.4 é‡åŒ–éƒ¨ç½²

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡åŒ–æ¨¡å‹éƒ¨ç½²
"""

import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# æ–¹æ³•1ï¼šåŠ è½½æ—¶é‡åŒ–
model = AutoModelForSequenceClassification.from_pretrained(
    "./merged_model",
    load_in_8bit=True,  # 8ä½é‡åŒ–
    device_map="auto"
)

# æ–¹æ³•2ï¼šè®­ç»ƒåé‡åŒ–
model = AutoModelForSequenceClassification.from_pretrained("./merged_model")

# åŠ¨æ€é‡åŒ–
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},
    dtype=torch.qint8
)

# ä¿å­˜é‡åŒ–æ¨¡å‹
torch.save(quantized_model.state_dict(), "./quantized_model.pth")

# æ•ˆæœï¼š
# - æ¨¡å‹å¤§å°å‡å°‘75%
# - æ¨ç†é€Ÿåº¦æå‡3-4å€
# - ç²¾åº¦æŸå¤±<1%
```

### 7.5 ONNXéƒ¨ç½²

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è½¬æ¢ä¸ºONNXæ ¼å¼éƒ¨ç½²
"""

import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# 1. åŠ è½½æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained("./merged_model")
tokenizer = AutoTokenizer.from_pretrained("./merged_model")
model.eval()

# 2. å‡†å¤‡ç¤ºä¾‹è¾“å…¥
dummy_input = tokenizer("ç¤ºä¾‹æ–‡æœ¬", return_tensors="pt")

# 3. å¯¼å‡ºä¸ºONNX
torch.onnx.export(
    model,
    tuple(dummy_input.values()),
    "./model.onnx",
    input_names=['input_ids', 'attention_mask'],
    output_names=['logits'],
    dynamic_axes={
        'input_ids': {0: 'batch', 1: 'sequence'},
        'attention_mask': {0: 'batch', 1: 'sequence'},
        'logits': {0: 'batch'}
    },
    opset_version=14
)

# 4. ä½¿ç”¨ONNX Runtimeæ¨ç†
import onnxruntime as ort

session = ort.InferenceSession("./model.onnx")

# æ¨ç†
inputs = tokenizer("æµ‹è¯•æ–‡æœ¬", return_tensors="np")
outputs = session.run(
    None,
    {
        'input_ids': inputs['input_ids'],
        'attention_mask': inputs['attention_mask']
    }
)

# ä¼˜åŠ¿ï¼š
# - è·¨å¹³å°éƒ¨ç½²
# - æ¨ç†é€Ÿåº¦å¿«
# - æ”¯æŒå¤šç§ç¡¬ä»¶åŠ é€Ÿ
```

### 7.6 Dockeréƒ¨ç½²

```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶æ¨¡å‹å’Œä»£ç 
COPY merged_model/ ./model/
COPY app.py .

# æš´éœ²ç«¯å£
EXPOSE 5000

# å¯åŠ¨æœåŠ¡
CMD ["python", "app.py"]
```

```bash
# æ„å»ºé•œåƒ
docker build -t lora-model-api .

# è¿è¡Œå®¹å™¨
docker run -p 5000:5000 --gpus all lora-model-api
```

---

## 8. å¸¸è§é—®é¢˜è§£ç­”

### 8.1 LoRAç›¸å…³

**Q1: LoRAé€‚åˆæ‰€æœ‰æ¨¡å‹å—ï¼Ÿ**

A: ä¸æ˜¯ã€‚LoRAæœ€é€‚åˆï¼š
- Transformeræ¶æ„çš„æ¨¡å‹
- æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹
- é¢„è®­ç»ƒæ¨¡å‹çš„å¾®è°ƒ

ä¸å¤ªé€‚åˆï¼š
- ä»å¤´è®­ç»ƒçš„æ¨¡å‹
- éTransformeræ¶æ„
- æ¨¡å‹ç»“æ„éœ€è¦å¤§å¹…æ”¹å˜çš„åœºæ™¯

**Q2: å¦‚ä½•é€‰æ‹©LoRAçš„ç§©ï¼ˆrï¼‰ï¼Ÿ**

A: ç»éªŒæ³•åˆ™ï¼š
- ç®€å•ä»»åŠ¡ï¼ˆåˆ†ç±»ï¼‰ï¼šr=4-8
- ä¸­ç­‰ä»»åŠ¡ï¼ˆNERã€QAï¼‰ï¼šr=8-16
- å¤æ‚ä»»åŠ¡ï¼ˆç”Ÿæˆï¼‰ï¼šr=16-64
- å¤§æ¨¡å‹ï¼ˆ>7Bï¼‰ï¼šå¯ä»¥ç”¨æ›´å¤§çš„r

å»ºè®®ï¼šä»å°çš„rå¼€å§‹ï¼Œé€æ­¥å¢å¤§ç›´åˆ°æ€§èƒ½ä¸å†æå‡ã€‚

**Q3: LoRAè®­ç»ƒåç²¾åº¦ä¸‹é™æ€ä¹ˆåŠï¼Ÿ**

A: å°è¯•ï¼š
1. å¢å¤§rï¼ˆç§©ï¼‰
2. å¢å¤§lora_alpha
3. æ·»åŠ æ›´å¤štarget_modules
4. è°ƒæ•´å­¦ä¹ ç‡
5. å¢åŠ è®­ç»ƒæ•°æ®
6. ä½¿ç”¨æ›´é•¿çš„è®­ç»ƒæ—¶é—´

**Q4: å¯ä»¥åŒæ—¶ä½¿ç”¨å¤šä¸ªLoRAå—ï¼Ÿ**

A: å¯ä»¥ï¼æœ‰ä¸¤ç§æ–¹å¼ï¼š
1. ä¸²è¡Œï¼šä¸€ä¸ªä»»åŠ¡ä¸€ä¸ªLoRA
2. å¹¶è¡Œï¼šå¤šä¸ªLoRAåŒæ—¶æ¿€æ´»ï¼ˆéœ€è¦åˆå¹¶ï¼‰

**Q5: LoRAæƒé‡å¯ä»¥åˆå¹¶å—ï¼Ÿ**

A: å¯ä»¥ï¼ä½¿ç”¨`merge_and_unload()`æ–¹æ³•ï¼š
```python
model = model.merge_and_unload()
```
åˆå¹¶åå°±æ˜¯ä¸€ä¸ªæ™®é€šæ¨¡å‹ï¼Œå¯ä»¥æ­£å¸¸éƒ¨ç½²ã€‚

### 8.2 é‡åŒ–ç›¸å…³

**Q6: é‡åŒ–ä¼šæŸå¤±å¤šå°‘ç²¾åº¦ï¼Ÿ**

A: é€šå¸¸ï¼š
- FP16ï¼šå‡ ä¹æ— æŸå¤±ï¼ˆ<0.1%ï¼‰
- INT8ï¼šè½»å¾®æŸå¤±ï¼ˆ0.5-1%ï¼‰
- INT4ï¼šå¯æ¥å—æŸå¤±ï¼ˆ1-3%ï¼‰

**Q7: é‡åŒ–åå¯ä»¥ç»§ç»­è®­ç»ƒå—ï¼Ÿ**

A: å¯ä»¥ï¼è¿™å«QLoRAï¼ˆQuantized LoRAï¼‰ï¼š
```python
# 4ä½é‡åŒ– + LoRAè®­ç»ƒ
model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    load_in_4bit=True
)
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)
```

**Q8: INT8å’ŒINT4é‡åŒ–å¦‚ä½•é€‰æ‹©ï¼Ÿ**

A: 
- INT8ï¼šç²¾åº¦è¦æ±‚é«˜ï¼Œæœ‰ä¸€å®šæ˜¾å­˜
- INT4ï¼šæ˜¾å­˜æåº¦å—é™ï¼Œå¯æ¥å—ç²¾åº¦æŸå¤±

### 8.3 è®­ç»ƒç›¸å…³

**Q9: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ**

A: å¤šç§æ–¹æ³•ï¼š
1. ä½¿ç”¨é‡åŒ–ï¼ˆ4ä½/8ä½ï¼‰
2. å‡å°batch size
3. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
4. ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
5. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
6. ä½¿ç”¨LoRAï¼ˆæœ¬èº«å°±çœæ˜¾å­˜ï¼‰

**Q10: è®­ç»ƒé€Ÿåº¦æ…¢æ€ä¹ˆåŠï¼Ÿ**

A: ä¼˜åŒ–æ–¹æ³•ï¼š
1. ä½¿ç”¨æ··åˆç²¾åº¦ï¼ˆFP16/BF16ï¼‰
2. å¢å¤§batch size
3. ä½¿ç”¨å¤šGPUè®­ç»ƒ
4. ä½¿ç”¨æ›´å¿«çš„ä¼˜åŒ–å™¨ï¼ˆAdamWï¼‰
5. å‡å°‘loggingé¢‘ç‡

### 8.4 éƒ¨ç½²ç›¸å…³

**Q11: å¦‚ä½•é€‰æ‹©éƒ¨ç½²æ–¹å¼ï¼Ÿ**

A: æ ¹æ®åœºæ™¯ï¼š
- å•ä»»åŠ¡ç”Ÿäº§ï¼šåˆå¹¶éƒ¨ç½²
- å¤šä»»åŠ¡åˆ‡æ¢ï¼šç‹¬ç«‹éƒ¨ç½²
- èµ„æºå—é™ï¼šé‡åŒ–éƒ¨ç½²
- è·¨å¹³å°ï¼šONNXéƒ¨ç½²

**Q12: æ¨ç†é€Ÿåº¦æ…¢æ€ä¹ˆåŠï¼Ÿ**

A: ä¼˜åŒ–æ–¹æ³•ï¼š
1. ä½¿ç”¨é‡åŒ–æ¨¡å‹
2. ä½¿ç”¨ONNX Runtime
3. æ‰¹é‡æ¨ç†
4. ä½¿ç”¨GPU
5. æ¨¡å‹è’¸é¦

---

## 9. æ€»ç»“

### 9.1 LoRAçš„æ ¸å¿ƒä»·å€¼

1. **å‚æ•°æ•ˆç‡**: åªéœ€è®­ç»ƒ0.1%-1%çš„å‚æ•°
2. **æ˜¾å­˜å‹å¥½**: æ˜¾å­˜éœ€æ±‚å‡å°‘60%-80%
3. **è®­ç»ƒå¿«é€Ÿ**: è®­ç»ƒæ—¶é—´å‡å°‘50%-70%
4. **å­˜å‚¨èŠ‚çœ**: LoRAæƒé‡åªæœ‰å‡ MB
5. **çµæ´»åˆ‡æ¢**: å¯ä»¥å¿«é€Ÿåˆ‡æ¢ä¸åŒä»»åŠ¡
6. **ç²¾åº¦ä¿æŒ**: æ€§èƒ½æ¥è¿‘å…¨é‡å¾®è°ƒ

### 9.2 æœ€ä½³å®è·µ

1. **æ•°æ®å‡†å¤‡**: é«˜è´¨é‡æ•°æ® > å¤§é‡æ•°æ®
2. **å‚æ•°é€‰æ‹©**: ä»å°çš„rå¼€å§‹ï¼Œé€æ­¥è°ƒæ•´
3. **è®­ç»ƒç›‘æ§**: å¯†åˆ‡å…³æ³¨éªŒè¯é›†æ€§èƒ½
4. **æ—©åœç­–ç•¥**: é¿å…è¿‡æ‹Ÿåˆ
5. **ç»„åˆä¼˜åŒ–**: LoRA + é‡åŒ– = æœ€ä½³æ•ˆç‡
6. **éƒ¨ç½²é€‰æ‹©**: æ ¹æ®åœºæ™¯é€‰æ‹©åˆé€‚æ–¹å¼

### 9.3 é€‚ç”¨åœºæ™¯æ€»ç»“

| åœºæ™¯ | æ¨èæ–¹æ¡ˆ | é¢„æœŸæ•ˆæœ |
|------|---------|---------|
| æ¶ˆè´¹çº§GPUè®­ç»ƒå¤§æ¨¡å‹ | LoRA + 4ä½é‡åŒ– | 7Bæ¨¡å‹åªéœ€4-6GB |
| å¤šä»»åŠ¡å¿«é€Ÿåˆ‡æ¢ | ç‹¬ç«‹LoRAéƒ¨ç½² | ç§’çº§åˆ‡æ¢ |
| èµ„æºå—é™æ¨ç† | é‡åŒ– + è’¸é¦ | æ¨¡å‹å‡å°‘90% |
| ç‰¹å®šé¢†åŸŸé€‚é… | LoRAå¾®è°ƒ | å¿«é€Ÿé€‚é… |
| ç”Ÿäº§ç¯å¢ƒéƒ¨ç½² | åˆå¹¶ + é‡åŒ– | ç¨³å®šé«˜æ•ˆ |

### 9.4 æœªæ¥å±•æœ›

- **æ›´é«˜æ•ˆçš„LoRAå˜ä½“**: QLoRAã€AdaLoRAç­‰
- **è‡ªåŠ¨åŒ–å‚æ•°æœç´¢**: è‡ªåŠ¨æ‰¾åˆ°æœ€ä¼˜rå’Œalpha
- **å¤šæ¨¡æ€LoRA**: ç»Ÿä¸€çš„å¤šæ¨¡æ€å¾®è°ƒæ¡†æ¶
- **ç¡¬ä»¶ä¼˜åŒ–**: ä¸“é—¨çš„LoRAæ¨ç†åŠ é€Ÿ

---

## 10. å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£
- [PEFTåº“æ–‡æ¡£](https://huggingface.co/docs/peft)
- [Transformersæ–‡æ¡£](https://huggingface.co/docs/transformers)
- [LoRAè®ºæ–‡](https://arxiv.org/abs/2106.09685)

### å®ç”¨å·¥å…·
- [PEFTåº“](https://github.com/huggingface/peft)
- [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)
- [ONNX Runtime](https://onnxruntime.ai/)

### å­¦ä¹ èµ„æº
- [Hugging Faceè¯¾ç¨‹](https://huggingface.co/course)
- [LoRAæ•™ç¨‹](https://huggingface.co/blog/lora)
- [æ¨¡å‹ä¼˜åŒ–æŒ‡å—](https://huggingface.co/docs/optimum)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2026-02-01  
**ä½œè€…**: Transformerså®æˆ˜è®­ç»ƒé¡¹ç›®

