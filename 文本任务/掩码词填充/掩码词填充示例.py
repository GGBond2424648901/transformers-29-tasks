#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ©ç è¯å¡«å……ï¼ˆFill-Maskï¼‰å®æˆ˜ç¤ºä¾‹
ä½¿ç”¨ BERT ç­‰æ¨¡å‹è¿›è¡Œæ©ç è¯é¢„æµ‹
"""

import os

# è®¾ç½®æ¨¡å‹ç¼“å­˜è·¯å¾„
os.environ['HF_HOME'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'
os.environ['TRANSFORMERS_CACHE'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'

from transformers import pipeline

print("=" * 70)
print("ğŸ­ æ©ç è¯å¡«å……å®æˆ˜ç¤ºä¾‹")
print("=" * 70)
print(f"ğŸ“ æ¨¡å‹ç¼“å­˜è·¯å¾„: {os.environ['HF_HOME']}")
print("=" * 70)

# 1. åˆ›å»ºæ©ç è¯å¡«å…… pipeline
print("\nğŸ“¦ æ­¥éª¤ 1: åŠ è½½æ¨¡å‹")
print("-" * 70)

# ä½¿ç”¨ä¸­æ–‡ BERT æ¨¡å‹
unmasker = pipeline(
    "fill-mask",
    model="bert-base-chinese"
)

print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
print(f"   æ¨¡å‹: bert-base-chinese")
print(f"   ä»»åŠ¡: æ©ç è¯å¡«å……")

# 2. åŸºç¡€å¡«å……ç¤ºä¾‹
print("\nğŸ” æ­¥éª¤ 2: åŸºç¡€æ©ç è¯å¡«å……")
print("-" * 70)

text = "ä»Šå¤©å¤©æ°”çœŸ[MASK]ï¼"
print(f"è¾“å…¥æ–‡æœ¬: {text}")

results = unmasker(text)

print("\nâœ… å¡«å……å®Œæˆï¼")
print(f"\nğŸ“Š é¢„æµ‹ç»“æœï¼ˆTop 5ï¼‰:")
for i, result in enumerate(results, 1):
    print(f"   {i}. {result['token_str']:<10} ç½®ä¿¡åº¦: {result['score']:.2%}")
    print(f"      å®Œæ•´å¥å­: {result['sequence']}")

# 3. å¤šä¸ªæ©ç è¯
print("\n" + "=" * 70)
print("ğŸ“ æ­¥éª¤ 3: å¤šä¸ªæ©ç è¯å¡«å……")
print("=" * 70)

sentences = [
    "æˆ‘å–œæ¬¢åƒ[MASK]ã€‚",
    "ä»–æ˜¯ä¸€ä½[MASK]çš„ç§‘å­¦å®¶ã€‚",
    "è¿™æœ¬ä¹¦éå¸¸[MASK]ã€‚",
    "åŒ—äº¬æ˜¯ä¸­å›½çš„[MASK]ã€‚"
]

for sentence in sentences:
    print(f"\nè¾“å…¥: {sentence}")
    results = unmasker(sentence, top_k=3)
    
    print("é¢„æµ‹:")
    for i, result in enumerate(results, 1):
        print(f"   {i}. {result['token_str']:<8} ({result['score']:.2%}) - {result['sequence']}")

# 4. å¥å­è¡¥å…¨
print("\n" + "=" * 70)
print("âœï¸  æ­¥éª¤ 4: å¥å­è¡¥å…¨")
print("=" * 70)

incomplete_sentences = [
    "äººå·¥æ™ºèƒ½æ˜¯[MASK]çš„æœªæ¥ã€‚",
    "å­¦ä¹ ç¼–ç¨‹éœ€è¦[MASK]å’Œè€å¿ƒã€‚",
    "å¥åº·çš„ç”Ÿæ´»æ–¹å¼åŒ…æ‹¬[MASK]å’Œè¿åŠ¨ã€‚"
]

print("å¥å­è¡¥å…¨ç¤ºä¾‹ï¼š\n")

for sentence in incomplete_sentences:
    results = unmasker(sentence, top_k=1)
    completed = results[0]['sequence']
    word = results[0]['token_str']
    
    print(f"åŸå¥: {sentence}")
    print(f"è¡¥å…¨: {completed}")
    print(f"å¡«å…¥è¯: {word} (ç½®ä¿¡åº¦: {results[0]['score']:.2%})\n")

# 5. æ–‡æœ¬çº é”™
print("=" * 70)
print("ğŸ”§ æ­¥éª¤ 5: æ–‡æœ¬çº é”™ï¼ˆå®éªŒæ€§ï¼‰")
print("=" * 70)

# å°†å¯èƒ½é”™è¯¯çš„è¯æ›¿æ¢ä¸º [MASK]ï¼Œè®©æ¨¡å‹é¢„æµ‹æ­£ç¡®çš„è¯
error_texts = [
    ("æˆ‘[MASK]å¤©å»äº†å…¬å›­ã€‚", "æ˜¨"),  # æ­£ç¡®åº”è¯¥æ˜¯"æ˜¨"
    ("è¿™ä¸ªé—®é¢˜å¾ˆ[MASK]å•ã€‚", "ç®€"),  # æ­£ç¡®åº”è¯¥æ˜¯"ç®€"
]

print("æ–‡æœ¬çº é”™ç¤ºä¾‹ï¼š\n")

for text, expected in error_texts:
    results = unmasker(text, top_k=3)
    
    print(f"è¾“å…¥: {text}")
    print(f"æœŸæœ›: {expected}")
    print("é¢„æµ‹:")
    for i, result in enumerate(results, 1):
        is_correct = "âœ…" if result['token_str'] == expected else "  "
        print(f"   {is_correct} {i}. {result['token_str']:<8} ({result['score']:.2%})")
    print()

# 6. ä½¿ç”¨æŠ€å·§
print("=" * 70)
print("ğŸ’¡ ä½¿ç”¨æŠ€å·§")
print("=" * 70)
print("""
æ©ç è¯å¡«å……çš„ä½¿ç”¨æŠ€å·§ï¼š

1. æ©ç æ ‡è®°ï¼š
   - BERT ä¸­æ–‡: [MASK]
   - BERT è‹±æ–‡: [MASK]
   - RoBERTa: <mask>
   - ä¸åŒæ¨¡å‹ä½¿ç”¨ä¸åŒçš„æ©ç æ ‡è®°

2. æ§åˆ¶è¾“å‡ºæ•°é‡ï¼š
   results = unmasker(text, top_k=10)  # è¿”å›å‰ 10 ä¸ªé¢„æµ‹

3. å¤šä¸ªæ©ç ï¼š
   - ä¸€æ¬¡åªèƒ½å¡«å……ä¸€ä¸ª [MASK]
   - å¤šä¸ªæ©ç éœ€è¦åˆ†åˆ«å¤„ç†

4. ä¸Šä¸‹æ–‡å¾ˆé‡è¦ï¼š
   - æä¾›è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
   - ä¸Šä¸‹æ–‡è¶Šä¸°å¯Œï¼Œé¢„æµ‹è¶Šå‡†ç¡®

ç¤ºä¾‹ä»£ç ï¼š

# æŒ‡å®šè¿”å›æ•°é‡
results = unmasker("ä»Šå¤©å¤©æ°”çœŸ[MASK]ï¼", top_k=10)

# è·å–æœ€ä½³é¢„æµ‹
best_prediction = results[0]['token_str']

# è·å–å®Œæ•´å¥å­
completed_sentence = results[0]['sequence']
""")

# 7. åº”ç”¨åœºæ™¯
print("\n" + "=" * 70)
print("ğŸ¯ åº”ç”¨åœºæ™¯")
print("=" * 70)
print("""
æ©ç è¯å¡«å……çš„ä¸»è¦åº”ç”¨ï¼š

1. ğŸ“ æ™ºèƒ½è¾“å…¥æ³•
   - è¯è¯­è”æƒ³
   - è‡ªåŠ¨è¡¥å…¨
   - è¾“å…¥å»ºè®®

2. ğŸ”§ æ–‡æœ¬çº é”™
   - æ‹¼å†™æ£€æŸ¥
   - è¯­æ³•çº æ­£
   - é”™åˆ«å­—ä¿®æ­£

3. ğŸ“š è¯­è¨€å­¦ä¹ 
   - å¡«ç©ºç»ƒä¹ 
   - è¯æ±‡æµ‹è¯•
   - è¯­å¢ƒç†è§£

4. ğŸ¤– å¯¹è¯ç³»ç»Ÿ
   - å¥å­è¡¥å…¨
   - æ„å›¾ç†è§£
   - ä¸Šä¸‹æ–‡æ¨ç†

5. ğŸ“Š æ•°æ®å¢å¼º
   - ç”Ÿæˆç›¸ä¼¼å¥å­
   - æ‰©å……è®­ç»ƒæ•°æ®
   - åŒä¹‰è¯æ›¿æ¢
""")

# 8. æ¨¡å‹æ¨è
print("\n" + "=" * 70)
print("ğŸ¨ æ¨¡å‹æ¨è")
print("=" * 70)
print("""
ä¸­æ–‡æ¨¡å‹ï¼š
- bert-base-chinese: é€šç”¨ä¸­æ–‡ BERT
- hfl/chinese-roberta-wwm-ext: ä¸­æ–‡ RoBERTaï¼ˆæ•ˆæœæ›´å¥½ï¼‰
- hfl/chinese-bert-wwm-ext: ä¸­æ–‡ BERT WWM

è‹±æ–‡æ¨¡å‹ï¼š
- bert-base-uncased: é€šç”¨è‹±æ–‡ BERT
- roberta-base: è‹±æ–‡ RoBERTa
- albert-base-v2: è½»é‡çº§ ALBERT

ä½¿ç”¨æ–¹æ³•ï¼š
unmasker = pipeline("fill-mask", model="hfl/chinese-roberta-wwm-ext")
""")

# 9. æ€§èƒ½å¯¹æ¯”
print("\n" + "=" * 70)
print("âš¡ ä¸åŒæ¨¡å‹æ€§èƒ½å¯¹æ¯”")
print("=" * 70)

test_sentence = "æˆ‘å–œæ¬¢[MASK]ç¼–ç¨‹ã€‚"

models = [
    "bert-base-chinese",
    # "hfl/chinese-roberta-wwm-ext",  # å–æ¶ˆæ³¨é‡Šä»¥æµ‹è¯•
]

print(f"æµ‹è¯•å¥å­: {test_sentence}\n")

for model_name in models:
    print(f"æ¨¡å‹: {model_name}")
    try:
        temp_unmasker = pipeline("fill-mask", model=model_name)
        results = temp_unmasker(test_sentence, top_k=3)
        
        for i, result in enumerate(results, 1):
            print(f"   {i}. {result['token_str']:<8} ({result['score']:.2%})")
    except Exception as e:
        print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
    print()

print("=" * 70)
print("âœ¨ ç¤ºä¾‹å®Œæˆï¼")
print("=" * 70)
