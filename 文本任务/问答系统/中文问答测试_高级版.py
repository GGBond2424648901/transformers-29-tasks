#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸­æ–‡é—®ç­”ç³»ç»Ÿæµ‹è¯• - é«˜çº§ç‰ˆ
æµ‹è¯•ä½¿ç”¨å®Œæ•´æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹
"""

import os
os.environ['HF_HOME'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'
os.environ['TRANSFORMERS_CACHE'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'

from transformers import pipeline
import torch

print("=" * 70)
print("â“ ä¸­æ–‡é—®ç­”ç³»ç»Ÿæµ‹è¯• - é«˜çº§ç‰ˆ")
print("=" * 70)

# æ£€æŸ¥è®¾å¤‡
device = 0 if torch.cuda.is_available() else -1
device_name = "GPU" if device == 0 else "CPU"
print(f"\nğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device_name}")

# åŠ è½½æ¨¡å‹
print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")

# ä¼˜å…ˆä½¿ç”¨é«˜çº§ç‰ˆæ¨¡å‹
model_path = "ä¸­æ–‡é—®ç­”æ¨¡å‹_é«˜çº§ç‰ˆ"

if not os.path.exists(model_path):
    print(f"âš ï¸  æœªæ‰¾åˆ°é«˜çº§ç‰ˆæ¨¡å‹: {model_path}")
    model_path = "ä¸­æ–‡é—®ç­”æ¨¡å‹"
    if not os.path.exists(model_path):
        print("âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹")
        model_path = "hfl/chinese-roberta-wwm-ext"

qa_pipeline = pipeline(
    "question-answering",
    model=model_path,
    device=device
)

print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")

# æµ‹è¯•ä¸­æ–‡é—®ç­”
print("\n" + "=" * 70)
print("ğŸ§ª æµ‹è¯•ä¸­æ–‡é—®ç­”")
print("=" * 70)

# ç¤ºä¾‹ 1: åœ°ç†çŸ¥è¯†
context1 = """
åŒ—äº¬æ˜¯ä¸­åäººæ°‘å…±å’Œå›½çš„é¦–éƒ½ï¼Œæ˜¯å…¨å›½çš„æ”¿æ²»ä¸­å¿ƒã€æ–‡åŒ–ä¸­å¿ƒã€‚åŒ—äº¬ä½äºååŒ—å¹³åŸåŒ—éƒ¨ï¼Œ
èƒŒé ç‡•å±±ï¼Œæ¯—é‚»å¤©æ´¥å¸‚å’Œæ²³åŒ—çœã€‚åŒ—äº¬æœ‰ç€3000ä½™å¹´çš„å»ºåŸå²å’Œ850ä½™å¹´çš„å»ºéƒ½å²ï¼Œ
æ˜¯ä¸–ç•Œä¸Šæ‹¥æœ‰ä¸–ç•Œæ–‡åŒ–é—äº§æ•°æœ€å¤šçš„åŸå¸‚ã€‚åŒ—äº¬å¯¹å¤–å¼€æ”¾çš„æ—…æ¸¸æ™¯ç‚¹è¾¾200å¤šå¤„ï¼Œ
æœ‰ä¸–ç•Œä¸Šæœ€å¤§çš„çš‡å®«ç´«ç¦åŸã€ç¥­å¤©ç¥åº™å¤©å›ã€çš‡å®¶å›­æ—åŒ—æµ·å…¬å›­ã€é¢å’Œå›­å’Œåœ†æ˜å›­ï¼Œ
è¿˜æœ‰å…«è¾¾å²­é•¿åŸã€æ…•ç”°å³ªé•¿åŸä»¥åŠä¸–ç•Œä¸Šæœ€å¤§çš„å››åˆé™¢æ­ç‹åºœç­‰åèƒœå¤è¿¹ã€‚
"""

questions1 = [
    "åŒ—äº¬æ˜¯ä»€ä¹ˆï¼Ÿ",
    "åŒ—äº¬ä½äºå“ªé‡Œï¼Ÿ",
    "åŒ—äº¬æœ‰å¤šå°‘å¹´çš„å»ºéƒ½å²ï¼Ÿ",
    "åŒ—äº¬æœ‰å“ªäº›è‘—åæ™¯ç‚¹ï¼Ÿ"
]

print(f"\nğŸ“„ ä¸Šä¸‹æ–‡ 1: åŒ—äº¬ä»‹ç»")
print(f"{context1.strip()[:100]}...\n")

for i, question in enumerate(questions1, 1):
    result = qa_pipeline(question=question, context=context1)
    
    # æ ¹æ®ç½®ä¿¡åº¦æ˜¾ç¤ºä¸åŒçš„æ ‡è®°
    confidence = result['score']
    if confidence > 0.5:
        mark = "âœ…"
    elif confidence > 0.2:
        mark = "âš ï¸"
    else:
        mark = "âŒ"
    
    print(f"{mark} {i}. é—®é¢˜: {question}")
    print(f"   ç­”æ¡ˆ: {result['answer']}")
    print(f"   ç½®ä¿¡åº¦: {result['score']:.2%}\n")

# ç¤ºä¾‹ 2: ç§‘æŠ€çŸ¥è¯†
context2 = """
äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼Œç®€ç§°AIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œ
å®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚
è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚
äººå·¥æ™ºèƒ½ä»è¯ç”Ÿä»¥æ¥ï¼Œç†è®ºå’ŒæŠ€æœ¯æ—¥ç›Šæˆç†Ÿï¼Œåº”ç”¨é¢†åŸŸä¹Ÿä¸æ–­æ‰©å¤§ã€‚å¯ä»¥è®¾æƒ³ï¼Œ
æœªæ¥äººå·¥æ™ºèƒ½å¸¦æ¥çš„ç§‘æŠ€äº§å“ï¼Œå°†ä¼šæ˜¯äººç±»æ™ºæ…§çš„"å®¹å™¨"ã€‚äººå·¥æ™ºèƒ½å¯ä»¥å¯¹äººçš„æ„è¯†ã€
æ€ç»´çš„ä¿¡æ¯è¿‡ç¨‹è¿›è¡Œæ¨¡æ‹Ÿã€‚äººå·¥æ™ºèƒ½ä¸æ˜¯äººçš„æ™ºèƒ½ï¼Œä½†èƒ½åƒäººé‚£æ ·æ€è€ƒã€ä¹Ÿå¯èƒ½è¶…è¿‡äººçš„æ™ºèƒ½ã€‚
"""

questions2 = [
    "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    "äººå·¥æ™ºèƒ½çš„ç ”ç©¶åŒ…æ‹¬å“ªäº›é¢†åŸŸï¼Ÿ",
    "äººå·¥æ™ºèƒ½èƒ½è¶…è¿‡äººçš„æ™ºèƒ½å—ï¼Ÿ"
]

print("\n" + "=" * 70)
print(f"ğŸ“„ ä¸Šä¸‹æ–‡ 2: äººå·¥æ™ºèƒ½ä»‹ç»")
print(f"{context2.strip()[:100]}...\n")

for i, question in enumerate(questions2, 1):
    result = qa_pipeline(question=question, context=context2)
    
    confidence = result['score']
    if confidence > 0.5:
        mark = "âœ…"
    elif confidence > 0.2:
        mark = "âš ï¸"
    else:
        mark = "âŒ"
    
    print(f"{mark} {i}. é—®é¢˜: {question}")
    print(f"   ç­”æ¡ˆ: {result['answer']}")
    print(f"   ç½®ä¿¡åº¦: {result['score']:.2%}\n")

# ç¤ºä¾‹ 3: å†å²çŸ¥è¯†
context3 = """
é•¿åŸæ˜¯ä¸­å›½å¤ä»£çš„å†›äº‹é˜²å¾¡å·¥ç¨‹ï¼Œæ˜¯ä¸€é“é«˜å¤§ã€åšå›ºè€Œè¿ç»µä¸æ–­çš„é•¿å£ï¼Œ
ç”¨ä»¥é™éš”æ•Œéª‘çš„è¡ŒåŠ¨ã€‚é•¿åŸä¸æ˜¯ä¸€é“å•çº¯å­¤ç«‹çš„åŸå¢™ï¼Œè€Œæ˜¯ä»¥åŸå¢™ä¸ºä¸»ä½“ï¼Œ
åŒå¤§é‡çš„åŸã€éšœã€äº­ã€æ ‡ç›¸ç»“åˆçš„é˜²å¾¡ä½“ç³»ã€‚é•¿åŸä¿®ç­‘çš„å†å²å¯ä¸Šæº¯åˆ°è¥¿å‘¨æ—¶æœŸï¼Œ
å‘ç”Ÿåœ¨é¦–éƒ½é•äº¬çš„è‘—åå…¸æ•…"çƒ½ç«æˆè¯¸ä¾¯"å°±æºäºæ­¤ã€‚æ˜¥ç§‹æˆ˜å›½æ—¶æœŸåˆ—å›½äº‰éœ¸ï¼Œ
äº’ç›¸é˜²å®ˆï¼Œé•¿åŸä¿®ç­‘è¿›å…¥ç¬¬ä¸€ä¸ªé«˜æ½®ï¼Œä½†æ­¤æ—¶ä¿®ç­‘çš„é•¿åº¦éƒ½æ¯”è¾ƒçŸ­ã€‚
ç§¦ç­å…­å›½ç»Ÿä¸€å¤©ä¸‹åï¼Œç§¦å§‹çš‡è¿æ¥å’Œä¿®ç¼®æˆ˜å›½é•¿åŸï¼Œå§‹æœ‰ä¸‡é‡Œé•¿åŸä¹‹ç§°ã€‚
"""

questions3 = [
    "é•¿åŸæ˜¯ä»€ä¹ˆï¼Ÿ",
    "é•¿åŸçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ",
    "è°ä¿®å»ºäº†ä¸‡é‡Œé•¿åŸï¼Ÿ"
]

print("\n" + "=" * 70)
print(f"ğŸ“„ ä¸Šä¸‹æ–‡ 3: é•¿åŸä»‹ç»")
print(f"{context3.strip()[:100]}...\n")

for i, question in enumerate(questions3, 1):
    result = qa_pipeline(question=question, context=context3)
    
    confidence = result['score']
    if confidence > 0.5:
        mark = "âœ…"
    elif confidence > 0.2:
        mark = "âš ï¸"
    else:
        mark = "âŒ"
    
    print(f"{mark} {i}. é—®é¢˜: {question}")
    print(f"   ç­”æ¡ˆ: {result['answer']}")
    print(f"   ç½®ä¿¡åº¦: {result['score']:.2%}\n")

# ç¤ºä¾‹ 4: æ›´å¤æ‚çš„é—®é¢˜
context4 = """
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒåŸºäºäººå·¥ç¥ç»ç½‘ç»œçš„ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨å¤šå±‚æ¬¡çš„ç¥ç»ç½‘ç»œæ¥è¿›è¡Œå­¦ä¹ å’Œæ¨¡å¼è¯†åˆ«ã€‚
æ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ•°æ®çš„å¤šå±‚æ¬¡è¡¨ç¤ºï¼Œåœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
2012å¹´ï¼Œæ·±åº¦å­¦ä¹ åœ¨ImageNetå›¾åƒè¯†åˆ«ç«èµ›ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œé”™è¯¯ç‡å¤§å¹…é™ä½ï¼Œä»æ­¤æ·±åº¦å­¦ä¹ å¼€å§‹åœ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œå¹¿æ³›åº”ç”¨ã€‚
ç›®å‰ï¼Œæ·±åº¦å­¦ä¹ å·²ç»æˆä¸ºäººå·¥æ™ºèƒ½é¢†åŸŸæœ€é‡è¦çš„æŠ€æœ¯ä¹‹ä¸€ï¼Œæ¨åŠ¨äº†è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ã€æ™ºèƒ½åŠ©æ‰‹ç­‰åº”ç”¨çš„å‘å±•ã€‚
"""

questions4 = [
    "æ·±åº¦å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ",
    "æ·±åº¦å­¦ä¹ åœ¨å“ªä¸€å¹´å–å¾—çªç ´ï¼Ÿ",
    "æ·±åº¦å­¦ä¹ æœ‰å“ªäº›åº”ç”¨ï¼Ÿ"
]

print("\n" + "=" * 70)
print(f"ğŸ“„ ä¸Šä¸‹æ–‡ 4: æ·±åº¦å­¦ä¹ ä»‹ç»")
print(f"{context4.strip()[:100]}...\n")

for i, question in enumerate(questions4, 1):
    result = qa_pipeline(question=question, context=context4)
    
    confidence = result['score']
    if confidence > 0.5:
        mark = "âœ…"
    elif confidence > 0.2:
        mark = "âš ï¸"
    else:
        mark = "âŒ"
    
    print(f"{mark} {i}. é—®é¢˜: {question}")
    print(f"   ç­”æ¡ˆ: {result['answer']}")
    print(f"   ç½®ä¿¡åº¦: {result['score']:.2%}\n")

# æ€§èƒ½ç»Ÿè®¡
print("\n" + "=" * 70)
print("ğŸ“Š ç½®ä¿¡åº¦è¯´æ˜")
print("=" * 70)
print("""
âœ… é«˜ç½®ä¿¡åº¦ (>50%): æ¨¡å‹éå¸¸ç¡®å®šç­”æ¡ˆæ­£ç¡®
âš ï¸  ä¸­ç­‰ç½®ä¿¡åº¦ (20%-50%): æ¨¡å‹æ‰¾åˆ°äº†ç›¸å…³ç­”æ¡ˆï¼Œä½†ä¸å¤ªç¡®å®š
âŒ ä½ç½®ä¿¡åº¦ (<20%): æ¨¡å‹ä¸ç¡®å®šç­”æ¡ˆï¼Œå¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒ

ğŸ’¡ æç¤ºï¼š
- å¦‚æœå¤§éƒ¨åˆ†ç­”æ¡ˆç½®ä¿¡åº¦ >50%ï¼Œè¯´æ˜æ¨¡å‹è®­ç»ƒæ•ˆæœå¾ˆå¥½
- å¦‚æœç½®ä¿¡åº¦æ™®éè¾ƒä½ï¼Œå»ºè®®ä½¿ç”¨æ›´å¤šæ•°æ®é‡æ–°è®­ç»ƒ
- é«˜çº§ç‰ˆæ¨¡å‹ä½¿ç”¨å®Œæ•´æ•°æ®é›†ï¼Œç½®ä¿¡åº¦åº”è¯¥æ˜æ˜¾æé«˜
""")

# äº¤äº’å¼é—®ç­”
print("\n" + "=" * 70)
print("ğŸ’¬ ä½¿ç”¨æ¨¡å‹çš„ä»£ç ç¤ºä¾‹")
print("=" * 70)
print(f"""
from transformers import pipeline

qa = pipeline("question-answering", model="{model_path}")

context = "ä½ çš„ä¸Šä¸‹æ–‡..."
question = "ä½ çš„é—®é¢˜ï¼Ÿ"

result = qa(question=question, context=context)
print(f"ç­”æ¡ˆ: {{result['answer']}}")
print(f"ç½®ä¿¡åº¦: {{result['score']:.2%}}")
""")

print("\n" + "=" * 70)
print("âœ¨ æµ‹è¯•å®Œæˆï¼")
print("=" * 70)
