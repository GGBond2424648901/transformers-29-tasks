# 🚀 中文问答系统训练指南

## ✅ 准备工作已完成

所有文件已准备就绪，可以直接开始训练！

### 📁 文件清单

```
问答系统/
├── 简单训练示例.py      ✅ 中文问答训练脚本（已修复）
├── 开始训练.bat          ✅ 一键启动训练
├── 中文问答测试.py       ✅ 测试训练好的模型
├── 快速开始.md           ✅ 详细使用指南
└── run_qa.py             ✅ 官方完整训练脚本
```

---

## 🎯 开始训练（3种方法）

### 方法 1：双击批处理文件（最简单）

1. 找到文件：`开始训练.bat`
2. 双击运行
3. 等待训练完成（约 5-10 分钟）

### 方法 2：命令行运行

```bash
cd D:\transformers训练\transformers-main\实战训练\文本任务\问答系统
D:\aaaalokda\envs\myenv\python.exe 简单训练示例.py
```

### 方法 3：在 IDE 中运行

直接在 VS Code 或 PyCharm 中打开 `简单训练示例.py` 并运行

---

## 📊 训练过程说明

### 训练配置

- **模型**: bert-base-chinese（中文 BERT）
- **数据集**: CMRC2018 中文问答数据集（500条训练，50条验证）
- **训练轮数**: 2 epochs
- **批次大小**: 8（GPU）
- **学习率**: 3e-5
- **混合精度**: FP16（GPU 加速）

### 预期输出

```
======================================================================
❓ 问答系统训练示例
======================================================================

📦 步骤 1: 加载数据集
----------------------------------------------------------------------
正在下载 CMRC2018 中文数据集...
✅ 数据集下载成功
✅ 训练集大小: 500
✅ 验证集大小: 50

📊 数据示例:
上下文: 北京是中华人民共和国的首都...
问题: 北京是什么？
答案: {'text': ['中华人民共和国的首都'], 'answer_start': [3]}

📦 步骤 2: 加载模型
----------------------------------------------------------------------
✅ 模型加载成功: bert-base-chinese

📦 步骤 3: 数据预处理
----------------------------------------------------------------------
✅ 数据预处理完成

📦 步骤 4: 配置训练参数
----------------------------------------------------------------------
✅ 训练参数:
   批次大小: 8
   学习率: 3e-5
   训练轮数: 2

📦 步骤 5: 创建 Trainer
----------------------------------------------------------------------
✅ Trainer 创建成功

======================================================================
🚀 开始训练
======================================================================

Epoch 1/2: [训练进度条]
Epoch 2/2: [训练进度条]

======================================================================
✅ 训练完成！
======================================================================

📦 保存模型...
✅ 模型已保存到: ./中文问答模型

📊 评估模型...

评估结果:
  eval_loss: 1.2345
  eval_runtime: 10.5s
  ...

======================================================================
✨ 示例完成！
======================================================================
```

---

## 🧪 测试训练好的模型

训练完成后，运行测试脚本：

```bash
D:\aaaalokda\envs\myenv\python.exe 中文问答测试.py
```

### 测试示例

脚本会自动测试 3 个场景：

1. **地理知识**：关于北京的问答
2. **科技知识**：关于人工智能的问答
3. **历史知识**：关于长城的问答

### 预期输出

```
======================================================================
❓ 中文问答系统测试
======================================================================

📦 加载模型...
✅ 模型加载成功: ./中文问答模型

======================================================================
🧪 测试中文问答
======================================================================

📄 上下文 1: 北京介绍

1. 问题: 北京是什么？
   答案: 中华人民共和国的首都
   置信度: 95.23%

2. 问题: 北京位于哪里？
   答案: 华北平原北部
   置信度: 88.45%

...
```

---

## 💡 常见问题

### 1. 数据集下载失败

**不用担心！** 脚本会自动使用内置的示例数据进行训练。

### 2. 内存不足

修改 `简单训练示例.py` 中的批次大小：

```python
per_device_train_batch_size=4,  # 从 8 改为 4
```

### 3. 没有 GPU

移除 FP16 参数：

```python
# fp16=True,  # 注释掉这行
```

训练时间会增加，但仍然可以完成。

### 4. 想使用更好的模型

修改模型名称：

```python
model_name = "hfl/chinese-roberta-wwm-ext"  # 中文 RoBERTa（效果更好）
# model_name = "hfl/chinese-bert-wwm-ext"   # 中文 BERT WWM
```

---

## 📈 训练时间估算

| 硬件配置 | 预计时间 |
|---------|---------|
| RTX 3070 (8GB) + FP16 | 5-8 分钟 |
| RTX 3070 (8GB) 无 FP16 | 10-15 分钟 |
| CPU (16核) | 30-60 分钟 |

---

## 🎯 使用训练好的模型

### Python 代码

```python
import os
os.environ['HF_HOME'] = r'D:\transformers训练\transformers-main\预训练模型下载处'
os.environ['TRANSFORMERS_CACHE'] = r'D:\transformers训练\transformers-main\预训练模型下载处'

from transformers import pipeline

# 加载模型
qa = pipeline("question-answering", model="./中文问答模型")

# 问答
context = "北京是中华人民共和国的首都，是全国的政治中心、文化中心。"
question = "北京是什么？"

result = qa(question=question, context=context)

print(f"答案: {result['answer']}")
print(f"置信度: {result['score']:.2%}")
```

### 部署为 API

可以参考 `实战训练/情感分析/` 中的 Flask API 示例，将问答模型部署为 Web 服务。

---

## 🔧 高级配置

### 使用自己的数据

准备 JSON 格式数据（SQuAD 格式）：

```json
{
  "data": [
    {
      "title": "文档标题",
      "paragraphs": [
        {
          "context": "这是上下文内容...",
          "qas": [
            {
              "question": "问题是什么？",
              "id": "1",
              "answers": [
                {
                  "text": "答案",
                  "answer_start": 5
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}
```

使用官方脚本训练：

```bash
python run_qa.py \
  --model_name_or_path bert-base-chinese \
  --train_file train.json \
  --validation_file val.json \
  --do_train \
  --do_eval \
  --per_device_train_batch_size 8 \
  --learning_rate 3e-5 \
  --num_train_epochs 3 \
  --max_seq_length 512 \
  --doc_stride 128 \
  --output_dir ./my_qa_model
```

---

## 📚 相关资源

- [CMRC2018 数据集](https://github.com/ymcui/cmrc2018)
- [SQuAD 数据集](https://rajpurkar.github.io/SQuAD-explorer/)
- [Hugging Face 问答教程](https://huggingface.co/docs/transformers/tasks/question_answering)
- [中文预训练模型](https://github.com/ymcui/Chinese-BERT-wwm)

---

## ✨ 准备好了吗？

**现在就开始训练你的第一个中文问答模型！**

双击 `开始训练.bat` 或运行：

```bash
cd D:\transformers训练\transformers-main\实战训练\文本任务\问答系统
开始训练.bat
```

训练完成后，运行 `中文问答测试.py` 查看效果！

---

**祝训练顺利！** 🎉
