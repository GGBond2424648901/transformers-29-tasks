# é—®ç­”ç³»ç»Ÿï¼ˆQuestion Answeringï¼‰

## ğŸ“– ä»»åŠ¡ç®€ä»‹

é—®ç­”ç³»ç»Ÿæ˜¯ä¸€ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œç»™å®šä¸€æ®µæ–‡æœ¬ï¼ˆä¸Šä¸‹æ–‡ï¼‰å’Œä¸€ä¸ªé—®é¢˜ï¼Œæ¨¡å‹éœ€è¦ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°é—®é¢˜çš„ç­”æ¡ˆã€‚

### åº”ç”¨åœºæ™¯

- ğŸ¤– æ™ºèƒ½å®¢æœï¼šè‡ªåŠ¨å›ç­”ç”¨æˆ·é—®é¢˜
- ğŸ” æœç´¢å¼•æ“ï¼šä»æ–‡æ¡£ä¸­æå–ç­”æ¡ˆ
- ğŸ“š çŸ¥è¯†é—®ç­”ï¼šæ„å»ºçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ
- ğŸ“„ æ–‡æ¡£ç†è§£ï¼šå¿«é€Ÿä»é•¿æ–‡æ¡£ä¸­æ‰¾åˆ°ä¿¡æ¯

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. è®­ç»ƒæ¨¡å‹

**æœ€ç®€å•çš„æ–¹å¼ï¼šåŒå‡»è¿è¡Œ**

```
å¼€å§‹è®­ç»ƒ.bat
```

**æˆ–ä½¿ç”¨å‘½ä»¤è¡Œï¼š**

```bash
cd å®æˆ˜è®­ç»ƒ\æ–‡æœ¬ä»»åŠ¡\é—®ç­”ç³»ç»Ÿ
D:\aaaalokda\envs\myenv\python.exe ç®€å•è®­ç»ƒç¤ºä¾‹.py
```

### 2. æµ‹è¯•æ¨¡å‹

```bash
D:\aaaalokda\envs\myenv\python.exe ä¸­æ–‡é—®ç­”æµ‹è¯•.py
```

---

## ğŸ“Š è®­ç»ƒé…ç½®

| é…ç½®é¡¹ | å€¼ |
|--------|-----|
| æ¨¡å‹ | bert-base-chinese |
| æ•°æ®é›† | CMRC2018ï¼ˆä¸­æ–‡ï¼‰ |
| è®­ç»ƒæ ·æœ¬ | 500 æ¡ |
| éªŒè¯æ ·æœ¬ | 50 æ¡ |
| è®­ç»ƒè½®æ•° | 2 epochs |
| æ‰¹æ¬¡å¤§å° | 8 |
| å­¦ä¹ ç‡ | 3e-5 |
| è®­ç»ƒæ—¶é—´ | 5-10 åˆ†é’Ÿï¼ˆGPUï¼‰ |

---

## ğŸ’» ä½¿ç”¨ç¤ºä¾‹

### Pipeline æ–¹å¼ï¼ˆæ¨èï¼‰

```python
import os
os.environ['HF_HOME'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'
os.environ['TRANSFORMERS_CACHE'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'

from transformers import pipeline

# åŠ è½½æ¨¡å‹
qa = pipeline("question-answering", model="./ä¸­æ–‡é—®ç­”æ¨¡å‹")

# é—®ç­”
context = """
åŒ—äº¬æ˜¯ä¸­åäººæ°‘å…±å’Œå›½çš„é¦–éƒ½ï¼Œæ˜¯å…¨å›½çš„æ”¿æ²»ä¸­å¿ƒã€æ–‡åŒ–ä¸­å¿ƒã€‚
åŒ—äº¬ä½äºååŒ—å¹³åŸåŒ—éƒ¨ï¼ŒèƒŒé ç‡•å±±ï¼Œæ¯—é‚»å¤©æ´¥å¸‚å’Œæ²³åŒ—çœã€‚
"""

question = "åŒ—äº¬æ˜¯ä»€ä¹ˆï¼Ÿ"

result = qa(question=question, context=context)

print(f"ç­”æ¡ˆ: {result['answer']}")
print(f"ç½®ä¿¡åº¦: {result['score']:.2%}")
```

### æ‰‹åŠ¨æ–¹å¼

```python
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch

# åŠ è½½æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained("./ä¸­æ–‡é—®ç­”æ¨¡å‹")
model = AutoModelForQuestionAnswering.from_pretrained("./ä¸­æ–‡é—®ç­”æ¨¡å‹")

# å‡†å¤‡è¾“å…¥
context = "åŒ—äº¬æ˜¯ä¸­åäººæ°‘å…±å’Œå›½çš„é¦–éƒ½ï¼Œæ˜¯å…¨å›½çš„æ”¿æ²»ä¸­å¿ƒã€æ–‡åŒ–ä¸­å¿ƒã€‚"
question = "åŒ—äº¬æ˜¯ä»€ä¹ˆï¼Ÿ"

inputs = tokenizer(question, context, return_tensors="pt")

# æ¨ç†
with torch.no_grad():
    outputs = model(**inputs)

# è·å–ç­”æ¡ˆ
answer_start = torch.argmax(outputs.start_logits)
answer_end = torch.argmax(outputs.end_logits) + 1

answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(inputs["input_ids"][0][answer_start:answer_end])
)

print(f"ç­”æ¡ˆ: {answer}")
```

---

## ğŸ“ æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `ç®€å•è®­ç»ƒç¤ºä¾‹.py` | ç®€åŒ–çš„è®­ç»ƒè„šæœ¬ï¼Œé€‚åˆå¿«é€Ÿä¸Šæ‰‹ |
| `å¼€å§‹è®­ç»ƒ.bat` | ä¸€é”®å¯åŠ¨è®­ç»ƒ |
| `ä¸­æ–‡é—®ç­”æµ‹è¯•.py` | æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹ |
| `run_qa.py` | å®˜æ–¹å®Œæ•´è®­ç»ƒè„šæœ¬ï¼Œæ”¯æŒæ›´å¤šå‚æ•° |
| `å¿«é€Ÿå¼€å§‹.md` | è¯¦ç»†çš„ä½¿ç”¨æŒ‡å— |
| `è®­ç»ƒè¯´æ˜.md` | å®Œæ•´çš„è®­ç»ƒè¯´æ˜æ–‡æ¡£ |
| `requirements.txt` | ä¾èµ–åŒ…åˆ—è¡¨ |

---

## ğŸ¯ æ¨èæ¨¡å‹

### ä¸­æ–‡æ¨¡å‹

| æ¨¡å‹ | è¯´æ˜ | æ¨èåº¦ |
|------|------|--------|
| `bert-base-chinese` | åŸºç¡€ä¸­æ–‡ BERT | â­â­â­ |
| `hfl/chinese-roberta-wwm-ext` | ä¸­æ–‡ RoBERTaï¼ˆæ•ˆæœæ›´å¥½ï¼‰ | â­â­â­â­â­ |
| `hfl/chinese-bert-wwm-ext` | ä¸­æ–‡ BERT WWM | â­â­â­â­ |

### è‹±æ–‡æ¨¡å‹

| æ¨¡å‹ | è¯´æ˜ | æ¨èåº¦ |
|------|------|--------|
| `bert-base-uncased` | åŸºç¡€è‹±æ–‡ BERT | â­â­â­ |
| `roberta-base` | è‹±æ–‡ RoBERTa | â­â­â­â­ |
| `distilbert-base-uncased` | è½»é‡çº§ BERT | â­â­â­â­ |

---

## ğŸ“š æ•°æ®é›†

### ä¸­æ–‡æ•°æ®é›†

- **CMRC2018**: ä¸­æ–‡æœºå™¨é˜…è¯»ç†è§£æ•°æ®é›†
- **DRCD**: ç¹ä½“ä¸­æ–‡é˜…è¯»ç†è§£æ•°æ®é›†
- **DuReader**: ç™¾åº¦ä¸­æ–‡é˜…è¯»ç†è§£æ•°æ®é›†

### è‹±æ–‡æ•°æ®é›†

- **SQuAD**: æ–¯å¦ç¦é—®ç­”æ•°æ®é›†ï¼ˆæœ€å¸¸ç”¨ï¼‰
- **SQuAD 2.0**: åŒ…å«æ— ç­”æ¡ˆé—®é¢˜
- **Natural Questions**: Google è‡ªç„¶é—®é¢˜æ•°æ®é›†

---

## ğŸ’¡ è®­ç»ƒæŠ€å·§

### 1. æé«˜å‡†ç¡®ç‡

- ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆå¦‚ RoBERTaï¼‰
- å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆ3-5 epochsï¼‰
- ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®
- è°ƒæ•´å­¦ä¹ ç‡ï¼ˆ2e-5 åˆ° 5e-5ï¼‰

### 2. åŠ å¿«è®­ç»ƒé€Ÿåº¦

- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16ï¼‰
- å¢å¤§æ‰¹æ¬¡å¤§å°ï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
- å‡å°åºåˆ—é•¿åº¦

### 3. èŠ‚çœæ˜¾å­˜

- å‡å°æ‰¹æ¬¡å¤§å°
- ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹
- å‡å°åºåˆ—é•¿åº¦

---

## ğŸ”§ å¸¸è§é—®é¢˜

### Q: è®­ç»ƒæ—¶æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

A: å‡å°æ‰¹æ¬¡å¤§å°ï¼š
```python
per_device_train_batch_size=4  # æˆ–æ›´å°
```

### Q: æ²¡æœ‰ GPU å¯ä»¥è®­ç»ƒå—ï¼Ÿ

A: å¯ä»¥ï¼Œä½†éœ€è¦ï¼š
1. ç§»é™¤ `fp16=True` å‚æ•°
2. é¢„æœŸè®­ç»ƒæ—¶é—´ä¼šå¢åŠ  5-10 å€

### Q: å¦‚ä½•ä½¿ç”¨è‡ªå·±çš„æ•°æ®ï¼Ÿ

A: å‡†å¤‡ SQuAD æ ¼å¼çš„ JSON æ–‡ä»¶ï¼Œä½¿ç”¨ `run_qa.py` è®­ç»ƒï¼š
```bash
python run_qa.py \
  --model_name_or_path bert-base-chinese \
  --train_file train.json \
  --validation_file val.json \
  --do_train \
  --do_eval \
  --output_dir ./my_model
```

### Q: å¦‚ä½•è¯„ä¼°æ¨¡å‹æ•ˆæœï¼Ÿ

A: ä¸»è¦çœ‹ä¸¤ä¸ªæŒ‡æ ‡ï¼š
- **Exact Match (EM)**: å®Œå…¨åŒ¹é…ç‡
- **F1 Score**: F1 åˆ†æ•°

BERT åœ¨ SQuAD ä¸Šçš„é¢„æœŸï¼šEM ~80%, F1 ~88%

---

## ğŸ”— ç›¸å…³èµ„æº

- [Hugging Face é—®ç­”æ•™ç¨‹](https://huggingface.co/docs/transformers/tasks/question_answering)
- [SQuAD æ•°æ®é›†](https://rajpurkar.github.io/SQuAD-explorer/)
- [CMRC2018 æ•°æ®é›†](https://github.com/ymcui/cmrc2018)
- [ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹](https://github.com/ymcui/Chinese-BERT-wwm)

---

## âœ¨ å¼€å§‹è®­ç»ƒ

å‡†å¤‡å¥½äº†å—ï¼Ÿè¿è¡Œ `å¼€å§‹è®­ç»ƒ.bat` å¼€å§‹ä½ çš„ç¬¬ä¸€ä¸ªé—®ç­”æ¨¡å‹è®­ç»ƒï¼

è®­ç»ƒå®Œæˆåï¼Œè¿è¡Œ `ä¸­æ–‡é—®ç­”æµ‹è¯•.py` æŸ¥çœ‹æ•ˆæœã€‚

**ç¥è®­ç»ƒé¡ºåˆ©ï¼** ğŸš€
