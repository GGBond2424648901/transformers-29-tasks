# ğŸ“„ æ–‡æœ¬æ‘˜è¦ï¼ˆText Summarizationï¼‰

## ğŸ“– ä»»åŠ¡ç®€ä»‹

æ–‡æœ¬æ‘˜è¦æ˜¯å°†é•¿æ–‡æœ¬å‹ç¼©æˆç®€çŸ­æ‘˜è¦çš„ä»»åŠ¡ï¼Œä¿ç•™åŸæ–‡çš„å…³é”®ä¿¡æ¯ã€‚åˆ†ä¸ºæŠ½å–å¼æ‘˜è¦ï¼ˆä»åŸæ–‡ä¸­é€‰æ‹©å¥å­ï¼‰å’Œç”Ÿæˆå¼æ‘˜è¦ï¼ˆç”Ÿæˆæ–°çš„å¥å­ï¼‰ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

- **ğŸ“° æ–°é—»æ‘˜è¦**: è‡ªåŠ¨ç”Ÿæˆæ–°é—»æ ‡é¢˜å’Œæ‘˜è¦
- **ğŸ“š æ–‡æ¡£æ€»ç»“**: é•¿æ–‡æ¡£çš„å¿«é€Ÿæ¦‚è§ˆ
- **ğŸ“§ é‚®ä»¶æ‘˜è¦**: æå–é‚®ä»¶è¦ç‚¹
- **ğŸ¬ è§†é¢‘å­—å¹•**: ç”Ÿæˆè§†é¢‘å†…å®¹æ‘˜è¦
- **ğŸ“Š æŠ¥å‘Šç”Ÿæˆ**: è‡ªåŠ¨ç”Ÿæˆä¼šè®®çºªè¦ã€ç ”ç©¶æŠ¥å‘Šæ‘˜è¦

## ğŸ“ æ–‡ä»¶è¯´æ˜

- `run_summarization.py` - ä½¿ç”¨ Trainer API è®­ç»ƒæ‘˜è¦æ¨¡å‹
- `run_summarization_no_trainer.py` - ä¸ä½¿ç”¨ Trainer è®­ç»ƒ
- `requirements.txt` - ä¾èµ–åŒ…åˆ—è¡¨
- `README.md` - æœ¬æ–‡ä»¶

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. è¿è¡Œè®­ç»ƒ

#### ä½¿ç”¨ CNN/DailyMail æ•°æ®é›†

```bash
python run_summarization.py \
    --model_name_or_path facebook/bart-base \
    --dataset_name cnn_dailymail \
    --dataset_config_name "3.0.0" \
    --output_dir ./output \
    --do_train \
    --do_eval \
    --per_device_train_batch_size 4 \
    --num_train_epochs 3 \
    --predict_with_generate
```

#### ä½¿ç”¨æœ¬åœ°æ•°æ®

```bash
python run_summarization.py \
    --model_name_or_path facebook/bart-base \
    --train_file train.json \
    --validation_file val.json \
    --text_column article \
    --summary_column summary \
    --output_dir ./output \
    --do_train \
    --do_eval
```

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### Pipeline æ¨ç†

```python
from transformers import pipeline

# åˆ›å»ºæ‘˜è¦ç”Ÿæˆå™¨
summarizer = pipeline(
    "summarization",
    model="facebook/bart-large-cnn"
)

# ç”Ÿæˆæ‘˜è¦
article = """
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚ä»æ™ºèƒ½æ‰‹æœºåˆ°è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œ
AI æŠ€æœ¯æ— å¤„ä¸åœ¨ã€‚æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥åˆ†æå¤§é‡æ•°æ®ï¼Œè¯†åˆ«æ¨¡å¼ï¼Œ
å¹¶åšå‡ºé¢„æµ‹ã€‚æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿ
äººè„‘çš„å·¥ä½œæ–¹å¼ã€‚éšç€è®¡ç®—èƒ½åŠ›çš„æå‡å’Œæ•°æ®çš„å¢åŠ ï¼ŒAI çš„åº”ç”¨
å°†ä¼šæ›´åŠ å¹¿æ³›ã€‚
"""

summary = summarizer(
    article,
    max_length=50,
    min_length=10,
    do_sample=False
)

print(summary[0]['summary_text'])
# è¾“å‡ºï¼šäººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ç”Ÿæ´»æ–¹å¼ï¼Œåº”ç”¨å¹¿æ³›ã€‚
```

### æ‰¹é‡æ‘˜è¦

```python
articles = [
    "é•¿æ–‡æœ¬1...",
    "é•¿æ–‡æœ¬2...",
    "é•¿æ–‡æœ¬3..."
]

summaries = summarizer(
    articles,
    max_length=50,
    min_length=10,
    batch_size=8
)

for i, summary in enumerate(summaries):
    print(f"æ–‡ç«  {i+1} æ‘˜è¦: {summary['summary_text']}")
```

### è®­ç»ƒè‡ªå®šä¹‰æ‘˜è¦æ¨¡å‹

```python
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq
)
from datasets import load_dataset

# 1. åŠ è½½æ•°æ®é›†
dataset = load_dataset("cnn_dailymail", "3.0.0")

# 2. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "facebook/bart-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# 3. æ•°æ®é¢„å¤„ç†
def preprocess_function(examples):
    inputs = examples["article"]
    targets = examples["highlights"]
    
    model_inputs = tokenizer(
        inputs,
        max_length=1024,
        truncation=True
    )
    
    labels = tokenizer(
        targets,
        max_length=128,
        truncation=True
    )
    
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset.map(
    preprocess_function,
    batched=True
)

# 4. è®­ç»ƒé…ç½®
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    num_train_epochs=3,
    predict_with_generate=True,
)

# 5. æ•°æ®æ•´ç†å™¨
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model
)

# 6. åˆ›å»º Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    data_collator=data_collator,
)

# 7. å¼€å§‹è®­ç»ƒ
trainer.train()
```

## ğŸ¨ æ¨èæ¨¡å‹

### è‹±æ–‡æ¨¡å‹

| æ¨¡å‹ | å¤§å° | ç‰¹ç‚¹ |
|------|------|------|
| facebook/bart-large-cnn | å¤§ | CNN/DailyMail è®­ç»ƒï¼Œæ–°é—»æ‘˜è¦æ•ˆæœå¥½ |
| google/pegasus-cnn_dailymail | å¤§ | ä¸“é—¨ä¸ºæ‘˜è¦è®¾è®¡ |
| t5-base | ä¸­ | é€šç”¨ Seq2Seq æ¨¡å‹ |
| google/pegasus-xsum | å¤§ | æç®€æ‘˜è¦é£æ ¼ |

### ä¸­æ–‡æ¨¡å‹

| æ¨¡å‹ | ç‰¹ç‚¹ |
|------|------|
| csebuetnlp/mT5_multilingual_XLSum | å¤šè¯­è¨€æ‘˜è¦ï¼Œæ”¯æŒä¸­æ–‡ |
| fnlp/bart-base-chinese | ä¸­æ–‡ BART |

## ğŸ“Š æ•°æ®æ ¼å¼

### JSON æ ¼å¼

```json
[
    {
        "article": "é•¿æ–‡æœ¬å†…å®¹...",
        "summary": "æ‘˜è¦å†…å®¹"
    }
]
```

### CSV æ ¼å¼

```csv
article,summary
"é•¿æ–‡æœ¬å†…å®¹...","æ‘˜è¦å†…å®¹"
```

## âš™ï¸ é‡è¦å‚æ•°

### ç”Ÿæˆå‚æ•°

```python
summarizer(
    text,
    max_length=130,        # æœ€å¤§æ‘˜è¦é•¿åº¦
    min_length=30,         # æœ€å°æ‘˜è¦é•¿åº¦
    do_sample=False,       # æ˜¯å¦é‡‡æ ·
    num_beams=4,           # Beam search æ•°é‡
    length_penalty=2.0,    # é•¿åº¦æƒ©ç½š
    early_stopping=True    # æ—©åœ
)
```

### è®­ç»ƒå‚æ•°

```bash
--max_source_length 1024       # è¾“å…¥æœ€å¤§é•¿åº¦
--max_target_length 128        # æ‘˜è¦æœ€å¤§é•¿åº¦
--val_max_target_length 128    # éªŒè¯æ—¶æ‘˜è¦æœ€å¤§é•¿åº¦
--num_beams 4                  # Beam search
--predict_with_generate        # ä½¿ç”¨ç”Ÿæˆæ¨¡å¼è¯„ä¼°
```

## ğŸ’¡ è®­ç»ƒæŠ€å·§

### 1. æ§åˆ¶æ‘˜è¦é•¿åº¦

```python
# çŸ­æ‘˜è¦
summary = summarizer(text, max_length=50, min_length=10)

# é•¿æ‘˜è¦
summary = summarizer(text, max_length=200, min_length=50)
```

### 2. æé«˜æ‘˜è¦è´¨é‡

```python
# ä½¿ç”¨ Beam Search
summary = summarizer(
    text,
    num_beams=5,           # å¢åŠ  beam æ•°é‡
    length_penalty=2.0,    # é•¿åº¦æƒ©ç½š
    early_stopping=True
)

# ä½¿ç”¨é‡‡æ ·
summary = summarizer(
    text,
    do_sample=True,
    top_k=50,
    top_p=0.95,
    temperature=0.7
)
```

### 3. å¤„ç†é•¿æ–‡æœ¬

```python
# åˆ†æ®µæ‘˜è¦
def summarize_long_text(text, max_chunk_length=1000):
    # åˆ†å‰²æ–‡æœ¬
    chunks = [text[i:i+max_chunk_length] 
              for i in range(0, len(text), max_chunk_length)]
    
    # åˆ†åˆ«æ‘˜è¦
    summaries = []
    for chunk in chunks:
        summary = summarizer(chunk, max_length=100)
        summaries.append(summary[0]['summary_text'])
    
    # åˆå¹¶æ‘˜è¦
    combined = " ".join(summaries)
    
    # å†æ¬¡æ‘˜è¦
    final_summary = summarizer(combined, max_length=150)
    return final_summary[0]['summary_text']
```

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

### ROUGE åˆ†æ•°

```python
from datasets import load_metric

rouge = load_metric("rouge")

predictions = ["é¢„æµ‹çš„æ‘˜è¦1", "é¢„æµ‹çš„æ‘˜è¦2"]
references = ["å‚è€ƒæ‘˜è¦1", "å‚è€ƒæ‘˜è¦2"]

results = rouge.compute(
    predictions=predictions,
    references=references
)

print(f"ROUGE-1: {results['rouge1'].mid.fmeasure:.4f}")
print(f"ROUGE-2: {results['rouge2'].mid.fmeasure:.4f}")
print(f"ROUGE-L: {results['rougeL'].mid.fmeasure:.4f}")
```

### æŒ‡æ ‡è¯´æ˜

- **ROUGE-1**: å•è¯é‡å 
- **ROUGE-2**: åŒè¯ç»„é‡å 
- **ROUGE-L**: æœ€é•¿å…¬å…±å­åºåˆ—

## ğŸ¯ åº”ç”¨ç¤ºä¾‹

### 1. æ–°é—»æ‘˜è¦

```python
news = """
ã€ç§‘æŠ€æ–°é—»ã€‘ä»Šå¤©ï¼ŒæŸç§‘æŠ€å…¬å¸å‘å¸ƒäº†æœ€æ–°çš„äººå·¥æ™ºèƒ½æ¨¡å‹...
ï¼ˆé•¿ç¯‡æ–°é—»å†…å®¹ï¼‰
"""

summary = summarizer(news, max_length=100)
print(f"æ–°é—»æ‘˜è¦: {summary[0]['summary_text']}")
```

### 2. ä¼šè®®çºªè¦

```python
meeting_notes = """
ä¼šè®®æ—¶é—´ï¼š2026å¹´1æœˆ31æ—¥
å‚ä¼šäººå‘˜ï¼š...
ä¼šè®®å†…å®¹ï¼š...
ï¼ˆè¯¦ç»†ä¼šè®®è®°å½•ï¼‰
"""

summary = summarizer(meeting_notes, max_length=150)
print(f"ä¼šè®®è¦ç‚¹: {summary[0]['summary_text']}")
```

### 3. æ–‡æ¡£æ€»ç»“

```python
document = """
ç ”ç©¶æŠ¥å‘Šï¼šäººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨
æ‘˜è¦ï¼š...
å¼•è¨€ï¼š...
æ–¹æ³•ï¼š...
ç»“æœï¼š...
è®¨è®ºï¼š...
ç»“è®ºï¼š...
"""

summary = summarizer(document, max_length=200)
print(f"æ–‡æ¡£æ‘˜è¦: {summary[0]['summary_text']}")
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **è®¡ç®—èµ„æº**: æ‘˜è¦æ¨¡å‹é€šå¸¸è¾ƒå¤§ï¼Œéœ€è¦ GPU åŠ é€Ÿ
2. **æ–‡æœ¬é•¿åº¦**: æ³¨æ„è¾“å…¥æ–‡æœ¬çš„æœ€å¤§é•¿åº¦é™åˆ¶
3. **æ‘˜è¦è´¨é‡**: ç”Ÿæˆçš„æ‘˜è¦å¯èƒ½ä¸å®Œç¾ï¼Œéœ€è¦äººå·¥å®¡æ ¸
4. **è¯­è¨€æ”¯æŒ**: å¤§å¤šæ•°æ¨¡å‹é’ˆå¯¹è‹±æ–‡ä¼˜åŒ–ï¼Œä¸­æ–‡æ•ˆæœå¯èƒ½è¾ƒå·®
5. **äº‹å®å‡†ç¡®æ€§**: ç”Ÿæˆå¼æ‘˜è¦å¯èƒ½äº§ç”Ÿä¸å‡†ç¡®çš„ä¿¡æ¯

## ğŸ”— ç›¸å…³èµ„æº

- [BART è®ºæ–‡](https://arxiv.org/abs/1910.13461)
- [PEGASUS è®ºæ–‡](https://arxiv.org/abs/1912.08777)
- [CNN/DailyMail æ•°æ®é›†](https://huggingface.co/datasets/cnn_dailymail)
- [ROUGE è¯„ä¼°æŒ‡æ ‡](https://en.wikipedia.org/wiki/ROUGE_(metric))
