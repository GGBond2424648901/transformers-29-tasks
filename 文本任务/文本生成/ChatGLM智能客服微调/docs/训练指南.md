# ğŸ“ ChatGLM æ™ºèƒ½å®¢æœè®­ç»ƒæŒ‡å—

## ğŸ“‹ è®­ç»ƒå‰å‡†å¤‡

### 1. ç¯å¢ƒæ£€æŸ¥

```bash
# æ£€æŸ¥ Python ç‰ˆæœ¬
python --version  # åº”è¯¥æ˜¯ 3.8+

# æ£€æŸ¥ GPU
python -c "import torch; print(torch.cuda.is_available())"  # åº”è¯¥è¾“å‡º True

# æ£€æŸ¥æ˜¾å­˜
python -c "import torch; print(torch.cuda.get_device_properties(0).total_memory / 1024**3, 'GB')"
```

### 2. å®‰è£…ä¾èµ–

```bash
cd å®æˆ˜è®­ç»ƒ/æ–‡æœ¬ä»»åŠ¡/æ–‡æœ¬ç”Ÿæˆ/ChatGLMæ™ºèƒ½å®¢æœå¾®è°ƒ
pip install -r requirements.txt
```

### 3. å‡†å¤‡æ•°æ®

- ç¼–è¾‘ `data/train.json` æ·»åŠ è®­ç»ƒæ•°æ®
- ç¼–è¾‘ `data/dev.json` æ·»åŠ éªŒè¯æ•°æ®
- å‚è€ƒ `data/æ•°æ®è¯´æ˜.md` äº†è§£æ•°æ®æ ¼å¼

---

## ğŸš€ å¼€å§‹è®­ç»ƒ

### æ–¹æ³• 1ï¼šä¸€é”®è®­ç»ƒï¼ˆæ¨èï¼‰

```bash
# Windows
åŒå‡» å¼€å§‹è®­ç»ƒ.bat

# æˆ–å‘½ä»¤è¡Œ
å¼€å§‹è®­ç»ƒ.bat
```

### æ–¹æ³• 2ï¼šå‘½ä»¤è¡Œè®­ç»ƒ

```bash
D:\aaaalokda\envs\myenv\python.exe chatglm_lora_finetune.py
```

---

## âš™ï¸ è®­ç»ƒå‚æ•°è¯´æ˜

### åŸºç¡€å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | å»ºè®® |
|------|--------|------|------|
| num_train_epochs | 3 | è®­ç»ƒè½®æ•° | 3-5 è½® |
| per_device_train_batch_size | 4 | æ‰¹æ¬¡å¤§å° | 2-8 |
| learning_rate | 5e-4 | å­¦ä¹ ç‡ | 1e-4 åˆ° 1e-3 |
| max_length | 512 | æœ€å¤§åºåˆ—é•¿åº¦ | 256-1024 |

### LoRA å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | å»ºè®® |
|------|--------|------|------|
| lora_rank | 8 | LoRA ç§© | 4-16 |
| lora_alpha | 32 | ç¼©æ”¾å› å­ | rank * 4 |
| lora_dropout | 0.1 | Dropout ç‡ | 0.05-0.1 |

---

## ğŸ“Š è®­ç»ƒè¿‡ç¨‹

### è®­ç»ƒæµç¨‹

```
1. åŠ è½½ ChatGLM-6B åŸºç¡€æ¨¡å‹
   â†“
2. æ·»åŠ  LoRA é€‚é…å™¨
   â†“
3. åŠ è½½è®­ç»ƒæ•°æ®
   â†“
4. æ•°æ®é¢„å¤„ç†
   â†“
5. å¼€å§‹è®­ç»ƒï¼ˆæ˜¾ç¤ºè¿›åº¦ï¼‰
   â†“
6. ä¿å­˜ LoRA æƒé‡
   â†“
7. è®­ç»ƒå®Œæˆ
```

### è®­ç»ƒè¾“å‡º

```
======================================================================
ğŸ¤– ChatGLM-6B LoRA å¾®è°ƒ - æ™ºèƒ½å®¢æœ
======================================================================

âš™ï¸  è®­ç»ƒé…ç½®
======================================================================
æ¨¡å‹: THUDM/chatglm-6b
è®­ç»ƒæ•°æ®: data/train.json
éªŒè¯æ•°æ®: data/dev.json
LoRA Rank: 8
LoRA Alpha: 32
è¾“å‡ºç›®å½•: output/chatglm-customer-lora

ğŸ“¦ åŠ è½½æ¨¡å‹
======================================================================
æ­£åœ¨åŠ è½½ tokenizer...
âœ… Tokenizer åŠ è½½æˆåŠŸ

æ­£åœ¨åŠ è½½æ¨¡å‹...
âœ… æ¨¡å‹åŠ è½½æˆåŠŸ

ğŸ”§ é…ç½® LoRA
======================================================================
trainable params: 1,949,696 || all params: 6,175,391,744 || trainable%: 0.03
âœ… LoRA é…ç½®å®Œæˆ

ğŸ“Š å‡†å¤‡æ•°æ®
======================================================================
ğŸ“¥ åŠ è½½æ•°æ®: data/train.json
âœ… åŠ è½½äº† 10 æ¡æ•°æ®
ğŸ“¥ åŠ è½½æ•°æ®: data/dev.json
âœ… åŠ è½½äº† 2 æ¡æ•°æ®

å¤„ç†è®­ç»ƒæ•°æ®...
å¤„ç†éªŒè¯æ•°æ®...
âœ… è®­ç»ƒé›†: 10 æ¡
âœ… éªŒè¯é›†: 2 æ¡

ğŸš€ å¼€å§‹è®­ç»ƒ
======================================================================
Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:30<00:00, 15.0s/it]
Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:28<00:00, 14.8s/it]
Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:29<00:00, 14.9s/it]

ğŸ’¾ ä¿å­˜æ¨¡å‹
======================================================================
âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: output/chatglm-customer-lora

âœ¨ è®­ç»ƒå®Œæˆï¼
======================================================================
```

---

## ğŸ¯ è®­ç»ƒæ—¶é—´ä¼°ç®—

### åŸºäº RTX 3070 (8GB)

| æ•°æ®é‡ | è½®æ•° | é¢„è®¡æ—¶é—´ |
|--------|------|---------|
| 10 æ¡ | 3 | 5-10 åˆ†é’Ÿ |
| 50 æ¡ | 3 | 15-20 åˆ†é’Ÿ |
| 100 æ¡ | 3 | 30-40 åˆ†é’Ÿ |
| 500 æ¡ | 3 | 2-3 å°æ—¶ |

---

## ğŸ”§ å‚æ•°è°ƒä¼˜

### æ˜¾å­˜ä¸è¶³

**é—®é¢˜**ï¼š`CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. é™ä½ batch sizeï¼š
```python
per_device_train_batch_size=2  # ä» 4 æ”¹ä¸º 2
```

2. é™ä½ max_lengthï¼š
```python
max_length=256  # ä» 512 æ”¹ä¸º 256
```

3. å¢åŠ æ¢¯åº¦ç´¯ç§¯ï¼š
```python
gradient_accumulation_steps=8  # ä» 4 æ”¹ä¸º 8
```

### è®­ç»ƒå¤ªæ…¢

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. å¢åŠ  batch sizeï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰ï¼š
```python
per_device_train_batch_size=8
```

2. å‡å°‘æ•°æ®é•¿åº¦ï¼š
```python
max_length=256
```

3. ä½¿ç”¨æ··åˆç²¾åº¦ï¼ˆå·²é»˜è®¤å¼€å¯ï¼‰ï¼š
```python
fp16=True
```

### æ•ˆæœä¸å¥½

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. å¢åŠ è®­ç»ƒè½®æ•°ï¼š
```python
num_train_epochs=5  # ä» 3 æ”¹ä¸º 5
```

2. å¢åŠ è®­ç»ƒæ•°æ®ï¼ˆæœ€é‡è¦ï¼‰ï¼š
- æ·»åŠ æ›´å¤šé«˜è´¨é‡å¯¹è¯æ•°æ®
- è¦†ç›–æ›´å¤šé—®é¢˜ç±»å‹

3. è°ƒæ•´å­¦ä¹ ç‡ï¼š
```python
learning_rate=1e-4  # é™ä½å­¦ä¹ ç‡
```

4. å¢åŠ  LoRA rankï¼š
```python
lora_rank=16  # ä» 8 æ”¹ä¸º 16
```

---

## ğŸ“ˆ ç›‘æ§è®­ç»ƒ

### æŸ¥çœ‹è®­ç»ƒæ—¥å¿—

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ˜¾ç¤ºï¼š
- Lossï¼ˆæŸå¤±å€¼ï¼‰ï¼šè¶Šä½è¶Šå¥½
- Learning Rateï¼ˆå­¦ä¹ ç‡ï¼‰
- Steps/Secondï¼ˆè®­ç»ƒé€Ÿåº¦ï¼‰

### è¯„ä¼°æŒ‡æ ‡

- **è®­ç»ƒ Loss**ï¼šåº”è¯¥é€æ¸ä¸‹é™
- **éªŒè¯ Loss**ï¼šåº”è¯¥æ¥è¿‘è®­ç»ƒ Loss
- **è¿‡æ‹Ÿåˆæ£€æµ‹**ï¼šéªŒè¯ Loss ä¸Šå‡ = è¿‡æ‹Ÿåˆ

---

## ğŸ’¾ ä¿å­˜çš„æ–‡ä»¶

è®­ç»ƒå®Œæˆåï¼Œ`output/chatglm-customer-lora/` åŒ…å«ï¼š

```
output/chatglm-customer-lora/
â”œâ”€â”€ adapter_config.json      # LoRA é…ç½®
â”œâ”€â”€ adapter_model.bin         # LoRA æƒé‡ï¼ˆä¸»è¦æ–‡ä»¶ï¼‰
â”œâ”€â”€ tokenizer_config.json     # Tokenizer é…ç½®
â””â”€â”€ special_tokens_map.json   # ç‰¹æ®Š token æ˜ å°„
```

**é‡è¦**ï¼šåªéœ€è¦ä¿å­˜è¿™ä¸ªæ–‡ä»¶å¤¹ï¼ˆçº¦ 10-50MBï¼‰ï¼Œä¸éœ€è¦ä¿å­˜å®Œæ•´æ¨¡å‹ï¼ˆ12GBï¼‰ã€‚

---

## ğŸ§ª è®­ç»ƒåæµ‹è¯•

### 1. å¿«é€Ÿæµ‹è¯•

```bash
D:\aaaalokda\envs\myenv\python.exe test_model.py
```

é€‰æ‹©æµ‹è¯•æ¨¡å¼ï¼š
1. è‡ªåŠ¨æµ‹è¯•ï¼ˆé¢„è®¾é—®é¢˜ï¼‰
2. äº¤äº’å¼æµ‹è¯•ï¼ˆæ‰‹åŠ¨è¾“å…¥ï¼‰
3. å¯¹æ¯”æµ‹è¯•ï¼ˆåŸå§‹ vs å¾®è°ƒï¼‰

### 2. Web æµ‹è¯•

```bash
å¯åŠ¨å®¢æœç³»ç»Ÿ.bat
```

åœ¨æµè§ˆå™¨ä¸­è®¿é—®ï¼š`http://127.0.0.1:5000`

---

## ğŸ”„ æŒç»­ä¼˜åŒ–

### è¿­ä»£æµç¨‹

```
1. æ”¶é›†ç”¨æˆ·åé¦ˆ
   â†“
2. è¡¥å……è®­ç»ƒæ•°æ®
   â†“
3. é‡æ–°è®­ç»ƒæ¨¡å‹
   â†“
4. æµ‹è¯•è¯„ä¼°
   â†“
5. éƒ¨ç½²æ›´æ–°
   â†“
å›åˆ°æ­¥éª¤ 1
```

### æ•°æ®æ›´æ–°

1. ç¼–è¾‘ `data/train.json` æ·»åŠ æ–°æ•°æ®
2. è¿è¡Œ `å¼€å§‹è®­ç»ƒ.bat`
3. æµ‹è¯•æ–°æ¨¡å‹æ•ˆæœ
4. å¦‚æœæ»¡æ„ï¼Œæ›¿æ¢æ—§æ¨¡å‹

---

## â“ å¸¸è§é—®é¢˜

### Q1: é¦–æ¬¡è®­ç»ƒå¾ˆæ…¢ï¼Ÿ

A: é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½ ChatGLM-6Bï¼ˆçº¦ 12GBï¼‰ï¼Œéœ€è¦è¾ƒé•¿æ—¶é—´ã€‚åç»­è®­ç»ƒä¼šå¿«å¾ˆå¤šã€‚

### Q2: è®­ç»ƒä¸­æ–­äº†æ€ä¹ˆåŠï¼Ÿ

A: å¯ä»¥é‡æ–°è¿è¡Œè®­ç»ƒè„šæœ¬ï¼Œä¼šä»å¤´å¼€å§‹ã€‚å»ºè®®ä¿å­˜æ£€æŸ¥ç‚¹ï¼š
```python
save_steps=50  # æ¯ 50 æ­¥ä¿å­˜ä¸€æ¬¡
```

### Q3: å¦‚ä½•ä½¿ç”¨æ›´å¤šæ•°æ®ï¼Ÿ

A: ç›´æ¥åœ¨ `data/train.json` ä¸­æ·»åŠ æ›´å¤šå¯¹è¯å¯¹ï¼Œæ ¼å¼ä¿æŒä¸€è‡´å³å¯ã€‚

### Q4: å¯ä»¥åœ¨ CPU ä¸Šè®­ç»ƒå—ï¼Ÿ

A: å¯ä»¥ï¼Œä½†ä¼šéå¸¸æ…¢ï¼ˆ10-100 å€ï¼‰ã€‚ä¸æ¨èã€‚

### Q5: è®­ç»ƒååŸæ¨¡å‹ä¼šè¢«ä¿®æ”¹å—ï¼Ÿ

A: ä¸ä¼šã€‚LoRA åªä¿å­˜å¢é‡æƒé‡ï¼ŒåŸå§‹ ChatGLM-6B ä¿æŒä¸å˜ã€‚

---

## ğŸ“š è¿›é˜¶æŠ€å·§

### 1. ä½¿ç”¨æ›´å¤§çš„ LoRA Rank

```python
lora_rank=16  # æ›´å¤šå¯è®­ç»ƒå‚æ•°ï¼Œæ•ˆæœæ›´å¥½ï¼Œä½†éœ€è¦æ›´å¤šæ˜¾å­˜
```

### 2. å¤šè½®å¯¹è¯è®­ç»ƒ

ä¿®æ”¹æ•°æ®æ ¼å¼æ”¯æŒå¤šè½®å¯¹è¯ï¼š
```json
{
  "instruction": "ç”¨æˆ·é—®ï¼šå¦‚ä½•é€€è´§ï¼Ÿ",
  "input": "å†å²å¯¹è¯ï¼š...",
  "output": "å®¢æœå›ç­”ï¼š..."
}
```

### 3. é¢†åŸŸé€‚é…

é’ˆå¯¹ç‰¹å®šé¢†åŸŸï¼ˆåŒ»ç–—ã€æ³•å¾‹ã€é‡‘èï¼‰å‡†å¤‡ä¸“ä¸šæ•°æ®ã€‚

---

**å‡†å¤‡å¥½äº†å—ï¼Ÿè¿è¡Œ `å¼€å§‹è®­ç»ƒ.bat` å¼€å§‹è®­ç»ƒä½ çš„æ™ºèƒ½å®¢æœï¼** ğŸš€
