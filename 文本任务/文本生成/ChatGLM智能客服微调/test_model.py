#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•å¾®è°ƒåçš„ ChatGLM å®¢æœæ¨¡å‹
"""

import os
os.environ['HF_HOME'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'
os.environ['TRANSFORMERS_CACHE'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'

import torch
from transformers import AutoTokenizer, AutoModel
from peft import PeftModel

print("=" * 70)
print("ğŸ§ª æµ‹è¯• ChatGLM å®¢æœæ¨¡å‹")
print("=" * 70)

# ============================================================================
# åŠ è½½æ¨¡å‹
# ============================================================================

def load_model(base_model="THUDM/chatglm-6b", lora_path="output/chatglm-customer-lora"):
    """åŠ è½½åŸºç¡€æ¨¡å‹å’Œ LoRA æƒé‡"""
    
    print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
    
    try:
        # åŠ è½½ tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            base_model,
            trust_remote_code=True
        )
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        model = AutoModel.from_pretrained(
            base_model,
            trust_remote_code=True,
            device_map="auto",
            torch_dtype=torch.float16
        )
        
        # åŠ è½½ LoRA æƒé‡
        if os.path.exists(lora_path):
            print(f"âœ… åŠ è½½ LoRA æƒé‡: {lora_path}")
            model = PeftModel.from_pretrained(model, lora_path)
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ° LoRA æƒé‡ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹")
        
        model = model.eval()
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None

# ============================================================================
# æµ‹è¯•å¯¹è¯
# ============================================================================

def test_chat(model, tokenizer):
    """æµ‹è¯•å®¢æœå¯¹è¯"""
    
    print("\n" + "=" * 70)
    print("ğŸ’¬ å®¢æœå¯¹è¯æµ‹è¯•")
    print("=" * 70)
    
    # æµ‹è¯•é—®é¢˜åˆ—è¡¨
    test_questions = [
        "å¦‚ä½•é€€è´§ï¼Ÿ",
        "å‘è´§éœ€è¦å¤šä¹…ï¼Ÿ",
        "æ”¯æŒå“ªäº›æ”¯ä»˜æ–¹å¼ï¼Ÿ",
        "å¯ä»¥ä¿®æ”¹æ”¶è´§åœ°å€å—ï¼Ÿ",
        "å¦‚ä½•è”ç³»å®¢æœï¼Ÿ",
        "ä¼šå‘˜æœ‰ä»€ä¹ˆæƒç›Šï¼Ÿ",
        "å•†å“å¯ä»¥æ¢è´§å—ï¼Ÿ",
        "å¦‚ä½•æŸ¥è¯¢ç‰©æµï¼Ÿ",
    ]
    
    history = []
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n{'=' * 70}")
        print(f"é—®é¢˜ {i}: {question}")
        print("-" * 70)
        
        try:
            response, history = model.chat(
                tokenizer,
                f"ç”¨æˆ·é—®ï¼š{question}",
                history=[]  # æ¯æ¬¡æ¸…ç©ºå†å²ï¼Œç‹¬ç«‹å¯¹è¯
            )
            
            print(f"å›ç­”: {response}")
            
        except Exception as e:
            print(f"âŒ å¯¹è¯å¤±è´¥: {e}")
    
    print("\n" + "=" * 70)

# ============================================================================
# äº¤äº’å¼æµ‹è¯•
# ============================================================================

def interactive_test(model, tokenizer):
    """äº¤äº’å¼æµ‹è¯•"""
    
    print("\n" + "=" * 70)
    print("ğŸ® äº¤äº’å¼æµ‹è¯•æ¨¡å¼")
    print("=" * 70)
    print("è¾“å…¥é—®é¢˜è¿›è¡Œæµ‹è¯•ï¼Œè¾“å…¥ 'quit' é€€å‡º")
    print("-" * 70)
    
    history = []
    
    while True:
        try:
            user_input = input("\nç”¨æˆ·: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            
            if not user_input:
                continue
            
            # æ·»åŠ "ç”¨æˆ·é—®ï¼š"å‰ç¼€
            question = f"ç”¨æˆ·é—®ï¼š{user_input}"
            
            response, history = model.chat(
                tokenizer,
                question,
                history=[]  # å•è½®å¯¹è¯
            )
            
            print(f"\nå®¢æœ: {response}")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")

# ============================================================================
# å¯¹æ¯”æµ‹è¯•
# ============================================================================

def compare_models(base_model_name="THUDM/chatglm-6b", lora_path="output/chatglm-customer-lora"):
    """å¯¹æ¯”åŸå§‹æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹"""
    
    print("\n" + "=" * 70)
    print("ğŸ“Š æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    print("=" * 70)
    
    # åŠ è½½åŸå§‹æ¨¡å‹
    print("\n1ï¸âƒ£ åŠ è½½åŸå§‹ ChatGLM-6B...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
        base_model = AutoModel.from_pretrained(
            base_model_name,
            trust_remote_code=True,
            device_map="auto",
            torch_dtype=torch.float16
        ).eval()
        print("âœ… åŸå§‹æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åŸå§‹æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # åŠ è½½å¾®è°ƒæ¨¡å‹
    print("\n2ï¸âƒ£ åŠ è½½å¾®è°ƒæ¨¡å‹...")
    try:
        finetuned_model = PeftModel.from_pretrained(base_model, lora_path).eval()
        print("âœ… å¾®è°ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ å¾®è°ƒæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # æµ‹è¯•é—®é¢˜
    test_question = "ç”¨æˆ·é—®ï¼šå¦‚ä½•é€€è´§ï¼Ÿ"
    
    print("\n" + "=" * 70)
    print(f"æµ‹è¯•é—®é¢˜: {test_question}")
    print("=" * 70)
    
    # åŸå§‹æ¨¡å‹å›ç­”
    print("\nã€åŸå§‹æ¨¡å‹å›ç­”ã€‘")
    print("-" * 70)
    try:
        response1, _ = base_model.chat(tokenizer, test_question, history=[])
        print(response1)
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
    
    # å¾®è°ƒæ¨¡å‹å›ç­”
    print("\nã€å¾®è°ƒæ¨¡å‹å›ç­”ã€‘")
    print("-" * 70)
    try:
        response2, _ = finetuned_model.chat(tokenizer, test_question, history=[])
        print(response2)
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
    
    print("\n" + "=" * 70)
    print("ğŸ’¡ è§‚å¯Ÿä¸¤ä¸ªæ¨¡å‹çš„å›ç­”å·®å¼‚")
    print("   å¾®è°ƒæ¨¡å‹åº”è¯¥æ›´ç¬¦åˆå®¢æœé£æ ¼ï¼Œå›ç­”æ›´è§„èŒƒ")
    print("=" * 70)

# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    import sys
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ LoRA æƒé‡
    lora_path = "output/chatglm-customer-lora"
    
    if not os.path.exists(lora_path):
        print("\nâš ï¸  æœªæ‰¾åˆ° LoRA æƒé‡")
        print(f"   è·¯å¾„: {lora_path}")
        print("\nè¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ï¼š")
        print("   python chatglm_lora_finetune.py")
        print("   æˆ–åŒå‡» å¼€å§‹è®­ç»ƒ.bat")
        return
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_model(lora_path=lora_path)
    
    if model is None:
        return
    
    # é€‰æ‹©æµ‹è¯•æ¨¡å¼
    print("\n" + "=" * 70)
    print("é€‰æ‹©æµ‹è¯•æ¨¡å¼ï¼š")
    print("=" * 70)
    print("1. è‡ªåŠ¨æµ‹è¯•ï¼ˆé¢„è®¾é—®é¢˜ï¼‰")
    print("2. äº¤äº’å¼æµ‹è¯•ï¼ˆæ‰‹åŠ¨è¾“å…¥ï¼‰")
    print("3. å¯¹æ¯”æµ‹è¯•ï¼ˆåŸå§‹ vs å¾®è°ƒï¼‰")
    print("4. å…¨éƒ¨æµ‹è¯•")
    print("=" * 70)
    
    choice = input("\nè¯·é€‰æ‹© (1-4): ").strip()
    
    if choice == "1":
        test_chat(model, tokenizer)
    elif choice == "2":
        interactive_test(model, tokenizer)
    elif choice == "3":
        compare_models()
    elif choice == "4":
        test_chat(model, tokenizer)
        print("\n" + "=" * 70)
        input("æŒ‰ Enter ç»§ç»­äº¤äº’å¼æµ‹è¯•...")
        interactive_test(model, tokenizer)
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤è¿è¡Œè‡ªåŠ¨æµ‹è¯•")
        test_chat(model, tokenizer)
    
    print("\n" + "=" * 70)
    print("âœ¨ æµ‹è¯•å®Œæˆï¼")
    print("=" * 70)

if __name__ == "__main__":
    main()
