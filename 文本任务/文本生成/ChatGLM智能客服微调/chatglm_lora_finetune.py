#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ChatGLM-6B LoRA å¾®è°ƒè„šæœ¬ - æ™ºèƒ½å®¢æœç‰ˆæœ¬
ä½¿ç”¨ LoRA æŠ€æœ¯å¾®è°ƒ ChatGLM-6Bï¼Œæ‰“é€ ä¸“å±å®¢æœåŠ©æ‰‹
"""

import os
os.environ['HF_HOME'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'
os.environ['TRANSFORMERS_CACHE'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'

import json
import torch
from dataclasses import dataclass, field
from typing import Optional
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset

print("=" * 70)
print("ğŸ¤– Qwen2.5-1.5B LoRA å¾®è°ƒ - æ™ºèƒ½å®¢æœ")
print("=" * 70)

# ============================================================================
# 1. é…ç½®å‚æ•°
# ============================================================================

@dataclass
class ModelArguments:
    """æ¨¡å‹å‚æ•°"""
    model_name_or_path: str = field(
        default="Qwen/Qwen2.5-1.5B-Instruct",
        metadata={"help": "Qwen2.5-1.5B æ¨¡å‹è·¯å¾„ (3GB, ä¸­æ–‡æ”¯æŒå¥½, å…¼å®¹æ€§å¼º)"}
    )

@dataclass
class DataArguments:
    """æ•°æ®å‚æ•°"""
    train_file: str = field(
        default="data/train.json",
        metadata={"help": "è®­ç»ƒæ•°æ®æ–‡ä»¶"}
    )
    validation_file: str = field(
        default="data/dev.json",
        metadata={"help": "éªŒè¯æ•°æ®æ–‡ä»¶"}
    )
    max_length: int = field(
        default=512,
        metadata={"help": "æœ€å¤§åºåˆ—é•¿åº¦"}
    )

@dataclass
class LoraArguments:
    """LoRA å‚æ•°"""
    lora_rank: int = field(
        default=8,
        metadata={"help": "LoRA ç§©"}
    )
    lora_alpha: int = field(
        default=32,
        metadata={"help": "LoRA alpha"}
    )
    lora_dropout: float = field(
        default=0.1,
        metadata={"help": "LoRA dropout"}
    )

# ============================================================================
# 2. åŠ è½½æ•°æ®
# ============================================================================

def load_data(file_path):
    """åŠ è½½ JSON æ ¼å¼çš„è®­ç»ƒæ•°æ®"""
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # æ„å»ºç»å¯¹è·¯å¾„
    abs_file_path = os.path.join(script_dir, file_path)
    
    print(f"\nğŸ“¥ åŠ è½½æ•°æ®: {abs_file_path}")
    
    with open(abs_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"âœ… åŠ è½½äº† {len(data)} æ¡æ•°æ®")
    
    # è½¬æ¢ä¸º Dataset æ ¼å¼
    dataset = Dataset.from_list(data)
    return dataset

# ============================================================================
# 3. æ•°æ®é¢„å¤„ç†
# ============================================================================

def preprocess_function(examples, tokenizer, max_length=512):
    """
    æ•°æ®é¢„å¤„ç†å‡½æ•°
    å°† instruction + input + output è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
    """
    model_inputs = {"input_ids": [], "labels": []}
    
    for i in range(len(examples["instruction"])):
        instruction = examples["instruction"][i]
        input_text = examples["input"][i]
        output_text = examples["output"][i]
        
        # æ„å»ºæç¤ºè¯
        if input_text:
            prompt = f"{instruction}\n{input_text}"
        else:
            prompt = instruction
        
        # æ„å»ºå®Œæ•´å¯¹è¯
        # ChatGLM æ ¼å¼ï¼š[Round 1]\n\né—®ï¼š{prompt}\n\nç­”ï¼š{output}
        a_ids = tokenizer.encode(text=prompt, add_special_tokens=True, truncation=True, max_length=max_length)
        b_ids = tokenizer.encode(text=output_text, add_special_tokens=False, truncation=True, max_length=max_length)
        
        context_length = len(a_ids)
        input_ids = a_ids + b_ids + [tokenizer.eos_token_id]
        labels = [tokenizer.pad_token_id] * context_length + b_ids + [tokenizer.eos_token_id]
        
        model_inputs["input_ids"].append(input_ids)
        model_inputs["labels"].append(labels)
    
    return model_inputs

# ============================================================================
# 4. ä¸»è®­ç»ƒå‡½æ•°
# ============================================================================

def main():
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # å‚æ•°è®¾ç½®
    model_args = ModelArguments()
    data_args = DataArguments()
    lora_args = LoraArguments()
    
    # è¾“å‡ºç›®å½•ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
    output_dir = os.path.join(script_dir, "output/chatglm-customer-lora")
    
    print("\n" + "=" * 70)
    print("âš™ï¸  è®­ç»ƒé…ç½®")
    print("=" * 70)
    print(f"æ¨¡å‹: {model_args.model_name_or_path}")
    print(f"è®­ç»ƒæ•°æ®: {data_args.train_file}")
    print(f"éªŒè¯æ•°æ®: {data_args.validation_file}")
    print(f"LoRA Rank: {lora_args.lora_rank}")
    print(f"LoRA Alpha: {lora_args.lora_alpha}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # ========================================================================
    # åŠ è½½ Tokenizer å’Œæ¨¡å‹
    # ========================================================================
    
    print("\n" + "=" * 70)
    print("ğŸ“¦ åŠ è½½æ¨¡å‹")
    print("=" * 70)
    
    try:
        print(f"æ­£åœ¨åŠ è½½ tokenizer...")
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_args.model_name_or_path,
            trust_remote_code=True
        )
        
        # ä¿®å¤ ChatGLM tokenizer å…¼å®¹æ€§é—®é¢˜
        if not hasattr(tokenizer, 'vocab_size'):
            tokenizer.vocab_size = len(tokenizer.get_vocab()) if hasattr(tokenizer, 'get_vocab') else 130528
        if not hasattr(tokenizer, 'pad_token_id') or tokenizer.pad_token_id is None:
            tokenizer.pad_token_id = tokenizer.eos_token_id if hasattr(tokenizer, 'eos_token_id') else 2
        
        print("âœ… Tokenizer åŠ è½½æˆåŠŸ")
        
        print(f"\næ­£åœ¨åŠ è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_args.model_name_or_path,
            trust_remote_code=True,
            device_map="auto",
            torch_dtype=torch.float16
        )
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("\nğŸ’¡ æç¤ºï¼š")
        print("1. é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ ChatGLM-6Bï¼ˆçº¦ 12GBï¼‰")
        print("2. è¯·ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸")
        print("3. æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„")
        return
    
    # ========================================================================
    # é…ç½® LoRA
    # ========================================================================
    
    print("\n" + "=" * 70)
    print("ğŸ”§ é…ç½® LoRA")
    print("=" * 70)
    
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=lora_args.lora_rank,
        lora_alpha=lora_args.lora_alpha,
        lora_dropout=lora_args.lora_dropout,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]  # Qwen2 çš„æ³¨æ„åŠ›å±‚
    )
    
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    
    # ========================================================================
    # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    # ========================================================================
    
    print("\n" + "=" * 70)
    print("ğŸ“Š å‡†å¤‡æ•°æ®")
    print("=" * 70)
    
    train_dataset = load_data(data_args.train_file)
    eval_dataset = load_data(data_args.validation_file)
    
    # é¢„å¤„ç†
    print("\nå¤„ç†è®­ç»ƒæ•°æ®...")
    train_dataset = train_dataset.map(
        lambda x: preprocess_function(x, tokenizer, data_args.max_length),
        batched=True,
        remove_columns=train_dataset.column_names
    )
    
    print("å¤„ç†éªŒè¯æ•°æ®...")
    eval_dataset = eval_dataset.map(
        lambda x: preprocess_function(x, tokenizer, data_args.max_length),
        batched=True,
        remove_columns=eval_dataset.column_names
    )
    
    print(f"âœ… è®­ç»ƒé›†: {len(train_dataset)} æ¡")
    print(f"âœ… éªŒè¯é›†: {len(eval_dataset)} æ¡")
    
    # ========================================================================
    # è®­ç»ƒå‚æ•°
    # ========================================================================
    
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=5e-4,
        warmup_steps=50,
        logging_steps=10,
        save_steps=100,
        eval_steps=100,
        evaluation_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        fp16=True,
        report_to="none",
        remove_unused_columns=False,
    )
    
    # Data collator
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        padding=True
    )
    
    # ========================================================================
    # å¼€å§‹è®­ç»ƒ
    # ========================================================================
    
    print("\n" + "=" * 70)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ")
    print("=" * 70)
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
    )
    
    try:
        trainer.train()
        
        print("\n" + "=" * 70)
        print("ğŸ’¾ ä¿å­˜æ¨¡å‹")
        print("=" * 70)
        
        # ä¿å­˜ LoRA æƒé‡
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
        
        print("\n" + "=" * 70)
        print("âœ¨ è®­ç»ƒå®Œæˆï¼")
        print("=" * 70)
        print("\nä¸‹ä¸€æ­¥ï¼š")
        print("1. è¿è¡Œ test_model.py æµ‹è¯•æ¨¡å‹")
        print("2. è¿è¡Œ å¯åŠ¨å®¢æœç³»ç»Ÿ.bat å¯åŠ¨ Web æœåŠ¡")
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        print("\nğŸ’¡ å¯èƒ½çš„åŸå› ï¼š")
        print("1. æ˜¾å­˜ä¸è¶³ - å°è¯•å‡å° batch_size")
        print("2. æ•°æ®æ ¼å¼é”™è¯¯ - æ£€æŸ¥ JSON æ–‡ä»¶")
        print("3. æ¨¡å‹åŠ è½½å¤±è´¥ - æ£€æŸ¥ç½‘ç»œè¿æ¥")

if __name__ == "__main__":
    main()
