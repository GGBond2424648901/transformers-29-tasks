# ğŸŒ æœºå™¨ç¿»è¯‘ï¼ˆMachine Translationï¼‰

## ğŸ“– ä»»åŠ¡ç®€ä»‹

æœºå™¨ç¿»è¯‘æ˜¯å°†æ–‡æœ¬ä»ä¸€ç§è¯­è¨€è‡ªåŠ¨ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€çš„ä»»åŠ¡ã€‚ç°ä»£æœºå™¨ç¿»è¯‘ä¸»è¦ä½¿ç”¨ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆNMTï¼‰ï¼Œèƒ½å¤Ÿç”Ÿæˆæµç•…ã€å‡†ç¡®çš„è¯‘æ–‡ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

- **ğŸŒ è·¨è¯­è¨€äº¤æµ**: å®æ—¶ç¿»è¯‘èŠå¤©ã€é‚®ä»¶
- **ğŸ“š æ–‡æ¡£ç¿»è¯‘**: æŠ€æœ¯æ–‡æ¡£ã€åˆåŒã€è®ºæ–‡ç¿»è¯‘
- **ğŸ¬ å­—å¹•ç¿»è¯‘**: è§†é¢‘ã€ç”µå½±å­—å¹•
- **ğŸ›ï¸ ç”µå•†å›½é™…åŒ–**: å•†å“æè¿°å¤šè¯­è¨€å±•ç¤º
- **ğŸ“± åº”ç”¨æœ¬åœ°åŒ–**: è½¯ä»¶ç•Œé¢å¤šè¯­è¨€æ”¯æŒ

## ğŸ“ æ–‡ä»¶è¯´æ˜

- `run_translation.py` - ä½¿ç”¨ Trainer API è®­ç»ƒç¿»è¯‘æ¨¡å‹
- `run_translation_no_trainer.py` - ä¸ä½¿ç”¨ Trainer è®­ç»ƒ
- `requirements.txt` - ä¾èµ–åŒ…åˆ—è¡¨
- `README.md` - æœ¬æ–‡ä»¶

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. è¿è¡Œè®­ç»ƒ

#### ä½¿ç”¨ WMT æ•°æ®é›†

```bash
python run_translation.py \
    --model_name_or_path Helsinki-NLP/opus-mt-en-zh \
    --dataset_name wmt16 \
    --dataset_config_name ro-en \
    --source_lang en \
    --target_lang zh \
    --output_dir ./output \
    --do_train \
    --do_eval \
    --per_device_train_batch_size 8 \
    --num_train_epochs 3 \
    --predict_with_generate
```

#### ä½¿ç”¨æœ¬åœ°æ•°æ®

```bash
python run_translation.py \
    --model_name_or_path Helsinki-NLP/opus-mt-en-zh \
    --train_file train.json \
    --validation_file val.json \
    --source_lang en \
    --target_lang zh \
    --output_dir ./output \
    --do_train \
    --do_eval
```

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### Pipeline æ¨ç†

```python
from transformers import pipeline

# è‹±è¯‘ä¸­
translator_en_zh = pipeline(
    "translation",
    model="Helsinki-NLP/opus-mt-en-zh"
)

result = translator_en_zh("Hello, how are you?")
print(result[0]['translation_text'])
# è¾“å‡ºï¼šä½ å¥½ï¼Œä½ å¥½å—ï¼Ÿ

# ä¸­è¯‘è‹±
translator_zh_en = pipeline(
    "translation",
    model="Helsinki-NLP/opus-mt-zh-en"
)

result = translator_zh_en("ä»Šå¤©å¤©æ°”çœŸå¥½ï¼")
print(result[0]['translation_text'])
# è¾“å‡ºï¼šThe weather is really nice today!
```

### æ‰¹é‡ç¿»è¯‘

```python
texts = [
    "Good morning!",
    "How are you?",
    "Nice to meet you."
]

translations = translator_en_zh(texts)

for text, trans in zip(texts, translations):
    print(f"{text} -> {trans['translation_text']}")
```

### è®­ç»ƒè‡ªå®šä¹‰ç¿»è¯‘æ¨¡å‹

```python
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq
)
from datasets import load_dataset

# 1. åŠ è½½æ•°æ®é›†
dataset = load_dataset("wmt16", "ro-en")

# 2. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "Helsinki-NLP/opus-mt-en-ro"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# 3. æ•°æ®é¢„å¤„ç†
source_lang = "en"
target_lang = "ro"

def preprocess_function(examples):
    inputs = [ex[source_lang] for ex in examples["translation"]]
    targets = [ex[target_lang] for ex in examples["translation"]]
    
    model_inputs = tokenizer(
        inputs,
        max_length=128,
        truncation=True
    )
    
    labels = tokenizer(
        targets,
        max_length=128,
        truncation=True
    )
    
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset.map(
    preprocess_function,
    batched=True
)

# 4. è®­ç»ƒé…ç½®
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    num_train_epochs=3,
    predict_with_generate=True,
)

# 5. æ•°æ®æ•´ç†å™¨
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model
)

# 6. åˆ›å»º Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    data_collator=data_collator,
)

# 7. å¼€å§‹è®­ç»ƒ
trainer.train()
```

## ğŸ¨ æ¨èæ¨¡å‹

### Helsinki-NLP OPUS-MT ç³»åˆ—

| æ¨¡å‹ | è¯­è¨€å¯¹ | ç‰¹ç‚¹ |
|------|--------|------|
| Helsinki-NLP/opus-mt-en-zh | è‹±â†’ä¸­ | è½»é‡çº§ï¼Œé€Ÿåº¦å¿« |
| Helsinki-NLP/opus-mt-zh-en | ä¸­â†’è‹± | è½»é‡çº§ï¼Œé€Ÿåº¦å¿« |
| Helsinki-NLP/opus-mt-en-de | è‹±â†’å¾· | æ¬§æ´²è¯­è¨€æ•ˆæœå¥½ |
| Helsinki-NLP/opus-mt-en-fr | è‹±â†’æ³• | æ¬§æ´²è¯­è¨€æ•ˆæœå¥½ |

### å…¶ä»–æ¨¡å‹

| æ¨¡å‹ | ç‰¹ç‚¹ |
|------|------|
| facebook/mbart-large-50-many-to-many-mmt | å¤šè¯­è¨€äº’è¯‘ï¼Œæ”¯æŒ 50 ç§è¯­è¨€ |
| google/mt5-base | å¤šè¯­è¨€ T5ï¼Œæ”¯æŒç¿»è¯‘ä»»åŠ¡ |
| facebook/nllb-200-distilled-600M | æ”¯æŒ 200 ç§è¯­è¨€ |

## ğŸ“Š æ•°æ®æ ¼å¼

### JSON æ ¼å¼

```json
[
    {
        "en": "Hello, world!",
        "zh": "ä½ å¥½ï¼Œä¸–ç•Œï¼"
    },
    {
        "en": "Good morning.",
        "zh": "æ—©ä¸Šå¥½ã€‚"
    }
]
```

### CSV æ ¼å¼

```csv
source,target
"Hello, world!","ä½ å¥½ï¼Œä¸–ç•Œï¼"
"Good morning.","æ—©ä¸Šå¥½ã€‚"
```

## âš™ï¸ é‡è¦å‚æ•°

### ç”Ÿæˆå‚æ•°

```python
translator(
    text,
    max_length=128,        # æœ€å¤§ç¿»è¯‘é•¿åº¦
    num_beams=5,           # Beam search æ•°é‡
    early_stopping=True,   # æ—©åœ
    length_penalty=1.0     # é•¿åº¦æƒ©ç½š
)
```

### è®­ç»ƒå‚æ•°

```bash
--source_lang en               # æºè¯­è¨€
--target_lang zh               # ç›®æ ‡è¯­è¨€
--max_source_length 128        # æºæ–‡æœ¬æœ€å¤§é•¿åº¦
--max_target_length 128        # è¯‘æ–‡æœ€å¤§é•¿åº¦
--num_beams 5                  # Beam search
--predict_with_generate        # ä½¿ç”¨ç”Ÿæˆæ¨¡å¼è¯„ä¼°
```

## ğŸ’¡ ä½¿ç”¨æŠ€å·§

### 1. å¤šè¯­è¨€ç¿»è¯‘

```python
# ä½¿ç”¨ mBART è¿›è¡Œå¤šè¯­è¨€ç¿»è¯‘
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast

model = MBartForConditionalGeneration.from_pretrained(
    "facebook/mbart-large-50-many-to-many-mmt"
)
tokenizer = MBart50TokenizerFast.from_pretrained(
    "facebook/mbart-large-50-many-to-many-mmt"
)

# ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡
tokenizer.src_lang = "zh_CN"
encoded = tokenizer("ä»Šå¤©å¤©æ°”çœŸå¥½ï¼", return_tensors="pt")
generated_tokens = model.generate(
    **encoded,
    forced_bos_token_id=tokenizer.lang_code_to_id["en_XX"]
)
translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
print(translation[0])
```

### 2. å¤„ç†é•¿æ–‡æœ¬

```python
def translate_long_text(text, max_length=500):
    # æŒ‰å¥å­åˆ†å‰²
    sentences = text.split('ã€‚')
    
    # åˆ†æ‰¹ç¿»è¯‘
    translations = []
    for sentence in sentences:
        if sentence.strip():
            result = translator(sentence + 'ã€‚')
            translations.append(result[0]['translation_text'])
    
    return ' '.join(translations)
```

### 3. æé«˜ç¿»è¯‘è´¨é‡

```python
# ä½¿ç”¨æ›´å¤šçš„ beam
result = translator(
    text,
    num_beams=10,          # å¢åŠ  beam æ•°é‡
    length_penalty=1.2,    # è°ƒæ•´é•¿åº¦æƒ©ç½š
    early_stopping=True
)

# ä½¿ç”¨é‡‡æ ·
result = translator(
    text,
    do_sample=True,
    top_k=50,
    top_p=0.95,
    temperature=0.7
)
```

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

### BLEU åˆ†æ•°

```python
from datasets import load_metric

bleu = load_metric("sacrebleu")

predictions = ["The weather is nice today."]
references = [["Today's weather is good."]]

results = bleu.compute(
    predictions=predictions,
    references=references
)

print(f"BLEU: {results['score']:.2f}")
```

### å…¶ä»–æŒ‡æ ‡

- **BLEU**: æœ€å¸¸ç”¨çš„æœºå™¨ç¿»è¯‘è¯„ä¼°æŒ‡æ ‡
- **METEOR**: è€ƒè™‘åŒä¹‰è¯å’Œè¯å¹²
- **TER**: ç¿»è¯‘ç¼–è¾‘ç‡
- **chrF**: å­—ç¬¦çº§ F-score

## ğŸ¯ åº”ç”¨ç¤ºä¾‹

### 1. å®æ—¶èŠå¤©ç¿»è¯‘

```python
def chat_translator(message, source_lang="en", target_lang="zh"):
    # åŠ è½½å¯¹åº”çš„ç¿»è¯‘æ¨¡å‹
    model_name = f"Helsinki-NLP/opus-mt-{source_lang}-{target_lang}"
    translator = pipeline("translation", model=model_name)
    
    result = translator(message)
    return result[0]['translation_text']

# ä½¿ç”¨
user_message = "Hello, how can I help you?"
translated = chat_translator(user_message, "en", "zh")
print(translated)
```

### 2. æ–‡æ¡£ç¿»è¯‘

```python
def translate_document(file_path, source_lang="en", target_lang="zh"):
    # è¯»å–æ–‡æ¡£
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æŒ‰æ®µè½åˆ†å‰²
    paragraphs = content.split('\n\n')
    
    # ç¿»è¯‘æ¯ä¸ªæ®µè½
    translator = pipeline(
        "translation",
        model=f"Helsinki-NLP/opus-mt-{source_lang}-{target_lang}"
    )
    
    translated_paragraphs = []
    for para in paragraphs:
        if para.strip():
            result = translator(para)
            translated_paragraphs.append(result[0]['translation_text'])
    
    # ä¿å­˜ç¿»è¯‘ç»“æœ
    output_path = file_path.replace('.txt', f'_{target_lang}.txt')
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n\n'.join(translated_paragraphs))
    
    return output_path
```

### 3. å­—å¹•ç¿»è¯‘

```python
def translate_subtitles(srt_file, source_lang="en", target_lang="zh"):
    import pysrt
    
    # è¯»å–å­—å¹•
    subs = pysrt.open(srt_file)
    
    # ç¿»è¯‘
    translator = pipeline(
        "translation",
        model=f"Helsinki-NLP/opus-mt-{source_lang}-{target_lang}"
    )
    
    for sub in subs:
        result = translator(sub.text)
        sub.text = result[0]['translation_text']
    
    # ä¿å­˜
    output_file = srt_file.replace('.srt', f'_{target_lang}.srt')
    subs.save(output_file, encoding='utf-8')
    
    return output_file
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **è¯­è¨€å¯¹**: ä¸åŒè¯­è¨€å¯¹çš„ç¿»è¯‘è´¨é‡å·®å¼‚å¾ˆå¤§
2. **ä¸“ä¸šæœ¯è¯­**: ä¸“ä¸šé¢†åŸŸå¯èƒ½éœ€è¦å¾®è°ƒæ¨¡å‹
3. **æ–‡åŒ–å·®å¼‚**: ç¿»è¯‘éœ€è¦è€ƒè™‘æ–‡åŒ–èƒŒæ™¯
4. **æ–‡æœ¬é•¿åº¦**: æ³¨æ„æ¨¡å‹çš„æœ€å¤§é•¿åº¦é™åˆ¶
5. **è®¡ç®—èµ„æº**: å¤§æ¨¡å‹éœ€è¦ GPU åŠ é€Ÿ

## ğŸ”— ç›¸å…³èµ„æº

- [OPUS-MT æ¨¡å‹é›†åˆ](https://huggingface.co/Helsinki-NLP)
- [mBART è®ºæ–‡](https://arxiv.org/abs/2001.08210)
- [WMT ç¿»è¯‘ç«èµ›](https://www.statmt.org/wmt21/)
- [BLEU è¯„ä¼°æŒ‡æ ‡](https://en.wikipedia.org/wiki/BLEU)
