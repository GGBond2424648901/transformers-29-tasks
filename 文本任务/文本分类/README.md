# ğŸ“Š æ–‡æœ¬åˆ†ç±»ï¼ˆText Classificationï¼‰

## ğŸ“– ä»»åŠ¡ç®€ä»‹

æ–‡æœ¬åˆ†ç±»æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æœ€åŸºç¡€å’Œå¸¸è§çš„ä»»åŠ¡ä¹‹ä¸€ï¼Œç›®æ ‡æ˜¯å°†æ–‡æœ¬åˆ†é…åˆ°é¢„å®šä¹‰çš„ç±»åˆ«ä¸­ã€‚è¿™æ˜¯ä¸€ä¸ªç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œéœ€è¦æ ‡æ³¨çš„è®­ç»ƒæ•°æ®ã€‚

## ğŸ¯ åº”ç”¨åœºæ™¯

- **ğŸ˜Š æƒ…æ„Ÿåˆ†æ**: åˆ¤æ–­æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼ˆæ­£é¢/è´Ÿé¢/ä¸­æ€§ï¼‰
- **ğŸ“§ åƒåœ¾é‚®ä»¶è¿‡æ»¤**: è¯†åˆ«åƒåœ¾é‚®ä»¶å’Œæ­£å¸¸é‚®ä»¶
- **ğŸ“° æ–°é—»åˆ†ç±»**: å°†æ–°é—»åˆ†ç±»åˆ°ä¸åŒä¸»é¢˜ï¼ˆç§‘æŠ€ã€ä½“è‚²ã€å¨±ä¹ç­‰ï¼‰
- **ğŸ·ï¸ ä¸»é¢˜æ ‡æ³¨**: ä¸ºæ–‡æ¡£è‡ªåŠ¨æ·»åŠ ä¸»é¢˜æ ‡ç­¾
- **âš ï¸ å†…å®¹å®¡æ ¸**: è¯†åˆ«ä¸å½“å†…å®¹ã€æ•æ„Ÿä¿¡æ¯

## ğŸ“ æ–‡ä»¶è¯´æ˜

- `run_classification.py` - é€šç”¨æ–‡æœ¬åˆ†ç±»è®­ç»ƒè„šæœ¬
- `run_glue.py` - GLUE åŸºå‡†æµ‹è¯•è®­ç»ƒè„šæœ¬ï¼ˆä½¿ç”¨ Trainerï¼‰
- `run_glue_no_trainer.py` - GLUE è®­ç»ƒè„šæœ¬ï¼ˆä¸ä½¿ç”¨ Trainerï¼‰
- `run_xnli.py` - è·¨è¯­è¨€è‡ªç„¶è¯­è¨€æ¨ç†è®­ç»ƒ
- `requirements.txt` - ä¾èµ–åŒ…åˆ—è¡¨
- `README.md` - æœ¬æ–‡ä»¶

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. å‡†å¤‡æ•°æ®é›†

æ–‡æœ¬åˆ†ç±»éœ€è¦æ ‡æ³¨æ•°æ®ï¼Œæ ¼å¼é€šå¸¸ä¸ºï¼š

```
æ–‡æœ¬,æ ‡ç­¾
è¿™éƒ¨ç”µå½±çœŸå¥½çœ‹,æ­£é¢
æœåŠ¡æ€åº¦å¤ªå·®äº†,è´Ÿé¢
ä»·æ ¼è¿˜å¯ä»¥,ä¸­æ€§
```

### 3. è¿è¡Œè®­ç»ƒ

#### ä½¿ç”¨ Hugging Face æ•°æ®é›†

```bash
python run_classification.py \
    --model_name_or_path bert-base-chinese \
    --dataset_name ydshieh/coco_dataset_script \
    --output_dir ./output \
    --do_train \
    --do_eval \
    --per_device_train_batch_size 8 \
    --num_train_epochs 3
```

#### ä½¿ç”¨æœ¬åœ° CSV æ–‡ä»¶

```bash
python run_classification.py \
    --model_name_or_path bert-base-chinese \
    --train_file train.csv \
    --validation_file val.csv \
    --text_column_name text \
    --label_column_name label \
    --output_dir ./output \
    --do_train \
    --do_eval
```

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### Pipeline æ¨ç†ï¼ˆæ— éœ€è®­ç»ƒï¼‰

```python
from transformers import pipeline

# åˆ›å»ºåˆ†ç±»å™¨
classifier = pipeline(
    "text-classification",
    model="bert-base-chinese"
)

# åˆ†ç±»å•ä¸ªæ–‡æœ¬
result = classifier("è¿™éƒ¨ç”µå½±çœŸçš„å¤ªç²¾å½©äº†ï¼")
print(result)
# [{'label': 'POSITIVE', 'score': 0.98}]

# æ‰¹é‡åˆ†ç±»
texts = [
    "äº§å“è´¨é‡å¾ˆå¥½",
    "æœåŠ¡æ€åº¦å¤ªå·®",
    "ä»·æ ¼é€‚ä¸­"
]
results = classifier(texts)
```

### è®­ç»ƒè‡ªå®šä¹‰åˆ†ç±»å™¨

```python
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments
)
from datasets import load_dataset

# 1. åŠ è½½æ•°æ®é›†
dataset = load_dataset("csv", data_files={
    "train": "train.csv",
    "test": "test.csv"
})

# 2. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "bert-base-chinese"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=3  # ç±»åˆ«æ•°é‡
)

# 3. æ•°æ®é¢„å¤„ç†
def preprocess_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding=True
    )

tokenized_dataset = dataset.map(preprocess_function, batched=True)

# 4. è®­ç»ƒé…ç½®
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
)

# 5. åˆ›å»º Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
)

# 6. å¼€å§‹è®­ç»ƒ
trainer.train()

# 7. ä¿å­˜æ¨¡å‹
trainer.save_model("./my_classifier")
```

## ğŸ¨ æ¨èæ¨¡å‹

### ä¸­æ–‡æ¨¡å‹

| æ¨¡å‹ | å¤§å° | ç‰¹ç‚¹ |
|------|------|------|
| bert-base-chinese | ä¸­ | é€šç”¨ä¸­æ–‡ BERTï¼Œæ•ˆæœç¨³å®š |
| hfl/chinese-roberta-wwm-ext | ä¸­ | ä¸­æ–‡ RoBERTaï¼Œæ•ˆæœæ›´å¥½ |
| hfl/chinese-bert-wwm-ext | ä¸­ | å…¨è¯æ©ç  BERT |
| uer/roberta-base-finetuned-chinanews-chinese | ä¸­ | é’ˆå¯¹ä¸­æ–‡æ–°é—»ä¼˜åŒ– |

### è‹±æ–‡æ¨¡å‹

| æ¨¡å‹ | å¤§å° | ç‰¹ç‚¹ |
|------|------|------|
| bert-base-uncased | ä¸­ | é€šç”¨è‹±æ–‡ BERT |
| roberta-base | ä¸­ | è‹±æ–‡ RoBERTaï¼Œæ•ˆæœæ›´å¥½ |
| distilbert-base-uncased | å° | è½»é‡çº§ï¼Œé€Ÿåº¦å¿« |
| albert-base-v2 | å° | å‚æ•°å…±äº«ï¼Œå†…å­˜å ç”¨å° |

## ğŸ“Š æ•°æ®æ ¼å¼

### CSV æ ¼å¼

```csv
text,label
è¿™éƒ¨ç”µå½±çœŸå¥½çœ‹,1
æœåŠ¡æ€åº¦å¤ªå·®äº†,0
ä»·æ ¼è¿˜å¯ä»¥,2
```

### JSON æ ¼å¼

```json
[
    {"text": "è¿™éƒ¨ç”µå½±çœŸå¥½çœ‹", "label": 1},
    {"text": "æœåŠ¡æ€åº¦å¤ªå·®äº†", "label": 0},
    {"text": "ä»·æ ¼è¿˜å¯ä»¥", "label": 2}
]
```

### Hugging Face Dataset

```python
from datasets import load_dataset

# ä» Hub åŠ è½½
dataset = load_dataset("glue", "sst2")

# ä»æœ¬åœ°æ–‡ä»¶åŠ è½½
dataset = load_dataset("csv", data_files="data.csv")
```

## âš™ï¸ é‡è¦å‚æ•°

### è®­ç»ƒå‚æ•°

```bash
--model_name_or_path bert-base-chinese  # é¢„è®­ç»ƒæ¨¡å‹
--num_train_epochs 3                     # è®­ç»ƒè½®æ•°
--per_device_train_batch_size 16         # æ‰¹æ¬¡å¤§å°
--learning_rate 2e-5                     # å­¦ä¹ ç‡
--weight_decay 0.01                      # æƒé‡è¡°å‡
--warmup_steps 500                       # é¢„çƒ­æ­¥æ•°
--max_seq_length 128                     # æœ€å¤§åºåˆ—é•¿åº¦
```

### æ•°æ®å‚æ•°

```bash
--train_file train.csv                   # è®­ç»ƒæ–‡ä»¶
--validation_file val.csv                # éªŒè¯æ–‡ä»¶
--test_file test.csv                     # æµ‹è¯•æ–‡ä»¶
--text_column_name text                  # æ–‡æœ¬åˆ—å
--label_column_name label                # æ ‡ç­¾åˆ—å
```

### è¾“å‡ºå‚æ•°

```bash
--output_dir ./output                    # è¾“å‡ºç›®å½•
--save_strategy epoch                    # ä¿å­˜ç­–ç•¥
--save_total_limit 3                     # ä¿å­˜æ£€æŸ¥ç‚¹æ•°é‡
--load_best_model_at_end                 # åŠ è½½æœ€ä½³æ¨¡å‹
```

## ğŸ’¡ è®­ç»ƒæŠ€å·§

### 1. æ•°æ®ä¸å¹³è¡¡

```python
from sklearn.utils.class_weight import compute_class_weight

# è®¡ç®—ç±»åˆ«æƒé‡
class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(labels),
    y=labels
)

# åœ¨è®­ç»ƒæ—¶ä½¿ç”¨
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    compute_loss=lambda model, inputs: weighted_loss(model, inputs, class_weights)
)
```

### 2. æ•°æ®å¢å¼º

```python
# ä½¿ç”¨åŒä¹‰è¯æ›¿æ¢
from nlpaug.augmenter.word import SynonymAug

aug = SynonymAug(aug_src='wordnet')
augmented_text = aug.augment(text)

# å›è¯‘å¢å¼º
# ä¸­æ–‡ -> è‹±æ–‡ -> ä¸­æ–‡
```

### 3. è¶…å‚æ•°è°ƒä¼˜

```python
from transformers import Trainer

# ä½¿ç”¨ Optuna è¿›è¡Œè¶…å‚æ•°æœç´¢
def model_init():
    return AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=num_labels
    )

trainer = Trainer(
    model_init=model_init,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

# è¶…å‚æ•°æœç´¢
best_trial = trainer.hyperparameter_search(
    direction="maximize",
    backend="optuna",
    n_trials=10
)
```

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

### å¸¸ç”¨æŒ‡æ ‡

- **å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰**: æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬æ¯”ä¾‹
- **ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰**: é¢„æµ‹ä¸ºæ­£çš„æ ·æœ¬ä¸­çœŸæ­£ä¸ºæ­£çš„æ¯”ä¾‹
- **å¬å›ç‡ï¼ˆRecallï¼‰**: çœŸæ­£ä¸ºæ­£çš„æ ·æœ¬ä¸­è¢«é¢„æµ‹ä¸ºæ­£çš„æ¯”ä¾‹
- **F1 åˆ†æ•°**: ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡

### è®¡ç®—ç¤ºä¾‹

```python
from sklearn.metrics import classification_report

# é¢„æµ‹
predictions = trainer.predict(test_dataset)
preds = predictions.predictions.argmax(-1)

# è¯„ä¼°
report = classification_report(
    test_dataset['label'],
    preds,
    target_names=['è´Ÿé¢', 'ä¸­æ€§', 'æ­£é¢']
)
print(report)
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ•°æ®è´¨é‡**: æ ‡æ³¨æ•°æ®çš„è´¨é‡ç›´æ¥å½±å“æ¨¡å‹æ•ˆæœ
2. **ç±»åˆ«å¹³è¡¡**: æ³¨æ„å„ç±»åˆ«æ ·æœ¬æ•°é‡çš„å¹³è¡¡
3. **æ–‡æœ¬é•¿åº¦**: è¶…è¿‡æ¨¡å‹æœ€å¤§é•¿åº¦çš„æ–‡æœ¬ä¼šè¢«æˆªæ–­
4. **è¿‡æ‹Ÿåˆ**: å°æ•°æ®é›†å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œéœ€è¦æ­£åˆ™åŒ–
5. **è¯„ä¼°æŒ‡æ ‡**: æ ¹æ®ä»»åŠ¡é€‰æ‹©åˆé€‚çš„è¯„ä¼°æŒ‡æ ‡

## ğŸ”— ç›¸å…³èµ„æº

- [GLUE åŸºå‡†æµ‹è¯•](https://gluebenchmark.com/)
- [Hugging Face æ–‡æœ¬åˆ†ç±»æ•™ç¨‹](https://huggingface.co/docs/transformers/tasks/sequence_classification)
- [BERT è®ºæ–‡](https://arxiv.org/abs/1810.04805)
- [æƒ…æ„Ÿåˆ†æå®Œæ•´ç¤ºä¾‹](../../æƒ…æ„Ÿåˆ†æ/)
