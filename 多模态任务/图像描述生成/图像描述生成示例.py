#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å›¾åƒæè¿°ç”Ÿæˆç¤ºä¾‹
è‡ªåŠ¨ä¸ºå›¾åƒç”Ÿæˆæ–‡å­—æè¿°
"""

import os
os.environ['HF_HOME'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'
os.environ['TRANSFORMERS_CACHE'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'

from transformers import pipeline
from PIL import Image
import requests
from io import BytesIO

print("=" * 70)
print("ğŸ“¸ğŸ’¬ å›¾åƒæè¿°ç”Ÿæˆç¤ºä¾‹")
print("=" * 70)

# åˆ›å»ºå›¾åƒæè¿°ç”Ÿæˆå™¨
captioner = pipeline("image-to-text", model="Salesforce/blip-image-captioning-base")

print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")

# åŠ è½½å›¾åƒ
image_url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
image = Image.open(BytesIO(requests.get(image_url).content))

print("\nğŸ“¸ ç”Ÿæˆå›¾åƒæè¿°...")

# ç”Ÿæˆæè¿°
result = captioner(image)
print(f"\næè¿°: {result[0]['generated_text']}")

# ç”Ÿæˆå¤šä¸ªæè¿°
print("\nğŸ“ ç”Ÿæˆå¤šä¸ªå€™é€‰æè¿°ï¼š")
results = captioner(image, max_new_tokens=50, num_beams=5, num_return_sequences=3)

for i, res in enumerate(results, 1):
    print(f"{i}. {res['generated_text']}")

print("""
\nåº”ç”¨åœºæ™¯ï¼š
- â™¿ æ— éšœç¢è¾…åŠ© - ä¸ºè§†éšœäººå£«æè¿°å›¾åƒ
- ğŸ” å›¾ç‰‡ SEO - è‡ªåŠ¨ç”Ÿæˆ alt æ–‡æœ¬
- ğŸ“± ç¤¾äº¤åª’ä½“ - è‡ªåŠ¨ç”Ÿæˆå›¾ç‰‡è¯´æ˜
- ğŸ“š å†…å®¹ç®¡ç† - å›¾ç‰‡è‡ªåŠ¨æ ‡æ³¨

ä½¿ç”¨æŠ€å·§ï¼š
1. è°ƒæ•´ max_new_tokens æ§åˆ¶æè¿°é•¿åº¦
2. ä½¿ç”¨ num_beams æé«˜æè¿°è´¨é‡
3. num_return_sequences ç”Ÿæˆå¤šä¸ªå€™é€‰

æ¨èæ¨¡å‹ï¼š
- Salesforce/blip-image-captioning-base: é€šç”¨æè¿°
- Salesforce/blip2-opt-2.7b: æ›´è¯¦ç»†çš„æè¿°
- nlpconnect/vit-gpt2-image-captioning: è½»é‡çº§
""")

print("\nâœ¨ ç¤ºä¾‹å®Œæˆï¼")
