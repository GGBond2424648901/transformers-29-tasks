#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ç¤ºä¾‹
æ ¹æ®å›¾åƒå›ç­”é—®é¢˜
"""

import os
os.environ['HF_HOME'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'
os.environ['TRANSFORMERS_CACHE'] = r'D:\transformersè®­ç»ƒ\transformers-main\é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½å¤„'

from transformers import pipeline
from PIL import Image
import requests
from io import BytesIO

print("=" * 70)
print("ğŸ‘ï¸ğŸ’¬ è§†è§‰é—®ç­”ç¤ºä¾‹")
print("=" * 70)

# åˆ›å»º VQA pipeline
vqa = pipeline("visual-question-answering", model="Salesforce/blip-vqa-base")

print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")

# åŠ è½½å›¾åƒ
image_url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
image = Image.open(BytesIO(requests.get(image_url).content))

# æé—®
questions = [
    "What animal is in the image?",
    "What color is the cat?",
    "Is the cat sitting or standing?",
    "Where is the cat?"
]

print("\nğŸ“¸ å›¾åƒå·²åŠ è½½")
print("\nğŸ¤” å¼€å§‹æé—®ï¼š\n")

for question in questions:
    result = vqa(image=image, question=question)
    print(f"Q: {question}")
    print(f"A: {result[0]['answer']} (ç½®ä¿¡åº¦: {result[0]['score']:.2%})\n")

print("""
åº”ç”¨åœºæ™¯ï¼š
- ğŸ›ï¸ æ™ºèƒ½å®¢æœ - å•†å“å’¨è¯¢
- ğŸ” å›¾ç‰‡æœç´¢ - å†…å®¹ç†è§£
- â™¿ æ— éšœç¢è¾…åŠ© - å›¾åƒæè¿°
- ğŸ“š æ•™è‚² - å›¾åƒé—®ç­”

ä½¿ç”¨æŠ€å·§ï¼š
1. é—®é¢˜è¦å…·ä½“æ˜ç¡®
2. é¿å…éœ€è¦æ¨ç†çš„å¤æ‚é—®é¢˜
3. æ”¯æŒå¤šç§è¯­è¨€ï¼ˆéœ€è¦å¯¹åº”æ¨¡å‹ï¼‰

æ¨èæ¨¡å‹ï¼š
- Salesforce/blip-vqa-base: é€šç”¨ VQA
- Salesforce/blip2-opt-2.7b: æ›´å¼ºå¤§çš„æ¨¡å‹
- dandelin/vilt-b32-finetuned-vqa: ViLT æ¶æ„
""")

print("\nâœ¨ ç¤ºä¾‹å®Œæˆï¼")
