# è®­ç»ƒæ¨¡å‹ä½¿ç”¨æŒ‡å—

## ğŸ“š æ–‡æ¡£ç´¢å¼•

| æ–‡æ¡£ | è¯´æ˜ |
|------|------|
| `è®­ç»ƒæ¨¡å‹è¯¦è§£.md` | è¯¦ç»†è§£é‡Šæ¨¡å‹çš„ç»“æ„ã€å‚æ•°ã€åŸç† |
| `æ¨¡å‹ä½¿ç”¨æŒ‡å—.md` | æœ¬æ–‡æ¡£ï¼Œå¿«é€Ÿä¸Šæ‰‹æŒ‡å— |
| `Trainer_å®æˆ˜ç¤ºä¾‹.py` | è®­ç»ƒæ¨¡å‹çš„å®Œæ•´ä»£ç  |
| `ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹.py` | ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹ |
| `æ¨¡å‹éƒ¨ç½²ç¤ºä¾‹.py` | å°†æ¨¡å‹éƒ¨ç½²ä¸º Web API |
| `æµ‹è¯•APIå®¢æˆ·ç«¯.py` | æµ‹è¯• API æœåŠ¡ |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1ï¸âƒ£ ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆå‘½ä»¤è¡Œï¼‰

```bash
# è¿è¡Œé¢„æµ‹è„šæœ¬
D:\aaaalokda\envs\myenv\python.exe ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹.py
```

**åŠŸèƒ½ï¼š**
- è‡ªåŠ¨åŠ è½½æ¨¡å‹
- å¯¹é¢„è®¾æ–‡æœ¬è¿›è¡Œé¢„æµ‹
- æ”¯æŒäº¤äº’å¼è¾“å…¥

**ç¤ºä¾‹è¾“å‡ºï¼š**
```
============================================================
åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...
============================================================
âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼
âœ“ æ¨¡å‹ç±»å‹: bert
âœ“ å‚æ•°é‡: 102,269,186

============================================================
å¼€å§‹é¢„æµ‹...
============================================================

1. æ–‡æœ¬: è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œéå¸¸æ»¡æ„ï¼
   é¢„æµ‹: æ­£é¢ ğŸ˜Š
   ç½®ä¿¡åº¦: 94.68%
   è¯¦ç»†: æ­£é¢=94.68%, è´Ÿé¢=5.32%

2. æ–‡æœ¬: å¤ªå·®äº†ï¼Œå®Œå…¨ä¸å€¼è¿™ä¸ªä»·æ ¼
   é¢„æµ‹: è´Ÿé¢ ğŸ˜
   ç½®ä¿¡åº¦: 89.23%
   è¯¦ç»†: æ­£é¢=10.77%, è´Ÿé¢=89.23%
```

---

### 2ï¸âƒ£ åœ¨ Python ä»£ç ä¸­ä½¿ç”¨

#### æ–¹æ³• A: ä½¿ç”¨ Pipelineï¼ˆæœ€ç®€å•ï¼‰

```python
from transformers import pipeline

# åŠ è½½æ¨¡å‹
classifier = pipeline("text-classification", model="./my_sentiment_model")

# é¢„æµ‹
result = classifier("è¿™ä¸ªäº§å“å¾ˆå¥½")[0]
print(result)
# {'label': 'LABEL_1', 'score': 0.9468}

# æ‰¹é‡é¢„æµ‹
texts = ["è´¨é‡å¾ˆå¥½", "å¤ªå·®äº†", "ä¸€èˆ¬èˆ¬"]
results = classifier(texts)
for text, result in zip(texts, results):
    print(f"{text} â†’ {result['label']} ({result['score']:.2%})")
```

#### æ–¹æ³• B: æ‰‹åŠ¨åŠ è½½ï¼ˆæ›´çµæ´»ï¼‰

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch
import torch.nn.functional as F

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model = AutoModelForSequenceClassification.from_pretrained("./my_sentiment_model")
tokenizer = AutoTokenizer.from_pretrained("./my_sentiment_model")

# é¢„æµ‹å‡½æ•°
def predict(text):
    inputs = tokenizer(text, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs)
    probs = F.softmax(outputs.logits, dim=-1)
    return {
        'positive': probs[0][1].item(),
        'negative': probs[0][0].item()
    }

# ä½¿ç”¨
result = predict("è¿™ä¸ªäº§å“å¾ˆå¥½")
print(f"æ­£é¢: {result['positive']:.2%}")
print(f"è´Ÿé¢: {result['negative']:.2%}")
```

---

### 3ï¸âƒ£ éƒ¨ç½²ä¸º Web API æœåŠ¡

#### æ­¥éª¤ 1: å®‰è£…ä¾èµ–

```bash
# å®‰è£… Flask
D:\aaaalokda\envs\myenv\python.exe -m pip install flask
```

#### æ­¥éª¤ 2: å¯åŠ¨æœåŠ¡

```bash
# è¿è¡Œ API æœåŠ¡
D:\aaaalokda\envs\myenv\python.exe æ¨¡å‹éƒ¨ç½²ç¤ºä¾‹.py
```

**è¾“å‡ºï¼š**
```
æ­£åœ¨åŠ è½½æ¨¡å‹...
âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼

============================================================
æƒ…æ„Ÿåˆ†æ API æœåŠ¡å·²å¯åŠ¨ï¼
è®¿é—®: http://localhost:5000
============================================================

 * Running on http://0.0.0.0:5000
```

#### æ­¥éª¤ 3: æµ‹è¯• API

**æ–¹æ³• A: ä½¿ç”¨æµ‹è¯•è„šæœ¬**

```bash
# åœ¨æ–°çš„å‘½ä»¤è¡Œçª—å£è¿è¡Œ
D:\aaaalokda\envs\myenv\python.exe æµ‹è¯•APIå®¢æˆ·ç«¯.py
```

**æ–¹æ³• B: ä½¿ç”¨ curl**

```bash
# å•ä¸ªæ–‡æœ¬é¢„æµ‹
curl -X POST http://localhost:5000/predict \
     -H "Content-Type: application/json" \
     -d "{\"text\": \"è¿™ä¸ªäº§å“å¾ˆå¥½\"}"

# æ‰¹é‡é¢„æµ‹
curl -X POST http://localhost:5000/batch_predict \
     -H "Content-Type: application/json" \
     -d "{\"texts\": [\"è´¨é‡å¾ˆå¥½\", \"å¤ªå·®äº†\"]}"
```

**æ–¹æ³• C: ä½¿ç”¨ Python requests**

```python
import requests

# å•ä¸ªé¢„æµ‹
response = requests.post(
    "http://localhost:5000/predict",
    json={"text": "è¿™ä¸ªäº§å“å¾ˆå¥½"}
)
print(response.json())

# æ‰¹é‡é¢„æµ‹
response = requests.post(
    "http://localhost:5000/batch_predict",
    json={"texts": ["è´¨é‡å¾ˆå¥½", "å¤ªå·®äº†", "ä¸€èˆ¬èˆ¬"]}
)
print(response.json())
```

---

## ğŸ“Š API æ¥å£æ–‡æ¡£

### 1. å¥åº·æ£€æŸ¥

**è¯·æ±‚ï¼š**
```
GET /health
```

**å“åº”ï¼š**
```json
{
  "status": "ok",
  "model_loaded": true
}
```

---

### 2. å•ä¸ªæ–‡æœ¬é¢„æµ‹

**è¯·æ±‚ï¼š**
```
POST /predict
Content-Type: application/json

{
  "text": "è¿™ä¸ªäº§å“å¾ˆå¥½",
  "use_pipeline": false
}
```

**å“åº”ï¼š**
```json
{
  "text": "è¿™ä¸ªäº§å“å¾ˆå¥½",
  "sentiment": "æ­£é¢",
  "confidence": 0.9468,
  "probabilities": {
    "negative": 0.0532,
    "positive": 0.9468
  }
}
```

---

### 3. æ‰¹é‡æ–‡æœ¬é¢„æµ‹

**è¯·æ±‚ï¼š**
```
POST /batch_predict
Content-Type: application/json

{
  "texts": [
    "è´¨é‡å¾ˆå¥½",
    "å¤ªå·®äº†",
    "ä¸€èˆ¬èˆ¬"
  ]
}
```

**å“åº”ï¼š**
```json
{
  "count": 3,
  "results": [
    {
      "text": "è´¨é‡å¾ˆå¥½",
      "label": "LABEL_1",
      "score": 0.9468,
      "sentiment": "æ­£é¢"
    },
    {
      "text": "å¤ªå·®äº†",
      "label": "LABEL_0",
      "score": 0.8923,
      "sentiment": "è´Ÿé¢"
    },
    {
      "text": "ä¸€èˆ¬èˆ¬",
      "label": "LABEL_0",
      "score": 0.6234,
      "sentiment": "è´Ÿé¢"
    }
  ]
}
```

---

## ğŸ”§ é«˜çº§ç”¨æ³•

### 1. GPU åŠ é€Ÿ

```python
import torch

# æ£€æŸ¥ GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# å°†æ¨¡å‹ç§»åˆ° GPU
model = AutoModelForSequenceClassification.from_pretrained("./my_sentiment_model")
model.to(device)

# é¢„æµ‹æ—¶ä¹Ÿè¦ç§»åˆ° GPU
inputs = tokenizer(text, return_tensors="pt")
inputs = {k: v.to(device) for k, v in inputs.items()}

with torch.no_grad():
    outputs = model(**inputs)
```

### 2. æ‰¹é‡å¤„ç†ï¼ˆæé«˜æ•ˆç‡ï¼‰

```python
# æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æœ¬
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3", ...]

# ä½¿ç”¨ pipeline è‡ªåŠ¨æ‰¹å¤„ç†
classifier = pipeline("text-classification", model="./my_sentiment_model", batch_size=32)
results = classifier(texts)

# æˆ–æ‰‹åŠ¨æ‰¹å¤„ç†
inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
with torch.no_grad():
    outputs = model(**inputs)
probs = F.softmax(outputs.logits, dim=-1)
```

### 3. ä¿å­˜ä¸º ONNX æ ¼å¼ï¼ˆæ›´å¿«çš„æ¨ç†ï¼‰

```python
from transformers.onnx import export

# å¯¼å‡ºä¸º ONNX
export(
    preprocessor=tokenizer,
    model=model,
    config=model.config,
    opset=14,
    output="model.onnx"
)

# ä½¿ç”¨ ONNX Runtime æ¨ç†
import onnxruntime as ort

session = ort.InferenceSession("model.onnx")
inputs = tokenizer("è¿™ä¸ªäº§å“å¾ˆå¥½", return_tensors="np")
outputs = session.run(None, dict(inputs))
```

### 4. é‡åŒ–æ¨¡å‹ï¼ˆå‡å°ä½“ç§¯ï¼‰

```python
from transformers import AutoModelForSequenceClassification
import torch

# åŠ è½½æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained("./my_sentiment_model")

# åŠ¨æ€é‡åŒ–ï¼ˆå‡å° 4 å€ï¼‰
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

# ä¿å­˜é‡åŒ–æ¨¡å‹
torch.save(quantized_model.state_dict(), "quantized_model.pth")
```

---

## ğŸ“¦ æ¨¡å‹åˆ†å‘

### 1. ä¸Šä¼ åˆ° Hugging Face Hub

```python
from huggingface_hub import HfApi

# ç™»å½•
from huggingface_hub import login
login()  # è¾“å…¥ä½ çš„ token

# ä¸Šä¼ æ¨¡å‹
api = HfApi()
api.upload_folder(
    folder_path="./my_sentiment_model",
    repo_id="your-username/sentiment-model",
    repo_type="model"
)
```

### 2. ä» Hub ä¸‹è½½ä½¿ç”¨

```python
from transformers import pipeline

# ç›´æ¥ä» Hub åŠ è½½
classifier = pipeline(
    "text-classification",
    model="your-username/sentiment-model"
)

result = classifier("è¿™ä¸ªäº§å“å¾ˆå¥½")
```

### 3. æ‰“åŒ…ä¸º Docker é•œåƒ

```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
RUN pip install transformers torch flask

# å¤åˆ¶æ¨¡å‹æ–‡ä»¶
COPY my_sentiment_model /app/my_sentiment_model
COPY æ¨¡å‹éƒ¨ç½²ç¤ºä¾‹.py /app/app.py

# æš´éœ²ç«¯å£
EXPOSE 5000

# å¯åŠ¨æœåŠ¡
CMD ["python", "app.py"]
```

```bash
# æ„å»ºé•œåƒ
docker build -t sentiment-api .

# è¿è¡Œå®¹å™¨
docker run -p 5000:5000 sentiment-api
```

---

## ğŸ¯ å®é™…åº”ç”¨åœºæ™¯

### 1. ç”µå•†è¯„è®ºåˆ†æ

```python
# åˆ†æå•†å“è¯„è®º
reviews = [
    "è´¨é‡å¾ˆå¥½ï¼Œç‰©æµå¿«",
    "åŒ…è£…ç ´æŸï¼Œå·®è¯„",
    "æ€§ä»·æ¯”ä¸é”™"
]

results = classifier(reviews)
positive_count = sum(1 for r in results if r['label'] == 'LABEL_1')
print(f"å¥½è¯„ç‡: {positive_count/len(reviews):.2%}")
```

### 2. ç¤¾äº¤åª’ä½“ç›‘æ§

```python
# ç›‘æ§å“ç‰ŒæåŠ
tweets = get_tweets_about_brand()  # è·å–æ¨æ–‡
sentiments = classifier(tweets)

# ç»Ÿè®¡æƒ…æ„Ÿåˆ†å¸ƒ
positive = sum(1 for s in sentiments if s['label'] == 'LABEL_1')
negative = len(sentiments) - positive
print(f"æ­£é¢: {positive}, è´Ÿé¢: {negative}")
```

### 3. å®¢æœç³»ç»Ÿ

```python
# è‡ªåŠ¨åˆ†ç±»å®¢æˆ·åé¦ˆ
feedback = "ä½ ä»¬çš„æœåŠ¡å¤ªå·®äº†"
result = classifier(feedback)[0]

if result['label'] == 'LABEL_0' and result['score'] > 0.8:
    # è´Ÿé¢åé¦ˆï¼Œä¼˜å…ˆå¤„ç†
    alert_customer_service(feedback)
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. æ¨¡å‹é™åˆ¶

- **æœ€å¤§é•¿åº¦**: 512 ä¸ª tokenï¼ˆçº¦ 256 ä¸ªä¸­æ–‡å­—ç¬¦ï¼‰
- **è¯­è¨€**: ä»…æ”¯æŒä¸­æ–‡
- **ä»»åŠ¡**: ä»…æ”¯æŒäºŒåˆ†ç±»ï¼ˆæ­£é¢/è´Ÿé¢ï¼‰

### 2. æ€§èƒ½ä¼˜åŒ–

- ä½¿ç”¨ GPU å¯ä»¥æé€Ÿ 20-50 å€
- æ‰¹é‡å¤„ç†æ¯”å•ä¸ªå¤„ç†å¿« 5-10 å€
- ONNX æ ¼å¼æ¯” PyTorch å¿« 2-3 å€

### 3. å‡†ç¡®ç‡

- å½“å‰æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šå‡†ç¡®ç‡çº¦ 75%
- ä½¿ç”¨æ›´å¤šæ•°æ®è®­ç»ƒå¯ä»¥æé«˜å‡†ç¡®ç‡
- å¯¹äºç‰¹å®šé¢†åŸŸï¼Œå»ºè®®ä½¿ç”¨é¢†åŸŸæ•°æ®å¾®è°ƒ

---

## ğŸ” æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: æ¨¡å‹åŠ è½½å¤±è´¥

```
FileNotFoundError: [Errno 2] No such file or directory: './my_sentiment_model/config.json'
```

**è§£å†³æ–¹æ¡ˆï¼š**
- ç¡®ä¿ `my_sentiment_model` æ–‡ä»¶å¤¹å­˜åœ¨
- æ£€æŸ¥æ–‡ä»¶å¤¹ä¸­æ˜¯å¦æœ‰ `config.json` å’Œ `model.safetensors`

---

### é—®é¢˜ 2: CUDA å†…å­˜ä¸è¶³

```
RuntimeError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# å‡å°æ‰¹é‡å¤§å°
classifier = pipeline("text-classification", model="./my_sentiment_model", batch_size=8)

# æˆ–ä½¿ç”¨ CPU
classifier = pipeline("text-classification", model="./my_sentiment_model", device=-1)
```

---

### é—®é¢˜ 3: API è¿æ¥å¤±è´¥

```
requests.exceptions.ConnectionError: Failed to establish a new connection
```

**è§£å†³æ–¹æ¡ˆï¼š**
- ç¡®ä¿ API æœåŠ¡å·²å¯åŠ¨
- æ£€æŸ¥ç«¯å£ 5000 æ˜¯å¦è¢«å ç”¨
- æ£€æŸ¥é˜²ç«å¢™è®¾ç½®

---

## ğŸ“š ç›¸å…³èµ„æº

- [Transformers å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/transformers)
- [BERT è®ºæ–‡](https://arxiv.org/abs/1810.04805)
- [Hugging Face Hub](https://huggingface.co/models)
- [è®­ç»ƒæ¨¡å‹è¯¦è§£.md](./è®­ç»ƒæ¨¡å‹è¯¦è§£.md) - æ·±å…¥ç†è§£æ¨¡å‹ç»“æ„

---

## ğŸ“ æ€»ç»“

### æ¨¡å‹ä½¿ç”¨ä¸‰æ­¥èµ°

1. **åŠ è½½æ¨¡å‹**
   ```python
   from transformers import pipeline
   classifier = pipeline("text-classification", model="./my_sentiment_model")
   ```

2. **è¿›è¡Œé¢„æµ‹**
   ```python
   result = classifier("è¿™ä¸ªäº§å“å¾ˆå¥½")
   ```

3. **ä½¿ç”¨ç»“æœ**
   ```python
   print(result[0]['label'])  # LABEL_1 (æ­£é¢)
   print(result[0]['score'])  # 0.9468 (94.68%)
   ```

### ä¸‹ä¸€æ­¥

- âœ… ä½¿ç”¨æ›´å¤šæ•°æ®é‡æ–°è®­ç»ƒï¼Œæé«˜å‡†ç¡®ç‡
- âœ… å°è¯•å…¶ä»–é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ RoBERTaã€ELECTRAï¼‰
- âœ… æ‰©å±•åˆ°å¤šåˆ†ç±»ï¼ˆå¦‚ï¼šæ­£é¢ã€ä¸­æ€§ã€è´Ÿé¢ï¼‰
- âœ… éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼ˆDockerã€äº‘æœåŠ¡ï¼‰

**ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼** ğŸ‰
