#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†æé¢„æµ‹
"""

from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch
import torch.nn.functional as F

print("=" * 60)
print("åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
print("=" * 60)

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
import os
script_dir = os.path.dirname(os.path.abspath(__file__))
model_path = os.path.join(script_dir, "my_sentiment_model")

print(f"æ¨¡å‹è·¯å¾„: {model_path}")

# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
if not os.path.exists(model_path):
    print(f"\nâŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶å¤¹")
    print(f"   æœŸæœ›ä½ç½®: {model_path}")
    print(f"   å½“å‰ç›®å½•: {os.getcwd()}")
    print(f"\nğŸ’¡ è¯·å…ˆè¿è¡Œ Trainer_å®æˆ˜ç¤ºä¾‹.py è®­ç»ƒæ¨¡å‹")
    exit(1)

# 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model = AutoModelForSequenceClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼")
print(f"âœ“ æ¨¡å‹ç±»å‹: {model.config.model_type}")
print(f"âœ“ å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
print()

# 2. å‡†å¤‡æµ‹è¯•æ–‡æœ¬
test_texts = [
    "è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œéå¸¸æ»¡æ„ï¼",
    "å¤ªå·®äº†ï¼Œå®Œå…¨ä¸å€¼è¿™ä¸ªä»·æ ¼",
    "è¿˜å¯ä»¥ï¼Œä¸€èˆ¬èˆ¬",
    "ç‰©æµå¾ˆå¿«ï¼ŒåŒ…è£…å®Œå¥½ï¼Œæ¨èè´­ä¹°",
    "å®¢æœæ€åº¦æ¶åŠ£ï¼Œå†ä¹Ÿä¸ä¹°äº†",
    "æ€§ä»·æ¯”ä¸é”™ï¼Œå€¼å¾—å…¥æ‰‹",
]

print("=" * 60)
print("å¼€å§‹é¢„æµ‹...")
print("=" * 60)

# 3. å¯¹æ¯ä¸ªæ–‡æœ¬è¿›è¡Œé¢„æµ‹
for i, text in enumerate(test_texts, 1):
    # åˆ†è¯
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    
    # é¢„æµ‹ï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜ï¼‰
    with torch.no_grad():
        outputs = model(**inputs)
    
    # è®¡ç®—æ¦‚ç‡
    probs = F.softmax(outputs.logits, dim=-1)
    negative_prob = probs[0][0].item()
    positive_prob = probs[0][1].item()
    
    # åˆ¤æ–­æƒ…æ„Ÿ
    sentiment = "æ­£é¢ ğŸ˜Š" if positive_prob > negative_prob else "è´Ÿé¢ ğŸ˜"
    confidence = max(positive_prob, negative_prob)
    
    # è¾“å‡ºç»“æœ
    print(f"\n{i}. æ–‡æœ¬: {text}")
    print(f"   é¢„æµ‹: {sentiment}")
    print(f"   ç½®ä¿¡åº¦: {confidence:.2%}")
    print(f"   è¯¦ç»†: æ­£é¢={positive_prob:.2%}, è´Ÿé¢={negative_prob:.2%}")

print("\n" + "=" * 60)
print("é¢„æµ‹å®Œæˆï¼")
print("=" * 60)

# 4. äº¤äº’å¼é¢„æµ‹
print("\n" + "=" * 60)
print("äº¤äº’å¼é¢„æµ‹ï¼ˆè¾“å…¥ 'q' é€€å‡ºï¼‰")
print("=" * 60)

while True:
    user_input = input("\nè¯·è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬: ").strip()
    
    if user_input.lower() == 'q':
        print("å†è§ï¼")
        break
    
    if not user_input:
        print("âš ï¸  è¯·è¾“å…¥æœ‰æ•ˆæ–‡æœ¬")
        continue
    
    # é¢„æµ‹
    inputs = tokenizer(user_input, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    
    probs = F.softmax(outputs.logits, dim=-1)
    negative_prob = probs[0][0].item()
    positive_prob = probs[0][1].item()
    
    sentiment = "æ­£é¢ ğŸ˜Š" if positive_prob > negative_prob else "è´Ÿé¢ ğŸ˜"
    confidence = max(positive_prob, negative_prob)
    
    print(f"\n   é¢„æµ‹ç»“æœ: {sentiment}")
    print(f"   ç½®ä¿¡åº¦: {confidence:.2%}")
    print(f"   è¯¦ç»†æ¦‚ç‡: æ­£é¢={positive_prob:.2%}, è´Ÿé¢={negative_prob:.2%}")
