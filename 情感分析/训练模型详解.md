# è®­ç»ƒåçš„æ¨¡å‹è¯¦è§£

## ğŸ“ æ¨¡å‹æ–‡ä»¶ç»“æ„

è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹è¢«ä¿å­˜åœ¨ `my_sentiment_model/` æ–‡ä»¶å¤¹ä¸­ï¼ŒåŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š

```
my_sentiment_model/
â”œâ”€â”€ config.json              # æ¨¡å‹é…ç½®æ–‡ä»¶
â”œâ”€â”€ model.safetensors        # æ¨¡å‹æƒé‡æ–‡ä»¶ï¼ˆæ–°æ ¼å¼ï¼‰
â”œâ”€â”€ tokenizer_config.json    # åˆ†è¯å™¨é…ç½®
â”œâ”€â”€ tokenizer.json           # åˆ†è¯å™¨è¯æ±‡è¡¨å’Œè§„åˆ™
â””â”€â”€ training_args.bin        # è®­ç»ƒå‚æ•°ï¼ˆå¯é€‰ï¼‰
```

---

## ğŸ“„ å„æ–‡ä»¶è¯¦è§£

### 1. `config.json` - æ¨¡å‹é…ç½®æ–‡ä»¶

è¿™æ˜¯æ¨¡å‹çš„"è®¾è®¡å›¾çº¸"ï¼Œå®šä¹‰äº†æ¨¡å‹çš„æ¶æ„å’Œå‚æ•°ã€‚

```json
{
  "architectures": ["BertForSequenceClassification"],  // æ¨¡å‹ç±»å‹
  "model_type": "bert",                                // åŸºç¡€æ¨¡å‹
  "hidden_size": 768,                                  // éšè—å±‚å¤§å°
  "num_hidden_layers": 12,                             // Transformer å±‚æ•°
  "num_attention_heads": 12,                           // æ³¨æ„åŠ›å¤´æ•°
  "intermediate_size": 3072,                           // å‰é¦ˆç½‘ç»œå¤§å°
  "max_position_embeddings": 512,                      // æœ€å¤§åºåˆ—é•¿åº¦
  "vocab_size": 21128,                                 // è¯æ±‡è¡¨å¤§å°
  "num_labels": 2,                                     // åˆ†ç±»ç±»åˆ«æ•°
  "problem_type": "single_label_classification"        // ä»»åŠ¡ç±»å‹
}
```

**å…³é”®å‚æ•°è¯´æ˜ï¼š**

| å‚æ•° | å€¼ | å«ä¹‰ |
|------|-----|------|
| `hidden_size` | 768 | æ¯ä¸ªè¯çš„å‘é‡ç»´åº¦ |
| `num_hidden_layers` | 12 | BERT æœ‰ 12 å±‚ Transformer |
| `num_attention_heads` | 12 | æ¯å±‚æœ‰ 12 ä¸ªæ³¨æ„åŠ›å¤´ |
| `vocab_size` | 21128 | ä¸­æ–‡ BERT çš„è¯æ±‡é‡ |
| `max_position_embeddings` | 512 | æœ€å¤šå¤„ç† 512 ä¸ªè¯ |

**æ¨¡å‹å‚æ•°é‡è®¡ç®—ï¼š**
- æ€»å‚æ•°ï¼šçº¦ 102,269,186 ä¸ªï¼ˆ102Mï¼‰
- åŒ…æ‹¬ï¼šè¯åµŒå…¥ã€ä½ç½®ç¼–ç ã€12 å±‚ Transformerã€åˆ†ç±»å¤´

---

### 2. `model.safetensors` - æ¨¡å‹æƒé‡æ–‡ä»¶

è¿™æ˜¯æ¨¡å‹çš„"å¤§è„‘"ï¼ŒåŒ…å«æ‰€æœ‰è®­ç»ƒå¥½çš„å‚æ•°ï¼ˆæƒé‡ï¼‰ã€‚

**æ–‡ä»¶æ ¼å¼ï¼š**
- **SafeTensors**: æ–°çš„å®‰å…¨æ ¼å¼ï¼ˆæ¨èï¼‰
  - æ›´å¿«çš„åŠ è½½é€Ÿåº¦
  - æ›´å®‰å…¨ï¼ˆé˜²æ­¢æ¶æ„ä»£ç ï¼‰
  - æ›´å°çš„æ–‡ä»¶å¤§å°
  
- **æ—§æ ¼å¼**: `pytorch_model.bin`ï¼ˆPyTorch æ ¼å¼ï¼‰

**æ–‡ä»¶å¤§å°ï¼š**
- çº¦ 400-500 MBï¼ˆå–å†³äºæ¨¡å‹å¤§å°ï¼‰
- BERT-base: ~400MB
- BERT-large: ~1.3GB

**åŒ…å«çš„æƒé‡ï¼š**
```
bert.embeddings.word_embeddings.weight        # è¯åµŒå…¥
bert.embeddings.position_embeddings.weight    # ä½ç½®ç¼–ç 
bert.encoder.layer.0.attention.self.query     # æ³¨æ„åŠ›å±‚
bert.encoder.layer.0.attention.self.key
bert.encoder.layer.0.attention.self.value
...ï¼ˆ12 å±‚ï¼‰
classifier.weight                              # åˆ†ç±»å¤´
classifier.bias
```

---

### 3. `tokenizer_config.json` - åˆ†è¯å™¨é…ç½®

å®šä¹‰å¦‚ä½•å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—ã€‚

```json
{
  "tokenizer_class": "BertTokenizer",      // åˆ†è¯å™¨ç±»å‹
  "do_lower_case": false,                  // ä¸è½¬å°å†™ï¼ˆä¸­æ–‡ï¼‰
  "model_max_length": 512,                 // æœ€å¤§é•¿åº¦
  "cls_token": "[CLS]",                    // å¥å­å¼€å§‹æ ‡è®°
  "sep_token": "[SEP]",                    // å¥å­ç»“æŸæ ‡è®°
  "pad_token": "[PAD]",                    // å¡«å……æ ‡è®°
  "mask_token": "[MASK]",                  // æ©ç æ ‡è®°
  "unk_token": "[UNK]"                     // æœªçŸ¥è¯æ ‡è®°
}
```

**ç‰¹æ®Šæ ‡è®°è¯´æ˜ï¼š**
- `[CLS]`: å¥å­å¼€å¤´ï¼Œç”¨äºåˆ†ç±»ä»»åŠ¡
- `[SEP]`: å¥å­åˆ†éš”ç¬¦
- `[PAD]`: å¡«å……åˆ°ç›¸åŒé•¿åº¦
- `[MASK]`: æ©ç è¯­è¨€æ¨¡å‹è®­ç»ƒç”¨
- `[UNK]`: æœªçŸ¥è¯ï¼ˆä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼‰

---

### 4. `tokenizer.json` - è¯æ±‡è¡¨å’Œåˆ†è¯è§„åˆ™

åŒ…å«å®Œæ•´çš„è¯æ±‡è¡¨å’Œåˆ†è¯ç®—æ³•ã€‚

**å†…å®¹ï¼š**
- 21,128 ä¸ªä¸­æ–‡è¯æ±‡
- WordPiece åˆ†è¯ç®—æ³•
- è¯åˆ° ID çš„æ˜ å°„

**ç¤ºä¾‹ï¼š**
```json
{
  "vocab": {
    "[PAD]": 0,
    "[UNK]": 100,
    "[CLS]": 101,
    "[SEP]": 102,
    "çš„": 103,
    "æ˜¯": 104,
    ...
  }
}
```

---

## ğŸ§  æ¨¡å‹çš„å†…éƒ¨ç»“æ„

### å®Œæ•´çš„æ¨¡å‹æ¶æ„

```
è¾“å…¥æ–‡æœ¬: "è¿™ä¸ªäº§å“å¾ˆå¥½"
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Tokenizer (åˆ†è¯å™¨)               â”‚
â”‚     "è¿™ä¸ªäº§å“å¾ˆå¥½" â†’ [101, 6821, ...]â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Embedding Layer (åµŒå…¥å±‚)        â”‚
â”‚     Token IDs â†’ 768ç»´å‘é‡           â”‚
â”‚     + Position Embeddings           â”‚
â”‚     + Token Type Embeddings         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. BERT Encoder (12å±‚)             â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚     â”‚ Layer 1: Self-Attention â”‚    â”‚
â”‚     â”‚          + Feed Forward â”‚    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚     â”‚ Layer 2: Self-Attention â”‚    â”‚
â”‚     â”‚          + Feed Forward â”‚    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚     ...                             â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚     â”‚ Layer 12: Self-Attentionâ”‚    â”‚
â”‚     â”‚           + Feed Forwardâ”‚    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Pooler (æ± åŒ–å±‚)                  â”‚
â”‚     å– [CLS] ä½ç½®çš„å‘é‡              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Classifier (åˆ†ç±»å¤´)              â”‚
â”‚     768ç»´ â†’ 2ç»´ (æ­£é¢/è´Ÿé¢)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
è¾“å‡º: [0.05, 0.95] â†’ æ­£é¢ (95%)
```

---

## ğŸ”¢ æ¨¡å‹çš„æ•°å­¦è¡¨ç¤º

### 1. è¾“å…¥å¤„ç†
```
è¾“å…¥æ–‡æœ¬ â†’ Token IDs
"è¿™ä¸ªäº§å“å¾ˆå¥½" â†’ [101, 6821, 702, 772, 1501, 2523, 1962, 102]
```

### 2. åµŒå…¥å±‚
```python
# è¯åµŒå…¥
word_embeddings = Embedding(vocab_size=21128, hidden_size=768)

# ä½ç½®ç¼–ç 
position_embeddings = Embedding(max_position=512, hidden_size=768)

# ç»„åˆ
embeddings = word_embeddings + position_embeddings
# å½¢çŠ¶: [batch_size, seq_length, 768]
```

### 3. Transformer å±‚
```python
# æ¯ä¸€å±‚åŒ…å«ï¼š
class TransformerLayer:
    def forward(x):
        # 1. å¤šå¤´è‡ªæ³¨æ„åŠ›
        attention_output = MultiHeadAttention(x)
        x = LayerNorm(x + attention_output)
        
        # 2. å‰é¦ˆç½‘ç»œ
        ffn_output = FeedForward(x)
        x = LayerNorm(x + ffn_output)
        
        return x
```

### 4. åˆ†ç±»å¤´
```python
# å– [CLS] ä½ç½®çš„è¾“å‡º
cls_output = encoder_output[:, 0, :]  # [batch_size, 768]

# çº¿æ€§åˆ†ç±»
logits = Linear(768, 2)(cls_output)   # [batch_size, 2]

# Softmax å¾—åˆ°æ¦‚ç‡
probs = Softmax(logits)               # [0.05, 0.95]
```

---

## ğŸ’¾ æ¨¡å‹æ–‡ä»¶å¤§å°åˆ†æ

| æ–‡ä»¶ | å¤§å° | å æ¯” | è¯´æ˜ |
|------|------|------|------|
| `model.safetensors` | ~412 MB | 99% | æ¨¡å‹æƒé‡ |
| `tokenizer.json` | ~2 MB | <1% | è¯æ±‡è¡¨ |
| `config.json` | ~1 KB | <0.1% | é…ç½® |
| `tokenizer_config.json` | ~1 KB | <0.1% | åˆ†è¯å™¨é…ç½® |
| **æ€»è®¡** | **~414 MB** | 100% | |

---

## ğŸ”„ æ¨¡å‹çš„ä½¿ç”¨æµç¨‹

### åŠ è½½æ¨¡å‹
```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model = AutoModelForSequenceClassification.from_pretrained("./my_sentiment_model")
tokenizer = AutoTokenizer.from_pretrained("./my_sentiment_model")

# æ¨¡å‹ä¼šè‡ªåŠ¨è¯»å–ï¼š
# 1. config.json â†’ æ„å»ºæ¨¡å‹æ¶æ„
# 2. model.safetensors â†’ åŠ è½½æƒé‡
# 3. tokenizer.json â†’ åŠ è½½åˆ†è¯å™¨
```

### æ¨ç†è¿‡ç¨‹
```python
# 1. æ–‡æœ¬ â†’ Token IDs
text = "è¿™ä¸ªäº§å“å¾ˆå¥½"
inputs = tokenizer(text, return_tensors="pt")
# inputs = {
#     'input_ids': tensor([[101, 6821, 702, 772, 1501, 2523, 1962, 102]]),
#     'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])
# }

# 2. å‰å‘ä¼ æ’­
outputs = model(**inputs)
# outputs.logits = tensor([[-2.94, 2.94]])  # åŸå§‹åˆ†æ•°

# 3. è®¡ç®—æ¦‚ç‡
import torch.nn.functional as F
probs = F.softmax(outputs.logits, dim=-1)
# probs = tensor([[0.05, 0.95]])  # [è´Ÿé¢, æ­£é¢]

# 4. é¢„æµ‹ç»“æœ
prediction = torch.argmax(probs, dim=-1)
# prediction = 1 (æ­£é¢)
```

---

## ğŸ“Š æ¨¡å‹å‚æ•°è¯¦ç»†åˆ†è§£

### BERT-base-chinese å‚æ•°åˆ†å¸ƒ

```
æ€»å‚æ•°: 102,269,186 (çº¦ 102M)

1. è¯åµŒå…¥å±‚: 16,262,144
   - word_embeddings: 21,128 Ã— 768 = 16,226,304
   - position_embeddings: 512 Ã— 768 = 393,216
   - token_type_embeddings: 2 Ã— 768 = 1,536
   - LayerNorm: 768 Ã— 2 = 1,536

2. 12 ä¸ª Transformer å±‚: 85,054,464
   æ¯å±‚åŒ…å«:
   - Self-Attention: 2,362,368
     * Query: 768 Ã— 768 = 589,824
     * Key: 768 Ã— 768 = 589,824
     * Value: 768 Ã— 768 = 589,824
     * Output: 768 Ã— 768 = 589,824
   - Feed-Forward: 4,722,432
     * Intermediate: 768 Ã— 3072 = 2,359,296
     * Output: 3072 Ã— 768 = 2,359,296
   - LayerNorm: 3,072
   
   æ¯å±‚æ€»è®¡: 7,087,872
   12 å±‚: 7,087,872 Ã— 12 = 85,054,464

3. Pooler å±‚: 590,592
   - Dense: 768 Ã— 768 = 589,824
   - Bias: 768

4. åˆ†ç±»å¤´: 1,538
   - Weight: 768 Ã— 2 = 1,536
   - Bias: 2
```

---

## ğŸ¯ æ¨¡å‹çš„å®é™…åº”ç”¨

### 1. ç›´æ¥ä½¿ç”¨ï¼ˆPipelineï¼‰
```python
from transformers import pipeline

classifier = pipeline("text-classification", model="./my_sentiment_model")
result = classifier("è¿™ä¸ªäº§å“å¾ˆæ£’ï¼")
# [{'label': 'LABEL_1', 'score': 0.9468}]
```

### 2. æ‰¹é‡é¢„æµ‹
```python
texts = [
    "è´¨é‡å¾ˆå¥½ï¼Œæ¨èè´­ä¹°",
    "å¤ªå·®äº†ï¼Œä¸å€¼è¿™ä¸ªä»·",
    "è¿˜å¯ä»¥ï¼Œä¸€èˆ¬èˆ¬"
]

results = classifier(texts)
for text, result in zip(texts, results):
    print(f"{text} â†’ {result['label']} ({result['score']:.2%})")
```

### 3. é›†æˆåˆ°åº”ç”¨
```python
class SentimentAnalyzer:
    def __init__(self, model_path):
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    def predict(self, text):
        inputs = self.tokenizer(text, return_tensors="pt")
        outputs = self.model(**inputs)
        probs = F.softmax(outputs.logits, dim=-1)
        return {
            'positive': probs[0][1].item(),
            'negative': probs[0][0].item()
        }

# ä½¿ç”¨
analyzer = SentimentAnalyzer("./my_sentiment_model")
result = analyzer.predict("è¿™ä¸ªäº§å“å¾ˆå¥½")
print(f"æ­£é¢: {result['positive']:.2%}, è´Ÿé¢: {result['negative']:.2%}")
```

---

## ğŸ” æ¨¡å‹çš„å¯è§†åŒ–

### æ¨¡å‹ç»“æ„å›¾
```
BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,))
      (dropout): Dropout(p=0.1)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(768, 768)
              (key): Linear(768, 768)
              (value): Linear(768, 768)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(768, 768)
              (LayerNorm): LayerNorm((768,))
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(768, 3072)
          )
          (output): BertOutput(
            (dense): Linear(3072, 768)
            (LayerNorm): LayerNorm((768,))
            (dropout): Dropout(p=0.1)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(768, 768)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1)
  (classifier): Linear(768, 2)
)
```

---

## ğŸ“¦ æ¨¡å‹çš„åˆ†å‘å’Œéƒ¨ç½²

### 1. æœ¬åœ°ä½¿ç”¨
```python
model = AutoModelForSequenceClassification.from_pretrained("./my_sentiment_model")
```

### 2. ä¸Šä¼ åˆ° Hugging Face Hub
```python
from huggingface_hub import HfApi

api = HfApi()
api.upload_folder(
    folder_path="./my_sentiment_model",
    repo_id="your-username/sentiment-model",
    repo_type="model"
)
```

### 3. ä» Hub ä¸‹è½½ä½¿ç”¨
```python
model = AutoModelForSequenceClassification.from_pretrained(
    "your-username/sentiment-model"
)
```

### 4. è½¬æ¢ä¸º ONNXï¼ˆåŠ é€Ÿæ¨ç†ï¼‰
```python
from transformers.onnx import export

export(
    preprocessor=tokenizer,
    model=model,
    config=model.config,
    opset=14,
    output="model.onnx"
)
```

---

## ğŸ“ æ€»ç»“

### æ¨¡å‹çš„æœ¬è´¨
è®­ç»ƒåçš„æ¨¡å‹å®é™…ä¸Šæ˜¯ï¼š
1. **æ¶æ„å®šä¹‰**ï¼ˆconfig.jsonï¼‰- æ¨¡å‹çš„"éª¨æ¶"
2. **æƒé‡å‚æ•°**ï¼ˆmodel.safetensorsï¼‰- æ¨¡å‹çš„"çŸ¥è¯†"
3. **åˆ†è¯å™¨**ï¼ˆtokenizer.jsonï¼‰- æ–‡æœ¬å¤„ç†å·¥å…·

### æ¨¡å‹çš„å½¢å¼
- **ç‰©ç†å½¢å¼**: ä¸€ç»„æ–‡ä»¶ï¼ˆçº¦ 414 MBï¼‰
- **é€»è¾‘å½¢å¼**: ä¸€ä¸ªç¥ç»ç½‘ç»œï¼ˆ102M å‚æ•°ï¼‰
- **åŠŸèƒ½å½¢å¼**: ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»å™¨ï¼ˆè¾“å…¥æ–‡æœ¬ â†’ è¾“å‡ºç±»åˆ«ï¼‰

### æ¨¡å‹çš„ä»·å€¼
- åŒ…å«äº†ä»æ•°æ®ä¸­å­¦åˆ°çš„**çŸ¥è¯†**
- å¯ä»¥å¯¹æ–°æ–‡æœ¬è¿›è¡Œ**é¢„æµ‹**
- å¯ä»¥**è¿ç§»**åˆ°ç±»ä¼¼ä»»åŠ¡
- å¯ä»¥**åˆ†äº«**ç»™å…¶ä»–äººä½¿ç”¨

**ç®€å•æ¥è¯´ï¼šè®­ç»ƒåçš„æ¨¡å‹å°±æ˜¯ä¸€ä¸ª"æ™ºèƒ½å‡½æ•°"ï¼Œè¾“å…¥æ–‡æœ¬ï¼Œè¾“å‡ºåˆ†ç±»ç»“æœï¼** ğŸ¯
