"""
Trainer API å®æˆ˜ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Trainer è®­ç»ƒä¸€ä¸ªç®€å•çš„æ–‡æœ¬åˆ†ç±»æ¨¡å‹
"""

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding
)
from datasets import Dataset
import numpy as np

print("=" * 70)
print("ğŸš€ Trainer API å®æˆ˜ç¤ºä¾‹ï¼šæƒ…æ„Ÿåˆ†ç±»")
print("=" * 70)

# ============================================================================
# æ­¥éª¤ 1: å‡†å¤‡æ•°æ®
# ============================================================================
print("\nğŸ“Š æ­¥éª¤ 1: å‡†å¤‡è®­ç»ƒæ•°æ®")
print("-" * 70)

# åˆ›å»ºä¸€ä¸ªç®€å•çš„æƒ…æ„Ÿåˆ†ç±»æ•°æ®é›†
train_texts = [
    "è¿™ä¸ªäº§å“å¤ªæ£’äº†ï¼Œæˆ‘éå¸¸å–œæ¬¢ï¼",
    "è´¨é‡å¾ˆå·®ï¼Œå®Œå…¨ä¸å€¼è¿™ä¸ªä»·æ ¼ã€‚",
    "è¿˜å¯ä»¥ï¼Œç¬¦åˆé¢„æœŸã€‚",
    "éå¸¸æ»¡æ„ï¼Œä¼šæ¨èç»™æœ‹å‹ã€‚",
    "å¤ªå¤±æœ›äº†ï¼Œæµªè´¹é’±ã€‚",
    "æ€§ä»·æ¯”å¾ˆé«˜ï¼Œå€¼å¾—è´­ä¹°ã€‚",
    "ä¸æ¨èï¼Œæœ‰å¾ˆå¤šé—®é¢˜ã€‚",
    "è¶…å‡ºé¢„æœŸï¼Œéå¸¸å¥½ç”¨ï¼",
    "ä¸€èˆ¬èˆ¬ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„ã€‚",
    "å®Œç¾ï¼æ­£æ˜¯æˆ‘æƒ³è¦çš„ã€‚",
] * 10  # é‡å¤ 10 æ¬¡ä»¥å¢åŠ æ•°æ®é‡

train_labels = [1, 0, 1, 1, 0, 1, 0, 1, 1, 1] * 10  # 1=æ­£é¢, 0=è´Ÿé¢

eval_texts = [
    "å¾ˆå¥½ç”¨ï¼Œæ¨èè´­ä¹°ã€‚",
    "ä¸å¤ªæ»¡æ„ï¼Œæœ‰å¾…æ”¹è¿›ã€‚",
    "ç‰©è¶…æ‰€å€¼ï¼",
    "è´¨é‡ä¸€èˆ¬ã€‚",
]
eval_labels = [1, 0, 1, 1]

print(f"âœ… è®­ç»ƒæ ·æœ¬æ•°: {len(train_texts)}")
print(f"âœ… è¯„ä¼°æ ·æœ¬æ•°: {len(eval_texts)}")
print(f"âœ… ç¤ºä¾‹æ–‡æœ¬: {train_texts[0]}")
print(f"âœ… ç¤ºä¾‹æ ‡ç­¾: {train_labels[0]} (1=æ­£é¢, 0=è´Ÿé¢)")

# ============================================================================
# æ­¥éª¤ 2: åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
# ============================================================================
print("\nğŸ¤– æ­¥éª¤ 2: åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨")
print("-" * 70)

# ä½¿ç”¨ä¸­æ–‡ BERT æ¨¡å‹ï¼ˆå¦‚æœç½‘ç»œä¸å¥½ï¼Œå¯ä»¥æ¢æˆ bert-base-uncasedï¼‰
model_name = "bert-base-chinese"  # æˆ– "bert-base-uncased"

print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
print("â³ é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")

try:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=2  # äºŒåˆ†ç±»ï¼šæ­£é¢/è´Ÿé¢
    )
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    print(f"   å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    print("ğŸ’¡ æç¤ºï¼šå¦‚æœä¸‹è½½å¤±è´¥ï¼Œå¯ä»¥ï¼š")
    print("   1. ä½¿ç”¨é•œåƒç«™ï¼šexport HF_ENDPOINT=https://hf-mirror.com")
    print("   2. æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼šdistilbert-base-uncased")
    exit(1)

# ============================================================================
# æ­¥éª¤ 3: æ•°æ®é¢„å¤„ç†
# ============================================================================
print("\nğŸ”§ æ­¥éª¤ 3: æ•°æ®é¢„å¤„ç†ï¼ˆåˆ†è¯ï¼‰")
print("-" * 70)

def tokenize_function(examples):
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼"""
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

# åˆ›å»º Dataset å¯¹è±¡
train_dataset = Dataset.from_dict({
    "text": train_texts,
    "label": train_labels
})

eval_dataset = Dataset.from_dict({
    "text": eval_texts,
    "label": eval_labels
})

# åº”ç”¨åˆ†è¯
print("æ­£åœ¨å¯¹æ•°æ®è¿›è¡Œåˆ†è¯...")
train_dataset = train_dataset.map(tokenize_function, batched=True)
eval_dataset = eval_dataset.map(tokenize_function, batched=True)

print(f"âœ… åˆ†è¯å®Œæˆï¼")
print(f"   è®­ç»ƒé›†ç‰¹å¾: {train_dataset.column_names}")
print(f"   ç¤ºä¾‹è¾“å…¥: {train_dataset[0]['input_ids'][:10]}...")

# ============================================================================
# æ­¥éª¤ 4: å®šä¹‰è¯„ä¼°æŒ‡æ ‡
# ============================================================================
print("\nğŸ“ˆ æ­¥éª¤ 4: å®šä¹‰è¯„ä¼°æŒ‡æ ‡")
print("-" * 70)

def compute_metrics(eval_pred):
    """è®¡ç®—å‡†ç¡®ç‡"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    accuracy = (predictions == labels).mean()
    return {"accuracy": accuracy}

print("âœ… è¯„ä¼°æŒ‡æ ‡: å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰")

# ============================================================================
# æ­¥éª¤ 5: é…ç½®è®­ç»ƒå‚æ•°
# ============================================================================
print("\nâš™ï¸  æ­¥éª¤ 5: é…ç½®è®­ç»ƒå‚æ•°")
print("-" * 70)

# æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨
device = "cuda" if torch.cuda.is_available() else "cpu"
use_fp16 = torch.cuda.is_available()  # åªåœ¨ GPU ä¸Šä½¿ç”¨ FP16

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
import os
script_dir = os.path.dirname(os.path.abspath(__file__))
output_dir = os.path.join(script_dir, "my_sentiment_model")
logging_dir = os.path.join(script_dir, "logs")

training_args = TrainingArguments(
    # åŸºç¡€è®¾ç½®
    output_dir=output_dir,                    # è¾“å‡ºç›®å½•ï¼ˆç»å¯¹è·¯å¾„ï¼‰
    
    # è®­ç»ƒè®¾ç½®
    num_train_epochs=3,                       # è®­ç»ƒ 3 è½®
    per_device_train_batch_size=8,            # è®­ç»ƒæ‰¹æ¬¡å¤§å°
    per_device_eval_batch_size=16,            # è¯„ä¼°æ‰¹æ¬¡å¤§å°
    learning_rate=2e-5,                       # å­¦ä¹ ç‡
    weight_decay=0.01,                        # æƒé‡è¡°å‡
    
    # æ€§èƒ½ä¼˜åŒ–
    fp16=use_fp16,                            # æ··åˆç²¾åº¦ï¼ˆGPUï¼‰
    
    # æ—¥å¿—å’Œä¿å­˜
    logging_dir=logging_dir,                  # æ—¥å¿—ç›®å½•ï¼ˆç»å¯¹è·¯å¾„ï¼‰
    logging_steps=10,                         # æ¯ 10 æ­¥è®°å½•ä¸€æ¬¡
    save_strategy="epoch",                    # æ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡
    save_total_limit=2,                       # æœ€å¤šä¿å­˜ 2 ä¸ªæ£€æŸ¥ç‚¹
    
    # è¯„ä¼°è®¾ç½®
    eval_strategy="epoch",                    # æ¯ä¸ª epoch è¯„ä¼°ä¸€æ¬¡
    load_best_model_at_end=True,              # è®­ç»ƒç»“æŸåŠ è½½æœ€ä½³æ¨¡å‹
    metric_for_best_model="accuracy",         # ä½¿ç”¨å‡†ç¡®ç‡é€‰æ‹©æœ€ä½³æ¨¡å‹
    
    # å…¶ä»–
    seed=42,                                  # éšæœºç§å­
    report_to="none",                         # ä¸ä¸ŠæŠ¥åˆ°å¤–éƒ¨æœåŠ¡
)

print(f"âœ… è®­ç»ƒé…ç½®:")
print(f"   è®¾å¤‡: {device}")
print(f"   è®­ç»ƒè½®æ•°: {training_args.num_train_epochs}")
print(f"   æ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}")
print(f"   å­¦ä¹ ç‡: {training_args.learning_rate}")
print(f"   æ··åˆç²¾åº¦: {training_args.fp16}")

# ============================================================================
# æ­¥éª¤ 6: åˆ›å»º Trainer
# ============================================================================
print("\nğŸ¯ æ­¥éª¤ 6: åˆ›å»º Trainer")
print("-" * 70)

trainer = Trainer(
    model=model,                              # æ¨¡å‹
    args=training_args,                       # è®­ç»ƒå‚æ•°
    train_dataset=train_dataset,              # è®­ç»ƒæ•°æ®
    eval_dataset=eval_dataset,                # è¯„ä¼°æ•°æ®
    compute_metrics=compute_metrics,          # è¯„ä¼°æŒ‡æ ‡
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),  # æ•°æ®æ•´ç†å™¨
    processing_class=tokenizer,               # å¤„ç†ç±»ï¼ˆæ–°ç‰ˆæœ¬ç”¨è¿™ä¸ªä»£æ›¿ tokenizerï¼‰
)

print("âœ… Trainer åˆ›å»ºæˆåŠŸï¼")

# ============================================================================
# æ­¥éª¤ 7: å¼€å§‹è®­ç»ƒ
# ============================================================================
print("\nğŸš€ æ­¥éª¤ 7: å¼€å§‹è®­ç»ƒ")
print("=" * 70)
print("â³ è®­ç»ƒä¸­ï¼Œè¯·ç¨å€™...")
print()

try:
    # è®­ç»ƒæ¨¡å‹ï¼ˆå°±è¿™ä¸€è¡Œï¼ï¼‰
    train_result = trainer.train()
    
    print("\n" + "=" * 70)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)
    print(f"è®­ç»ƒæ—¶é—´: {train_result.metrics.get('train_runtime', 0):.2f} ç§’")
    if 'train_samples' in train_result.metrics:
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {train_result.metrics['train_samples']}")
    print(f"è®­ç»ƒæ­¥æ•°: {train_result.metrics.get('train_steps', 0)}")
    print(f"æœ€ç»ˆæŸå¤±: {train_result.metrics.get('train_loss', 0):.4f}")
    
except Exception as e:
    print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
    exit(1)

# ============================================================================
# æ­¥éª¤ 8: è¯„ä¼°æ¨¡å‹
# ============================================================================
print("\nğŸ“Š æ­¥éª¤ 8: è¯„ä¼°æ¨¡å‹")
print("=" * 70)

eval_result = trainer.evaluate()

print("âœ… è¯„ä¼°å®Œæˆï¼")
print(f"   å‡†ç¡®ç‡: {eval_result['eval_accuracy']:.2%}")
print(f"   æŸå¤±: {eval_result['eval_loss']:.4f}")

# ============================================================================
# æ­¥éª¤ 9: ä¿å­˜æ¨¡å‹
# ============================================================================
print("\nğŸ’¾ æ­¥éª¤ 9: ä¿å­˜æ¨¡å‹")
print("=" * 70)

trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)

print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")

# ============================================================================
# æ­¥éª¤ 10: æµ‹è¯•æ¨¡å‹
# ============================================================================
print("\nğŸ§ª æ­¥éª¤ 10: æµ‹è¯•æ¨¡å‹")
print("=" * 70)

# ä½¿ç”¨ pipeline è¿›è¡Œæ¨ç†
from transformers import pipeline

classifier = pipeline(
    "text-classification",
    model=output_dir,
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1
)

test_texts = [
    "è¿™ä¸ªäº§å“çœŸçš„å¾ˆæ£’ï¼",
    "å¤ªç³Ÿç³•äº†ï¼Œä¸æ¨èã€‚",
    "è¿˜è¡Œå§ï¼Œä¸€èˆ¬èˆ¬ã€‚",
]

print("æµ‹è¯•æ ·æœ¬é¢„æµ‹ç»“æœï¼š")
for text in test_texts:
    result = classifier(text)[0]
    label = "æ­£é¢ ğŸ˜Š" if result['label'] == 'LABEL_1' else "è´Ÿé¢ ğŸ˜"
    print(f"   æ–‡æœ¬: {text}")
    print(f"   é¢„æµ‹: {label} (ç½®ä¿¡åº¦: {result['score']:.2%})")
    print()

# ============================================================================
# æ€»ç»“
# ============================================================================
print("=" * 70)
print("âœ¨ æ€»ç»“")
print("=" * 70)
print("""
æ­å–œï¼ä½ å·²ç»æˆåŠŸä½¿ç”¨ Trainer API è®­ç»ƒäº†ä¸€ä¸ªæƒ…æ„Ÿåˆ†ç±»æ¨¡å‹ï¼

ğŸ¯ ä½ å­¦åˆ°äº†ä»€ä¹ˆï¼š
1. âœ… å¦‚ä½•å‡†å¤‡è®­ç»ƒæ•°æ®
2. âœ… å¦‚ä½•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
3. âœ… å¦‚ä½•é…ç½®è®­ç»ƒå‚æ•°
4. âœ… å¦‚ä½•ä½¿ç”¨ Trainer è®­ç»ƒæ¨¡å‹
5. âœ… å¦‚ä½•è¯„ä¼°å’Œä¿å­˜æ¨¡å‹
6. âœ… å¦‚ä½•ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹

ğŸ’¡ å…³é”®ä¼˜åŠ¿ï¼š
- åªéœ€ ~100 è¡Œä»£ç ï¼ˆåŒ…å«æ³¨é‡Šï¼‰
- è‡ªåŠ¨å¤„ç† GPUã€æ··åˆç²¾åº¦ã€æ—¥å¿—ç­‰
- ä»£ç æ¸…æ™°æ˜“æ‡‚ï¼Œæ˜“äºç»´æŠ¤

ğŸš€ ä¸‹ä¸€æ­¥ï¼š
- å°è¯•æ›´å¤§çš„æ•°æ®é›†
- è°ƒæ•´è¶…å‚æ•°ï¼ˆå­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ç­‰ï¼‰
- å°è¯•ä¸åŒçš„æ¨¡å‹ï¼ˆRoBERTaã€ALBERT ç­‰ï¼‰
- æ·»åŠ æ›´å¤šè¯„ä¼°æŒ‡æ ‡ï¼ˆF1ã€Precisionã€Recallï¼‰
- ä½¿ç”¨çœŸå®æ•°æ®é›†ï¼ˆå¦‚ IMDBã€SST-2ï¼‰

ğŸ“š æ›´å¤šèµ„æºï¼š
- å®˜æ–¹æ–‡æ¡£: https://huggingface.co/docs/transformers/training
- ç¤ºä¾‹ä»£ç : examples/pytorch/text-classification/
- ä¸­æ–‡æ–‡æ¡£: docs/source/zh/training.md
""")
print("=" * 70)
